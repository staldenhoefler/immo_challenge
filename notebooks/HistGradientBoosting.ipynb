{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import src.dataPipeline as dataPipeline\n",
    "import importlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "# Evaluating the model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error,r2_score ,make_scorer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "importlib.reload(dataPipeline)\n"
   ],
   "id": "db1c1001af0bf650"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def manual_cv_score(X, y, cv, model):\n",
    "    fold_train_mapes = []\n",
    "    fold_test_mapes = []\n",
    "\n",
    "    # Manual CV loop\n",
    "    for train_idx, test_idx in cv.split(X):\n",
    "        # Split the data for this fold\n",
    "        X_tr, X_te = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_tr, y_te = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        # Predict on training fold\n",
    "        y_tr_pred = model.predict(X_tr)\n",
    "        fold_train_mapes.append(mean_absolute_percentage_error(y_tr, y_tr_pred))\n",
    "\n",
    "        # Predict on test fold\n",
    "        y_te_pred = model.predict(X_te)\n",
    "        fold_test_mapes.append(mean_absolute_percentage_error(y_te, y_te_pred))\n",
    "\n",
    "    # Calculate mean & std for train/test MAPE across folds\n",
    "    train_mape_mean = np.mean(fold_train_mapes) * 100\n",
    "    train_mape_std  = np.std(fold_train_mapes)  * 100\n",
    "\n",
    "    test_mape_mean  = np.mean(fold_test_mapes)  * 100\n",
    "    test_mape_std   = np.std(fold_test_mapes)   * 100\n",
    "\n",
    "    return train_mape_mean, train_mape_std, test_mape_mean, test_mape_std"
   ],
   "id": "e1c68279108e2027"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dp = dataPipeline.DataPipeline()\n",
    "df = dp.runPipeline(\n",
    "    filePath=\"../data/immo_data_202208_v2.csv\",\n",
    "    imputer=None,\n",
    "    normalizeAndStandardize= False,\n",
    "    get_dummies = False\n",
    ")"
   ],
   "id": "3d7bddf11422e0c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = df.drop(columns=[\"Availability\"])\n",
    "df[\"type_unified\"] = df[\"type_unified\"].astype('category')"
   ],
   "id": "d538b0f381b05c8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.loc[df['Space extracted'] < 5, 'Space extracted'] = np.nan\n",
    "df.loc[df[\"Floor\"] >= 41, \"Floor\"] = np.nan\n",
    "\n",
    "#Filling Floor for House types with zeros\n",
    "house_types = [\n",
    "    'detached-house', 'villa', 'semi-detached-house', 'terrace-house',\n",
    "    'chalet', 'farmhouse', 'rustico', 'castle', 'detached-secondary-suite'\n",
    "]\n",
    "df.loc[\n",
    "    (df['type_unified'].isin(house_types)) & (df['Floor'].isna()),\n",
    "    'Floor'\n",
    "] = 0\n",
    "#Fill na with 0\n",
    "df[\"detail_responsive#surface_usable\"] = df[\"detail_responsive#surface_usable\"].fillna(0)\n",
    "df[\"Number of floors:\"] = df[\"Number of floors:\"].fillna(1)\n",
    "df[\"Plot_area_unified\"] = df[\"Plot_area_unified\"].fillna(0)"
   ],
   "id": "da4144325418037e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cat_col = [\"type_unified\"] + [f\"region_group_{i}\" for i in range(50)]\n",
    "\n",
    "house_type = [\"type_unified\"]\n",
    "num_col = [col for col in df.columns if col not in cat_col + [\"price_cleaned\"]]\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessors in a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, num_col),\n",
    "        ('cat', categorical_transformer, house_type)\n",
    "    ]\n",
    ")"
   ],
   "id": "1a428c7192fff5e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "hist_gradient_boosting = HistGradientBoostingRegressor(\n",
    "    max_iter=100, max_depth=10, random_state=42\n",
    ")\n"
   ],
   "id": "5592bc76466bf732"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create and combine preprocessing and modeling in a pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', hist_gradient_boosting)\n",
    "])\n",
    "\n",
    "# Separate target and features\n",
    "X = df.drop(columns=[\"price_cleaned\"])\n",
    "y = df[\"price_cleaned\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "pipeline.fit(X_train, y_train)"
   ],
   "id": "52c1aa3d06b29954"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_train_preprocessed = pd.DataFrame(X_train_preprocessed)\n",
    "\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "X_test_preprocessed = pd.DataFrame(X_test_preprocessed)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)"
   ],
   "id": "3fc7f85e0697d4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "learnrates = [0.1, 0.01]\n",
    "# Prepare lists to store results\n",
    "train_mape_scores_mean = []\n",
    "train_mape_scores_std = []\n",
    "test_mape_scores_mean = []\n",
    "test_mape_scores_std = []\n",
    "\n",
    "# Loop over different depths\n",
    "for lr in tqdm(learnrates):\n",
    "    # Define your estimator with the current max_depth\n",
    "    hist_gradient_boosting = HistGradientBoostingRegressor(\n",
    "        loss='squared_error',\n",
    "        quantile=None,\n",
    "        learning_rate=lr,\n",
    "        max_iter=5000,\n",
    "        max_leaf_nodes=31,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=0.0,\n",
    "        max_features=1.0,\n",
    "        max_bins=255,\n",
    "        categorical_features='warn',\n",
    "        monotonic_cst=None,\n",
    "        interaction_cst=None,\n",
    "        warm_start=False,\n",
    "        early_stopping='auto',\n",
    "        scoring='loss',\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=10,\n",
    "        tol=1e-07,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Build a Pipeline (you can add more steps if needed)\n",
    "    pipeline = Pipeline(steps=[('model', hist_gradient_boosting)])\n",
    "\n",
    "    # Call the function to perform manual CV and get MAPE metrics\n",
    "    train_mape_mean, train_mape_std, test_mape_mean, test_mape_std = manual_cv_score(\n",
    "        X=X_train_preprocessed,\n",
    "        y=y_train,\n",
    "        cv=cv,\n",
    "        model=pipeline\n",
    "    )\n",
    "\n",
    "    # Store results\n",
    "    train_mape_scores_mean.append(train_mape_mean)\n",
    "    train_mape_scores_std.append(train_mape_std)\n",
    "    test_mape_scores_mean.append(test_mape_mean)\n",
    "    test_mape_scores_std.append(test_mape_std)\n",
    "\n",
    "    # Print progress\n",
    "    print(\n",
    "        f\"learnrate: {lr}, \"\n",
    "        f\"Train MAPE: {train_mape_mean:.2f}% (±{train_mape_std:.2f}), \"\n",
    "        f\"Test MAPE: {test_mape_mean:.2f}% (±{test_mape_std:.2f})\"\n",
    "    )"
   ],
   "id": "b1fd3d76f32d9f6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.errorbar(learnrates, train_mape_scores_mean, yerr=train_mape_scores_std, label='Train MAPE')\n",
    "plt.errorbar(learnrates, test_mape_scores_mean, yerr=test_mape_scores_std, label='Test MAPE')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('MAPE (%)')\n",
    "plt.title('Train and Test MAPE for different Learning Rates')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "cc358a1b7cb00308"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "list_max_bins = [80, 128, 200]\n",
    "# Prepare lists to store results\n",
    "train_mape_scores_mean = []\n",
    "train_mape_scores_std = []\n",
    "test_mape_scores_mean = []\n",
    "test_mape_scores_std = []\n",
    "\n",
    "# Loop over different depths\n",
    "for mb in tqdm(list_max_bins):\n",
    "    # Define your estimator with the current max_depth\n",
    "    hist_gradient_boosting = HistGradientBoostingRegressor(\n",
    "        loss='squared_error',\n",
    "        quantile=None,\n",
    "        learning_rate=0.1,\n",
    "        max_iter=5000,\n",
    "        max_leaf_nodes=31,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=0.0,\n",
    "        max_features=1.0,\n",
    "        max_bins=mb,\n",
    "        categorical_features='warn',\n",
    "        monotonic_cst=None,\n",
    "        interaction_cst=None,\n",
    "        warm_start=False,\n",
    "        early_stopping='auto',\n",
    "        scoring='loss',\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=10,\n",
    "        tol=1e-07,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Build a Pipeline (you can add more steps if needed)\n",
    "    pipeline = Pipeline(steps=[('model', hist_gradient_boosting)])\n",
    "\n",
    "    # Call the function to perform manual CV and get MAPE metrics\n",
    "    train_mape_mean, train_mape_std, test_mape_mean, test_mape_std = manual_cv_score(\n",
    "        X=X_train_preprocessed,\n",
    "        y=y_train,\n",
    "        cv=cv,\n",
    "        model=pipeline\n",
    "    )\n",
    "\n",
    "    # Store results\n",
    "    train_mape_scores_mean.append(train_mape_mean)\n",
    "    train_mape_scores_std.append(train_mape_std)\n",
    "    test_mape_scores_mean.append(test_mape_mean)\n",
    "    test_mape_scores_std.append(test_mape_std)\n",
    "\n",
    "    # Print progress\n",
    "    print(\n",
    "        f\"Max binning: {mb}, \"\n",
    "        f\"Train MAPE: {train_mape_mean:.2f}% (±{train_mape_std:.2f}), \"\n",
    "        f\"Test MAPE: {test_mape_mean:.2f}% (±{test_mape_std:.2f})\"\n",
    "    )"
   ],
   "id": "a2bb307bd6b4365"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.errorbar(list_max_bins, train_mape_scores_mean, yerr=train_mape_scores_std, label='Train MAPE')\n",
    "plt.errorbar(list_max_bins, test_mape_scores_mean, yerr=test_mape_scores_std, label='Test MAPE')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('MAPE (%)')\n",
    "plt.title('Train and Test MAPE for different Learning Rates')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "7c48010926653a1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import optuna",
   "id": "96865607a7f14170"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_test",
   "id": "71be44f7c6089522"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_test = pd.get_dummies(X_test)\n",
    "X_train = pd.get_dummies(X_train)"
   ],
   "id": "d139092bef8004ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "id": "ddb42ea325bd5494"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 500),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10, 50),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 50),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 1e-3, 1e-1, log=True),\n",
    "        'max_bins': trial.suggest_int('max_bins',50, 255)\n",
    "    }\n",
    "    model = HistGradientBoostingRegressor(**params,early_stopping=True, validation_fraction=0.1, n_iter_no_change=10)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    return mape\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"Best parameters: \", study.best_params)\n",
    "print(\"Best MAPE: \", study.best_value)\n"
   ],
   "id": "1591efa4070843db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "best_hist_gradient_boosting = HistGradientBoostingRegressor(\n",
    "    loss='squared_error',\n",
    "    quantile=None,\n",
    "    learning_rate=study.best_params['learning_rate'],\n",
    "    max_iter=study.best_params['max_iter'],\n",
    "    max_leaf_nodes=study.best_params['max_leaf_nodes'],\n",
    "    max_depth=study.best_params['max_depth'],\n",
    "    min_samples_leaf=study.best_params['min_samples_leaf'],\n",
    "    l2_regularization=study.best_params['l2_regularization'],\n",
    "    max_features=1.0,\n",
    "    max_bins=study.best_params['max_bins'],\n",
    "    categorical_features='warn',\n",
    "    monotonic_cst=None,\n",
    "    interaction_cst=None,\n",
    "    warm_start=False,\n",
    "    early_stopping='auto',\n",
    "    scoring='loss',\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10,\n",
    "    tol=1e-07,\n",
    "    verbose=0,\n",
    "    random_state=42\n",
    ")"
   ],
   "id": "8fb64831638a3f9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "best_hist_gradient_boosting.fit(X_train, y_train)\n",
    "y_pred = best_hist_gradient_boosting.predict(X_test)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"MAPE: {mape:.2f}%\")"
   ],
   "id": "ff8c6223cbddc227"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Kaggle",
   "id": "aa969c74fb5abc73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_kaggle = dp.prepare_kaggle_dataset(\n",
    "    filePath=\"../data/test_data-Kaggle-v0.11.csv\",\n",
    "    imputer=None,\n",
    "    normalizeAndStandardize=False,\n",
    "    get_dummies=False\n",
    ")\n",
    "df_kaggle.head()\n",
    "df_kaggle.loc[df_kaggle['Space extracted'] < 5, 'Space extracted'] = np.nan\n",
    "df_kaggle.loc[df_kaggle[\"Floor\"] >= 41, \"Floor\"] = np.nan\n",
    "\n",
    "df_kaggle = df_kaggle.drop(columns=[\"Availability\"])\n",
    "\n",
    "cat_col = [\"type_unified\"] + [f\"region_group_{i}\" for i in range(50)]\n",
    "\n",
    "house_type = [\"type_unified\"]\n",
    "num_col = [col for col in df_kaggle.columns if col not in cat_col + [\"price_cleaned\"]]\n"
   ],
   "id": "3911969a15ce077"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_kaggle.head()",
   "id": "76d1968729eed34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_kaggle = df_kaggle.drop(['Type:', 'Hall height:'], axis=1)",
   "id": "136d670247418412"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_kaggle.head()",
   "id": "3ad723e793e72a0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_kaggle[\"type_unified\"] = df_kaggle[\"type_unified\"].astype('category')\n",
    "df_kaggle = pd.get_dummies(df_kaggle)"
   ],
   "id": "78cdb72b1b5d1848"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "num_col = [col for col in df_kaggle.columns if col not in cat_col + [\"price_cleaned\"]]",
   "id": "b5d8da9da5edad5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_kaggle[num_col] = scaler.fit_transform(df_kaggle[num_col])",
   "id": "2e3dc960bc830901"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "best_hist_gradient_boosting.fit(X_train, y_train)",
   "id": "cde1d2bb33a3c600"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "best_hist_gradient_boosting.fit(X_train, y_train)\n",
    "from src.utils.helperFunctions import create_kaggle_results\n",
    "\n",
    "results = best_hist_gradient_boosting.predict(df_kaggle)\n",
    "create_kaggle_results(results, path_to_kaggledata=\"../data/test_data-Kaggle-v0.11.csv\", csv_name='hist_gradient_v1')"
   ],
   "id": "6c7b2204607bdbc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5478eb8c2d671fce"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
