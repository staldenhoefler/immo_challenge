{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Vektorisierung der Beschreibung"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deede857473c518a"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-12T21:48:06.227525100Z",
     "start_time": "2025-01-12T21:47:58.847912600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:45: DtypeWarning: Columns (3,4,5,6,11,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,45,46,47,49,50,107,110,114,115,116,119,120,121,124,125,126,128,131,132) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(filePath)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                    detailed_description  price_cleaned\n0      DescriptionLuxuriöse Attika-Wohnung direkt an ...      1150000.0\n1      DescriptionStilvolle Liegenschaft an ruhiger L...      1420000.0\n2      detail_responsive#description_title2,5 Zimmerw...       720000.0\n3      DescriptionDieses äusserst grosszügige Minergi...      1430000.0\n4      DescriptionAus ehemals zwei Wohnungen wurde ei...       995000.0\n...                                                  ...            ...\n22476  Description\\n\"Hausteil mit verschieden Nutzung...       475000.0\n22477  Description\\n\"J'ADORE - Exklusives Wohnen in W...      1490000.0\n22478  Description\\n\"Einmalige Gelegenheit an sehr gu...      1450000.0\n22479  Description\\n\"LA VIE - Exklusives Wohnen in Ma...      1290000.0\n22480  Description\\n\"Historisches Altstadthaus im \"St...       780000.0\n\n[22481 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>detailed_description</th>\n      <th>price_cleaned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>DescriptionLuxuriöse Attika-Wohnung direkt an ...</td>\n      <td>1150000.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DescriptionStilvolle Liegenschaft an ruhiger L...</td>\n      <td>1420000.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>detail_responsive#description_title2,5 Zimmerw...</td>\n      <td>720000.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DescriptionDieses äusserst grosszügige Minergi...</td>\n      <td>1430000.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DescriptionAus ehemals zwei Wohnungen wurde ei...</td>\n      <td>995000.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22476</th>\n      <td>Description\\n\"Hausteil mit verschieden Nutzung...</td>\n      <td>475000.0</td>\n    </tr>\n    <tr>\n      <th>22477</th>\n      <td>Description\\n\"J'ADORE - Exklusives Wohnen in W...</td>\n      <td>1490000.0</td>\n    </tr>\n    <tr>\n      <th>22478</th>\n      <td>Description\\n\"Einmalige Gelegenheit an sehr gu...</td>\n      <td>1450000.0</td>\n    </tr>\n    <tr>\n      <th>22479</th>\n      <td>Description\\n\"LA VIE - Exklusives Wohnen in Ma...</td>\n      <td>1290000.0</td>\n    </tr>\n    <tr>\n      <th>22480</th>\n      <td>Description\\n\"Historisches Altstadthaus im \"St...</td>\n      <td>780000.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>22481 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from src.dataPipeline import DataPipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "\n",
    "columns_to_drop_all = [\n",
    "'Unnamed: 0.1', 'Unnamed: 0','Municipality','Living space',\n",
    "'Plot area','Floor space','location',\n",
    "'description',\n",
    "#'detailed_description',\n",
    "'url','table','detail_responsive#municipality',\n",
    "'detail_responsive#surface_living','detail_responsive#floor','detail_responsive#available_from',\n",
    "'Gemeinde','Wohnfläche','Stockwerk','Nutzfläche','Verfügbarkeit','Grundstücksfläche',\n",
    "'detail_responsive#surface_property','Commune','Surface habitable','Surface du terrain',\n",
    "'Surface utile','Disponibilité','Étage','Comune','Superficie abitabile','Disponibilità',\n",
    "'Piano','Superficie del terreno','Superficie utile','Municipality_merged',\n",
    "'Floor_merged','Living_space_merged','Plot_area_merged','Availability_merged','location_parsed',\n",
    "'title',\n",
    "'price','address','link','details_structured','index',\n",
    "'Locality','Zip','rooms','Floor_unified','Living_area_unified','space',\n",
    "'price_s','address_s','Surface living:','Land area:',\n",
    "'description_detailed','Floor space:','Volume:','plz','Number of toilets:','Gross yield:',\n",
    "'Minimum floor space:','space_cleaned'\n",
    "\n",
    "# Temporary columns\n",
    ", 'Gross return'\n",
    ", 'details'\n",
    ", 'Room height:'\n",
    ", 'features'\n",
    ", 'type'\n",
    ", 'provider' ,\n",
    "'type_unified',\n",
    "'Availability',\n",
    "'Floor',\n",
    "'detail_responsive#surface_usable',\n",
    "'Floor_space_merged',\n",
    "'lat',\n",
    "'lon',\n",
    "'ForestDensityL',\n",
    "'ForestDensityM',\n",
    "'ForestDensityS',\n",
    "'Latitude',\n",
    "'Longitude',\n",
    "'NoisePollutionRailwayL',\n",
    "'NoisePollutionRailwayM',\n",
    "'NoisePollutionRailwayS',\n",
    "'NoisePollutionRoadL',\n",
    "'NoisePollutionRoadM',\n",
    "'NoisePollutionRoadS',\n",
    "'PopulationDensityL',\n",
    "'PopulationDensityM',\n",
    "'PopulationDensityS',\n",
    "'RiversAndLakesL',\n",
    "'RiversAndLakesM',\n",
    "'RiversAndLakesS',\n",
    "'WorkplaceDensityL',\n",
    "'WorkplaceDensityM',\n",
    "'WorkplaceDensityS',\n",
    "'distanceToTrainStation',\n",
    "'gde_area_agriculture_percentage',\n",
    "'gde_area_forest_percentage',\n",
    "'gde_area_nonproductive_percentage',\n",
    "'gde_area_settlement_percentage',\n",
    "'gde_average_house_hold',\n",
    "'gde_empty_apartments',\n",
    "'gde_foreigners_percentage',\n",
    "'gde_new_homes_per_1000',\n",
    "'gde_politics_bdp',\n",
    "'gde_politics_cvp',\n",
    "'gde_politics_evp',\n",
    "'gde_politics_fdp',\n",
    "'gde_politics_glp',\n",
    "'gde_politics_gps',\n",
    "'gde_politics_pda',\n",
    "'gde_politics_rights',\n",
    "'gde_politics_sp',\n",
    "'gde_politics_svp',\n",
    "'gde_pop_per_km2',\n",
    "'gde_population',\n",
    "'gde_private_apartments',\n",
    "'gde_social_help_quota',\n",
    "'gde_tax',\n",
    "'gde_workers_sector1',\n",
    "'gde_workers_sector2',\n",
    "'gde_workers_sector3',\n",
    "'gde_workers_total',\n",
    "'plz_parsed',\n",
    "'No. of rooms:',\n",
    "'Number of apartments:',\n",
    "'Last refurbishment:',\n",
    "'Number of floors:',\n",
    "'Year built:',\n",
    "'Space extracted',\n",
    "'Plot_area_unified',\n",
    "]\n",
    "\n",
    "dp = DataPipeline()\n",
    "dp.readCsv(\"data/immo_data_202208_v2.csv\")\n",
    "dp.dropColumns(columns_to_drop_all)\n",
    "\n",
    "df = dp.getData()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# set the random seed to get comparable results\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "features = df['detailed_description']\n",
    "# Fill NaN values with empty string\n",
    "features = features.fillna('')\n",
    "target = df['price_cleaned']\n",
    "\n",
    "# Drop all columns with NaN values in price_cleaned\n",
    "features = features[target.notnull()]\n",
    "target = target[target.notnull()]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T21:48:06.318096200Z",
     "start_time": "2025-01-12T21:48:06.223186200Z"
    }
   },
   "id": "57611c658697b95a",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array(['00', '000', '0000', ..., 'œnothèque', 'œuvre', 'œuvres'],\n      dtype=object)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# prepare the data\n",
    "comments_train, comments_test, y_train, y_test = train_test_split(features, target, test_size = 0.3) \n",
    "\n",
    "# we use a count vectorizer to break the individual comments into tokens, one token per word\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(comments_train)\n",
    "\n",
    "# we create the training a matrix \n",
    "X_train = vectorizer.transform(comments_train)\n",
    "X_test = vectorizer.transform(comments_test)\n",
    "\n",
    "# train the classifier\n",
    "#classifier = LogisticRegression(max_iter=2000)\n",
    "#classifier.fit(X_train, y_train)\n",
    "vectorizer.get_feature_names_out()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T21:48:12.443685500Z",
     "start_time": "2025-01-12T21:48:06.300182500Z"
    }
   },
   "id": "5c8a9fc25a1bf792",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mensemble\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RandomForestRegressor\n\u001B[0;32m      3\u001B[0m classifier \u001B[38;5;241m=\u001B[39m RandomForestRegressor()\n\u001B[1;32m----> 4\u001B[0m \u001B[43mclassifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    478\u001B[0m trees \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    479\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_estimator(append\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m    480\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_more_estimators)\n\u001B[0;32m    481\u001B[0m ]\n\u001B[0;32m    483\u001B[0m \u001B[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001B[39;00m\n\u001B[0;32m    484\u001B[0m \u001B[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001B[39;00m\n\u001B[0;32m    485\u001B[0m \u001B[38;5;66;03m# making threading more efficient than multiprocessing in\u001B[39;00m\n\u001B[0;32m    486\u001B[0m \u001B[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001B[39;00m\n\u001B[0;32m    487\u001B[0m \u001B[38;5;66;03m# parallel_backend contexts set at a higher level,\u001B[39;00m\n\u001B[0;32m    488\u001B[0m \u001B[38;5;66;03m# since correctness does not rely on using threads.\u001B[39;00m\n\u001B[1;32m--> 489\u001B[0m trees \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    491\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    492\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprefer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mthreads\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_parallel_build_trees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    495\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    505\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    506\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    507\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    508\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    510\u001B[0m \u001B[38;5;66;03m# Collect newly grown trees\u001B[39;00m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mextend(trees)\n",
      "File \u001B[1;32mC:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     69\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     70\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     71\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     73\u001B[0m )\n\u001B[1;32m---> 74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1918\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1916\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[0;32m   1917\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1920\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[0;32m   1921\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[0;32m   1922\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[0;32m   1923\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[0;32m   1924\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[0;32m   1925\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[1;32mC:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1847\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1846\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1847\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1848\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1849\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[1;32mC:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    134\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 136\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001B[0m, in \u001B[0;36m_parallel_build_trees\u001B[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001B[0m\n\u001B[0;32m    189\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m class_weight \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced_subsample\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    190\u001B[0m         curr_sample_weight \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m compute_sample_weight(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m\"\u001B[39m, y, indices\u001B[38;5;241m=\u001B[39mindices)\n\u001B[1;32m--> 192\u001B[0m     \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    194\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurr_sample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    196\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    200\u001B[0m     tree\u001B[38;5;241m.\u001B[39m_fit(\n\u001B[0;32m    201\u001B[0m         X,\n\u001B[0;32m    202\u001B[0m         y,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    205\u001B[0m         missing_values_in_feature_mask\u001B[38;5;241m=\u001B[39mmissing_values_in_feature_mask,\n\u001B[0;32m    206\u001B[0m     )\n",
      "File \u001B[1;32mC:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001B[0m, in \u001B[0;36mBaseDecisionTree._fit\u001B[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001B[0m\n\u001B[0;32m    461\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    462\u001B[0m     builder \u001B[38;5;241m=\u001B[39m BestFirstTreeBuilder(\n\u001B[0;32m    463\u001B[0m         splitter,\n\u001B[0;32m    464\u001B[0m         min_samples_split,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    469\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_impurity_decrease,\n\u001B[0;32m    470\u001B[0m     )\n\u001B[1;32m--> 472\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    474\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    475\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "classifier = RandomForestRegressor()\n",
    "classifier.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T22:12:09.128587200Z",
     "start_time": "2025-01-12T21:48:12.443685500Z"
    }
   },
   "id": "eb16d7f5aceed90",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#test the data\n",
    "score = classifier.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score)\n",
    "\n",
    "#test the classifier with mape\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "y_pred = classifier.predict(X_test)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "print(\"MAPE:\", mape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-12T22:12:09.124889900Z"
    }
   },
   "id": "a466c500353935aa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, classifier.predict(X_test))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-12T22:12:09.127507700Z"
    }
   },
   "id": "873aaa4026f00be7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embedding for Text with BERT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fab0c90bae43a2c6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertForRegression(nn.Module):\n",
    "    def __init__(self, pretrained_model_name):\n",
    "        super(BertForRegression, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.fc1 = nn.Linear(self.bert.config.hidden_size, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output  # `[CLS]` token output\n",
    "        x = self.fc1(cls_output)\n",
    "        x = nn.ReLU()(x)\n",
    "        return self.fc2(x)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-18T10:33:06.765141Z"
    }
   },
   "id": "10fccb00c396d12b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RealEstateDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        description = self.dataframe.iloc[index]['detailed_description']\n",
    "        price = self.dataframe.iloc[index]['price_cleaned']\n",
    "\n",
    "        # Tokenisierung\n",
    "        encoding = self.tokenizer(\n",
    "            description,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),  # Entferne Batch-Dimension\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'target': torch.tensor(price, dtype=torch.float)\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-18T10:33:06.767138500Z"
    }
   },
   "id": "fe067b8490e3e417",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from src import torchModelRun\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "max_length = 128\n",
    "train_dataset = RealEstateDataset(train_df, tokenizer, max_length)\n",
    "test_dataset = RealEstateDataset(test_df, tokenizer, max_length)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#import sgd\n",
    "from torch.optim import SGD\n",
    "import torch.nn as nn\n",
    "\n",
    "# Modell initialisieren\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertForRegression('bert-base-uncased').to(device)\n",
    "\n",
    "# Optimizer und Verlustfunktion\n",
    "optimizer = SGD(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        target = batch['target'].view(-1, 1).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-18T10:33:06.768141200Z"
    }
   },
   "id": "eb0b02a703b27176",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Versuch mit Huggingface Library 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91b13581cc88a8fe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "df = df[df['price_cleaned'].notnull()]  # Entferne Zeilen mit fehlendem Preis\n",
    "df = df[df['detailed_description'].notnull()]  # Entferne Zeilen mit fehlender Beschreibung\n",
    "df['log_price'] = np.log(df['price_cleaned'])  # Logarithmiere den Preis\n",
    "\n",
    "# Erstelle HuggingFace Dataset\n",
    "data = Dataset.from_pandas(df[['detailed_description', 'log_price']].rename(columns={\n",
    "    'detailed_description': 'text', 'log_price': 'label'\n",
    "}))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-23T15:28:57.767301300Z",
     "start_time": "2024-12-23T15:28:56.719483800Z"
    }
   },
   "id": "93cbbf03513b8cfa",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/21466 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ce094446762437f89e33816f9789797"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "model_name = \"bert-base-german-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-Funktion\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=64)\n",
    "\n",
    "# Tokenisierung anwenden\n",
    "dataset = data.map(preprocess_function, batched=True)\n",
    "\n",
    "# Datenset splitten\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-23T15:30:00.783305Z",
     "start_time": "2024-12-23T15:28:58.863193100Z"
    }
   },
   "id": "920c6c9e39ac32c3",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\denis\\AppData\\Local\\Temp\\ipykernel_42284\\1745905395.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: denis-schatzmann. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.18.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\wandb\\run-20241223_163012-8j327cuv</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/denis-schatzmann/huggingface/runs/8j327cuv' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/denis-schatzmann/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/denis-schatzmann/huggingface' target=\"_blank\">https://wandb.ai/denis-schatzmann/huggingface</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/denis-schatzmann/huggingface/runs/8j327cuv' target=\"_blank\">https://wandb.ai/denis-schatzmann/huggingface/runs/8j327cuv</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='25758' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    2/25758 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Modell laden\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=1)  # num_labels=1 für Regression\n",
    "\n",
    "# BERT einfrieren\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model.classifier = nn.Linear(model.bert.config.hidden_size, 1)\n",
    "\n",
    "# Metriken für Regression\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.squeeze()\n",
    "    mse = ((predictions - labels) ** 2).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = (np.abs(predictions - labels) / labels).mean() * 100\n",
    "    return {\"mse\": mse, \"rmse\": rmse, \"mape\": mape}\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-6,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,\n",
    ")\n",
    "\n",
    "# Trainer initialisieren\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Modell trainieren\n",
    "trainer.train()\n",
    "\n",
    "# Modell speichern\n",
    "trainer.save_model(\"./regression_model\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-23T15:40:42.358552200Z",
     "start_time": "2024-12-23T15:30:07.297696900Z"
    }
   },
   "id": "df33e290fb088f94",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: 1274757.8\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "model_name = \"bert-base-german-cased\"\n",
    "model = BertForSequenceClassification.from_pretrained(\"./regression_model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Test the model with new data\n",
    "new_texts = [\"Alte Scheune in ländlicher Umgebung zu verkaufen. Ist sanierungsbedürftig. Kiesweg führt zum Haus.adfasdfasdfasdf viel text\"]\n",
    "tokenized = tokenizer(new_texts, truncation=True, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(**tokenized).logits\n",
    "    print(\"Predictions:\", np.exp(predictions.squeeze().numpy()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-23T15:51:38.138910700Z",
     "start_time": "2024-12-23T15:51:37.731097400Z"
    }
   },
   "id": "f744f9a3c21c5bf5",
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
