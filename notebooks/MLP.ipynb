{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-11T16:19:26.323712700Z",
     "start_time": "2024-11-11T16:19:18.657208700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:39: DtypeWarning: Columns (3,4,5,6,11,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,45,46,47,49,50,107,110,114,115,116,119,120,121,124,125,126,128,131,132) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(filePath)\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:62: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Space extracted']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:67: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Plot_area_unified']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:73: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Availability']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:77: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['No. of rooms:']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.dataPipeline import DataPipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "dp = DataPipeline()\n",
    "df = dp.runPipeline(normalizeAndStandardize=False)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: denis-schatzmann. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.18.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\wandb\\run-20241111_171929-pbhhnoyr</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/pbhhnoyr' target=\"_blank\">MLP-bs16-lr5e-05</a></strong> to <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/pbhhnoyr' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/pbhhnoyr</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 133.64595178845286, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 2, Loss: 16.344204555571764, Train MAPE: 208.74148513969334, Test MAPE: nan\n",
      "Epoch 3, Loss: 1.3763191432959732, Train MAPE: 81.5778908060885, Test MAPE: 80.70056893295002\n",
      "Epoch 4, Loss: 0.6457628226742662, Train MAPE: 66.52171879910874, Test MAPE: 65.50716505485309\n",
      "Epoch 5, Loss: 0.5498119457100314, Train MAPE: 61.99466845194499, Test MAPE: 61.47004110116422\n",
      "Epoch 6, Loss: 0.5111943863846105, Train MAPE: 64.14636256557772, Test MAPE: 64.22668991395679\n",
      "Epoch 7, Loss: 0.4835142846080078, Train MAPE: 58.19843794504801, Test MAPE: 58.39729451749663\n",
      "Epoch 8, Loss: 0.46321816838335717, Train MAPE: 56.204012947520994, Test MAPE: 56.82634499884802\n",
      "Epoch 9, Loss: 0.44723313335200837, Train MAPE: 53.182469552138755, Test MAPE: 53.72802733352293\n",
      "Epoch 10, Loss: 0.43462639478431353, Train MAPE: 55.71729372835707, Test MAPE: 56.90244307530789\n",
      "Epoch 11, Loss: 0.4226765029132366, Train MAPE: 51.65244998493414, Test MAPE: 52.541965755636504\n",
      "Epoch 12, Loss: 0.41384549594387926, Train MAPE: 52.154375920350525, Test MAPE: 53.31084153223932\n",
      "Epoch 13, Loss: 0.4043068967845248, Train MAPE: 52.621054432309904, Test MAPE: 54.09163913777942\n",
      "Epoch 14, Loss: 0.3957896360206878, Train MAPE: 51.88745265829152, Test MAPE: 53.475181190961166\n",
      "Epoch 15, Loss: 0.388009845516805, Train MAPE: 50.58959498569883, Test MAPE: 52.165684728136654\n",
      "Epoch 16, Loss: 0.38103302626431673, Train MAPE: 48.204794383871146, Test MAPE: 49.55939282169291\n",
      "Epoch 17, Loss: 0.37482407723350086, Train MAPE: 48.95556726346071, Test MAPE: 50.603014181510375\n",
      "Epoch 18, Loss: 0.3688073260516956, Train MAPE: 46.96942622963039, Test MAPE: 48.55639898489372\n",
      "Epoch 19, Loss: 0.36335145326695223, Train MAPE: 47.336000006226286, Test MAPE: 48.977228180013455\n",
      "Epoch 20, Loss: 0.3584459793293613, Train MAPE: 46.50000749127618, Test MAPE: 48.14748038938794\n",
      "Epoch 21, Loss: 0.35393657388872113, Train MAPE: 46.2082849283328, Test MAPE: 47.901886170415395\n",
      "Epoch 22, Loss: 0.3498177867040209, Train MAPE: 46.666552464715366, Test MAPE: 48.41594415823192\n",
      "Epoch 23, Loss: 0.3459454019350567, Train MAPE: 45.84807792970504, Test MAPE: 47.50947815739118\n",
      "Epoch 24, Loss: 0.3423825662112099, Train MAPE: 45.42833545947897, Test MAPE: 47.22349176585834\n",
      "Epoch 25, Loss: 0.33907660370285825, Train MAPE: 46.916588807380066, Test MAPE: 48.852283774368246\n",
      "Epoch 26, Loss: 0.33603020564749325, Train MAPE: 45.432747544913454, Test MAPE: 47.34221069295029\n",
      "Epoch 27, Loss: 0.3333020105058777, Train MAPE: 44.86419988171808, Test MAPE: 46.71278516344986\n",
      "Epoch 28, Loss: 0.3303997972273621, Train MAPE: 44.8197349975849, Test MAPE: 46.68345942765714\n",
      "Epoch 29, Loss: 0.32822765940907356, Train MAPE: 45.42801344422088, Test MAPE: 47.417757018961154\n",
      "Epoch 30, Loss: 0.32600075157209374, Train MAPE: 44.160659266066276, Test MAPE: 46.03957536201375\n",
      "Epoch 31, Loss: 0.3235479302875612, Train MAPE: 43.932375013417214, Test MAPE: 45.76296321060958\n",
      "Epoch 32, Loss: 0.32150730603299604, Train MAPE: 44.29156650236283, Test MAPE: 46.13767310441659\n",
      "Epoch 33, Loss: 0.3196774597012106, Train MAPE: 44.83113409678141, Test MAPE: 46.86397131134933\n",
      "Epoch 34, Loss: 0.31791333134832056, Train MAPE: 43.84241339475259, Test MAPE: 45.81624024419299\n",
      "Epoch 35, Loss: 0.31603674893749173, Train MAPE: 44.42367195699407, Test MAPE: 46.47043720071501\n",
      "Epoch 36, Loss: 0.31445334622229654, Train MAPE: 43.25790529579952, Test MAPE: 45.20537765032484\n",
      "Epoch 37, Loss: 0.3126170092829685, Train MAPE: 42.40311183710208, Test MAPE: 44.2305864635805\n",
      "Epoch 38, Loss: 0.3115514062045292, Train MAPE: 44.13251337950257, Test MAPE: 46.200923638433295\n",
      "Epoch 39, Loss: 0.3101315674768097, Train MAPE: 42.39141754062697, Test MAPE: 44.23850136212305\n",
      "Epoch 40, Loss: 0.3087589123725206, Train MAPE: 43.19663241046599, Test MAPE: 45.18614879434294\n",
      "Epoch 41, Loss: 0.30765717375124324, Train MAPE: 43.83409725167285, Test MAPE: 45.939499714419284\n",
      "Epoch 42, Loss: 0.3065311904739717, Train MAPE: 42.246439857044436, Test MAPE: 44.18641205958962\n",
      "Epoch 43, Loss: 0.30536003233543757, Train MAPE: 42.71781493000601, Test MAPE: 44.814403140193335\n",
      "Epoch 44, Loss: 0.30457457759599577, Train MAPE: 42.69118494713443, Test MAPE: 44.69909569788874\n",
      "Epoch 45, Loss: 0.3034343390372293, Train MAPE: 42.170540680282414, Test MAPE: 44.136327186154936\n",
      "Epoch 46, Loss: 0.3023833153747964, Train MAPE: 42.38193911409926, Test MAPE: 44.42716700781446\n",
      "Epoch 47, Loss: 0.30142237499527547, Train MAPE: 42.02374647513203, Test MAPE: 44.00201966743367\n",
      "Epoch 48, Loss: 0.30092852071098897, Train MAPE: 42.492776392794205, Test MAPE: 44.529677296451844\n",
      "Epoch 49, Loss: 0.2998156586212331, Train MAPE: 43.69493333663063, Test MAPE: 45.91361984171113\n",
      "Epoch 50, Loss: 0.29931548360692356, Train MAPE: 42.112742530340434, Test MAPE: 44.174491350516554\n",
      "Epoch 51, Loss: 0.2984103419314856, Train MAPE: 41.32563588043739, Test MAPE: 43.328843502832164\n",
      "Epoch 52, Loss: 0.29790888310720526, Train MAPE: 41.73280089915484, Test MAPE: 43.77303926810502\n",
      "Epoch 53, Loss: 0.29705167551064626, Train MAPE: 42.12649453261803, Test MAPE: 44.19705382748519\n",
      "Epoch 54, Loss: 0.296608562968757, Train MAPE: 41.93065038439871, Test MAPE: 44.04277095845813\n",
      "Epoch 55, Loss: 0.29606050467011574, Train MAPE: 41.44627031677071, Test MAPE: 43.49256320677238\n",
      "Epoch 56, Loss: 0.29531929291699127, Train MAPE: 41.90797464874969, Test MAPE: 43.98174116630656\n",
      "Epoch 57, Loss: 0.2946642153350444, Train MAPE: 42.43275661468506, Test MAPE: 44.65438951180384\n",
      "Epoch 58, Loss: 0.2943989813541886, Train MAPE: 41.86991927267491, Test MAPE: 43.982870546806275\n",
      "Epoch 59, Loss: 0.29383144413345846, Train MAPE: 41.70448558412749, Test MAPE: 43.8313444459726\n",
      "Epoch 60, Loss: 0.29341045138907845, Train MAPE: 42.55372596389946, Test MAPE: 44.71955020689772\n",
      "Epoch 61, Loss: 0.2927098849424343, Train MAPE: 42.30806526315623, Test MAPE: 44.47588832640456\n",
      "Epoch 62, Loss: 0.2924774591821706, Train MAPE: 41.46808718100361, Test MAPE: 43.56037983114534\n",
      "Epoch 63, Loss: 0.29210175960057083, Train MAPE: 41.976479839456495, Test MAPE: 44.191327343038196\n",
      "Epoch 64, Loss: 0.2916327465496872, Train MAPE: 40.65203504671996, Test MAPE: 42.731677584609784\n",
      "Epoch 65, Loss: 0.2912383220576007, Train MAPE: 40.930099938929764, Test MAPE: 43.041289352539714\n",
      "Epoch 66, Loss: 0.2910595168241824, Train MAPE: 41.10019076994096, Test MAPE: 43.19662655582377\n",
      "Epoch 67, Loss: 0.2907242979040776, Train MAPE: 40.96668219730772, Test MAPE: 43.07945163512038\n",
      "Epoch 68, Loss: 0.2903182768016711, Train MAPE: 41.70323626529211, Test MAPE: 43.85493227368066\n",
      "Epoch 69, Loss: 0.2897334015831865, Train MAPE: 41.178708104977666, Test MAPE: 43.3445501301947\n",
      "Epoch 70, Loss: 0.28958535341695807, Train MAPE: 40.823251954440416, Test MAPE: 42.9351435704781\n",
      "Epoch 71, Loss: 0.28928851605574973, Train MAPE: 41.31245585036004, Test MAPE: 43.483560398500664\n",
      "Epoch 72, Loss: 0.28895905231949925, Train MAPE: 41.97947645954702, Test MAPE: 44.298585590025056\n",
      "Epoch 73, Loss: 0.2884941536033976, Train MAPE: 39.96126327295413, Test MAPE: 41.979950038101975\n",
      "Epoch 74, Loss: 0.2884625134026182, Train MAPE: 41.98878818161186, Test MAPE: 44.284214377722854\n",
      "Epoch 75, Loss: 0.28805582974662725, Train MAPE: 40.98089132418578, Test MAPE: 43.14488449812575\n",
      "Epoch 76, Loss: 0.28797363023990874, Train MAPE: 40.5885946405345, Test MAPE: 42.70724089983002\n",
      "Epoch 77, Loss: 0.28756421145865285, Train MAPE: 41.25146124828821, Test MAPE: 43.475207980772126\n",
      "Epoch 78, Loss: 0.28740085172841606, Train MAPE: 41.94036416568975, Test MAPE: 44.22566666999387\n",
      "Epoch 79, Loss: 0.28718250207174784, Train MAPE: 40.783741695579444, Test MAPE: 42.92645646675662\n",
      "Epoch 80, Loss: 0.2868130399321003, Train MAPE: 40.70959936448897, Test MAPE: 42.916069076783536\n",
      "Epoch 81, Loss: 0.2866087130688388, Train MAPE: 41.12133141002436, Test MAPE: 43.30076785253775\n",
      "Epoch 82, Loss: 0.28648296900350473, Train MAPE: 41.16370327390474, Test MAPE: 43.432367621414144\n",
      "Epoch 83, Loss: 0.2862200831887366, Train MAPE: 41.67646823751515, Test MAPE: 44.01734132229802\n",
      "Epoch 84, Loss: 0.2860939353618814, Train MAPE: 41.12361524296903, Test MAPE: 43.39600956280174\n",
      "Epoch 85, Loss: 0.28574310017899535, Train MAPE: 40.78771829057014, Test MAPE: 43.033671105515225\n",
      "Epoch 86, Loss: 0.2855659453005626, Train MAPE: 40.260727126022864, Test MAPE: 42.368955806496956\n",
      "Epoch 87, Loss: 0.28552596820411325, Train MAPE: 41.287520877794286, Test MAPE: 43.59818492817815\n",
      "Epoch 88, Loss: 0.28539097586582446, Train MAPE: 41.46292204363593, Test MAPE: 43.76735655040588\n",
      "Epoch 89, Loss: 0.28521410797604885, Train MAPE: 40.92557748597244, Test MAPE: 43.198949143969664\n",
      "Epoch 90, Loss: 0.2859866487183447, Train MAPE: 41.69126284369107, Test MAPE: 44.025415369396875\n",
      "Epoch 91, Loss: 0.28467680856414224, Train MAPE: 41.362141384475535, Test MAPE: 43.684173583984375\n",
      "Epoch 92, Loss: 0.2844381266669638, Train MAPE: 40.53415800072681, Test MAPE: 42.77285007752938\n",
      "Epoch 93, Loss: 0.28425739635338726, Train MAPE: 40.17014416661756, Test MAPE: 42.429662668673025\n",
      "Epoch 94, Loss: 0.2841751172453508, Train MAPE: 40.486012722980014, Test MAPE: 42.74221924482657\n",
      "Epoch 95, Loss: 0.28465954147964373, Train MAPE: 41.2358451054014, Test MAPE: 43.54447809173339\n",
      "Epoch 96, Loss: 0.2849398918578337, Train MAPE: 41.020481225813946, Test MAPE: 43.28514858481072\n",
      "Epoch 97, Loss: 0.28372505144911936, Train MAPE: 41.098962308072494, Test MAPE: 43.407674707612145\n",
      "Epoch 98, Loss: 0.28364827897099926, Train MAPE: 40.30369590890819, Test MAPE: 42.53897627045578\n",
      "Epoch 99, Loss: 0.2833791003145021, Train MAPE: 40.49191492124535, Test MAPE: 42.77815498679317\n",
      "Epoch 100, Loss: 0.2833114097601381, Train MAPE: 40.22760593589695, Test MAPE: 42.46609754792487\n",
      "Epoch 101, Loss: 0.28312510074298275, Train MAPE: 40.018567931515044, Test MAPE: 42.29270527509838\n",
      "Epoch 102, Loss: 0.282837284407739, Train MAPE: 39.741068068317986, Test MAPE: 41.92244064392417\n",
      "Epoch 103, Loss: 0.28278029706628843, Train MAPE: 40.770005313829444, Test MAPE: 43.080693840660935\n",
      "Epoch 104, Loss: 0.2826820138413673, Train MAPE: 40.56344941237877, Test MAPE: 42.85371180066474\n",
      "Epoch 105, Loss: 0.2824143051672941, Train MAPE: 40.244436650988696, Test MAPE: 42.527230669282396\n",
      "Epoch 106, Loss: 0.2820952835781821, Train MAPE: 40.52119127470871, Test MAPE: 42.83433974524286\n",
      "Epoch 107, Loss: 0.28223973959684373, Train MAPE: 41.154504124871615, Test MAPE: 43.50100703916984\n",
      "Epoch 108, Loss: 0.2820958610067422, Train MAPE: 40.35306406569207, Test MAPE: 42.63206604147085\n",
      "Epoch 109, Loss: 0.28186208303032934, Train MAPE: 40.765291143833906, Test MAPE: 43.12499366432988\n",
      "Epoch 110, Loss: 0.2820042487619252, Train MAPE: 40.531526854942584, Test MAPE: 42.83404525163666\n",
      "Epoch 111, Loss: 0.2827069108727677, Train MAPE: 40.55843130089771, Test MAPE: 42.88939067441721\n",
      "Epoch 112, Loss: 0.28165541082278067, Train MAPE: 40.63464932058049, Test MAPE: 42.9661502582458\n",
      "Epoch 113, Loss: 0.2815282741326025, Train MAPE: 41.00436734166639, Test MAPE: 43.35327328945293\n",
      "Epoch 114, Loss: 0.28137245521634474, Train MAPE: 40.03719699574613, Test MAPE: 42.25271738374521\n",
      "Epoch 115, Loss: 0.2822114020500375, Train MAPE: 40.687875991032044, Test MAPE: 43.0230737435594\n",
      "Epoch 116, Loss: 0.2810894805252895, Train MAPE: 40.434429524958816, Test MAPE: 42.7155626210067\n",
      "Epoch 117, Loss: 0.28111924185663806, Train MAPE: 39.786471353728196, Test MAPE: 42.032000513562565\n",
      "Epoch 118, Loss: 0.28083578718119656, Train MAPE: 40.19372901916504, Test MAPE: 42.505798431887385\n",
      "Epoch 119, Loss: 0.2808267175594623, Train MAPE: 39.922701414700214, Test MAPE: 42.15461243637126\n",
      "Epoch 120, Loss: 0.28072701916098597, Train MAPE: 40.51369582647565, Test MAPE: 42.85398327824897\n",
      "Epoch 121, Loss: 0.2805524360388517, Train MAPE: 39.55705125852563, Test MAPE: 41.79886961111115\n",
      "Epoch 122, Loss: 0.2805428677763062, Train MAPE: 40.327683957417804, Test MAPE: 42.682154540402\n",
      "Epoch 123, Loss: 0.28024422313547, Train MAPE: 40.61897616331605, Test MAPE: 42.955795666487546\n",
      "Epoch 124, Loss: 0.28036187232910903, Train MAPE: 40.731281219131645, Test MAPE: 43.134378075280075\n",
      "Epoch 125, Loss: 0.28028698549873526, Train MAPE: 39.607036070988094, Test MAPE: 41.857994861960734\n",
      "Epoch 126, Loss: 0.28010027185938824, Train MAPE: 40.46336382284932, Test MAPE: 42.836692799831525\n",
      "Epoch 127, Loss: 0.280046118851537, Train MAPE: 39.981152424867126, Test MAPE: 42.25082955347629\n",
      "Epoch 128, Loss: 0.2799902679676982, Train MAPE: 40.60919211267055, Test MAPE: 42.951321082843855\n",
      "Epoch 129, Loss: 0.27982083875900027, Train MAPE: 40.20750840614582, Test MAPE: 42.5589895887605\n",
      "Epoch 130, Loss: 0.27980119030622913, Train MAPE: 40.583703999135686, Test MAPE: 42.95983576838516\n",
      "Epoch 131, Loss: 0.279676555028592, Train MAPE: 39.98985576191168, Test MAPE: 42.26128226374813\n",
      "Epoch 132, Loss: 0.2796099189069422, Train MAPE: 40.832186273596754, Test MAPE: 43.22310876718475\n",
      "Epoch 133, Loss: 0.2794261669504574, Train MAPE: 39.57211159344377, Test MAPE: 41.86004593161412\n",
      "Epoch 134, Loss: 0.27930130742747206, Train MAPE: 40.43577322247385, Test MAPE: 42.79346346407729\n",
      "Epoch 135, Loss: 0.27925615274700627, Train MAPE: 40.20885686326301, Test MAPE: 42.57639295869475\n",
      "Epoch 136, Loss: 0.2790957383426099, Train MAPE: 40.57991627660291, Test MAPE: 43.00920215432829\n",
      "Epoch 137, Loss: 0.2790850441817237, Train MAPE: 39.92669521529099, Test MAPE: 42.23401028221478\n",
      "Epoch 138, Loss: 0.27908317996296045, Train MAPE: 40.148665856766975, Test MAPE: 42.51375623043357\n",
      "Epoch 139, Loss: 0.27884133241899395, Train MAPE: 40.02158642801745, Test MAPE: 42.37781163642617\n",
      "Epoch 140, Loss: 0.27873377714348935, Train MAPE: 40.49659337230112, Test MAPE: 42.84060380541927\n",
      "Epoch 141, Loss: 0.2787347524244895, Train MAPE: 40.05040508467576, Test MAPE: 42.40177025168575\n",
      "Epoch 142, Loss: 0.27860597166093604, Train MAPE: 40.82388011011584, Test MAPE: 43.267099283335995\n",
      "Epoch 143, Loss: 0.278478711663649, Train MAPE: 39.8453760475948, Test MAPE: 42.17190939363781\n",
      "Epoch 144, Loss: 0.2785565470013468, Train MAPE: 40.115673665890746, Test MAPE: 42.497771777032845\n",
      "Epoch 145, Loss: 0.2784143506966788, Train MAPE: 40.310972150166826, Test MAPE: 42.677537429748206\n",
      "Epoch 146, Loss: 0.27834136280092014, Train MAPE: 40.33825932864485, Test MAPE: 42.72490541289383\n",
      "Epoch 147, Loss: 0.27832879327323246, Train MAPE: 39.62166912144628, Test MAPE: 41.94611596421965\n",
      "Epoch 148, Loss: 0.2780651855100503, Train MAPE: 40.16862521993703, Test MAPE: 42.49111262467208\n",
      "Epoch 149, Loss: 0.27812879771079824, Train MAPE: 40.11823535787648, Test MAPE: 42.509660273390864\n",
      "Epoch 150, Loss: 0.2781025617201438, Train MAPE: 40.76321172823851, Test MAPE: 43.19510874607608\n",
      "Epoch 151, Loss: 0.2779569509300007, Train MAPE: 39.84267403394326, Test MAPE: 42.227750890696015\n",
      "Epoch 152, Loss: 0.2779106992800003, Train MAPE: 40.82099873115276, Test MAPE: 43.30790961268121\n",
      "Epoch 153, Loss: 0.2779668629768936, Train MAPE: 40.083749909236516, Test MAPE: 42.44320256844283\n",
      "Epoch 154, Loss: 0.27776761977751363, Train MAPE: 39.86937264080705, Test MAPE: 42.22769023905491\n",
      "Epoch 155, Loss: 0.27772169878342373, Train MAPE: 39.92714217175013, Test MAPE: 42.27998896897958\n",
      "Epoch 156, Loss: 0.27743482003951897, Train MAPE: 40.773084436613935, Test MAPE: 43.27712792301945\n",
      "Epoch 157, Loss: 0.27755460500203327, Train MAPE: 40.386209687419324, Test MAPE: 42.82695723730502\n",
      "Epoch 158, Loss: 0.2774931966701801, Train MAPE: 39.801196683686356, Test MAPE: 42.15351377543431\n",
      "Epoch 159, Loss: 0.2773902816933462, Train MAPE: 40.257794656424686, Test MAPE: 42.64345295627379\n",
      "Epoch 160, Loss: 0.27724872662246913, Train MAPE: 40.40511139398334, Test MAPE: 42.89061615358409\n",
      "Epoch 161, Loss: 0.2772578376163354, Train MAPE: 40.80821500098568, Test MAPE: 43.29569653211905\n",
      "Epoch 162, Loss: 0.2772555826552983, Train MAPE: 40.12967461772349, Test MAPE: 42.536460595220404\n",
      "Epoch 163, Loss: 0.27721854961509335, Train MAPE: 40.26666450938959, Test MAPE: 42.66687772612789\n",
      "Epoch 164, Loss: 0.2770057510456135, Train MAPE: 40.31517710521303, Test MAPE: 42.78811494402847\n",
      "Epoch 165, Loss: 0.27689374350439544, Train MAPE: 39.512064262916304, Test MAPE: 41.829296677106186\n",
      "Epoch 166, Loss: 0.27674193327454316, Train MAPE: 39.39230425385223, Test MAPE: 41.72382478228204\n",
      "Epoch 167, Loss: 0.27678682408456146, Train MAPE: 39.11303405542483, Test MAPE: 41.40105928876125\n",
      "Epoch 168, Loss: 0.2776201090893184, Train MAPE: 39.67973410288493, Test MAPE: 42.039906806025364\n",
      "Epoch 169, Loss: 0.27677026569586377, Train MAPE: 39.964997179754846, Test MAPE: 42.38151821054658\n",
      "Epoch 170, Loss: 0.27660613234700826, Train MAPE: 39.09905577692492, Test MAPE: 41.41101949144624\n",
      "Epoch 171, Loss: 0.2766508151616516, Train MAPE: 40.011033825490664, Test MAPE: 42.44525009952985\n",
      "Epoch 172, Loss: 0.2763860431125109, Train MAPE: 39.089829899798865, Test MAPE: 41.37583365964506\n",
      "Epoch 173, Loss: 0.2765447816072867, Train MAPE: 39.094798424600185, Test MAPE: 41.36127886631534\n",
      "Epoch 174, Loss: 0.27647139069934684, Train MAPE: 39.91133133679971, Test MAPE: 42.33142124871466\n",
      "Epoch 175, Loss: 0.2762336705136916, Train MAPE: 39.87348119582253, Test MAPE: 42.29860241355589\n",
      "Epoch 176, Loss: 0.2763072367971656, Train MAPE: 39.39018860301752, Test MAPE: 41.74040219099847\n",
      "Epoch 177, Loss: 0.27613714153225394, Train MAPE: 40.85930907808501, Test MAPE: 43.38938536656766\n",
      "Epoch 178, Loss: 0.276170695461761, Train MAPE: 40.70110583140932, Test MAPE: 43.136370004342005\n",
      "Epoch 179, Loss: 0.2759978398980423, Train MAPE: 40.51932321526538, Test MAPE: 42.980768975879165\n",
      "Epoch 180, Loss: 0.2760968390680935, Train MAPE: 39.18655778512188, Test MAPE: 41.509851026151516\n",
      "Epoch 181, Loss: 0.2759718967580247, Train MAPE: 40.15539181807946, Test MAPE: 42.5781999056205\n",
      "Epoch 182, Loss: 0.27598922474887866, Train MAPE: 39.65480039640405, Test MAPE: 42.06746818297031\n",
      "Epoch 183, Loss: 0.27595454794836455, Train MAPE: 39.40131866630467, Test MAPE: 41.75138542032114\n",
      "Epoch 184, Loss: 0.27580100579679695, Train MAPE: 40.41043203945818, Test MAPE: 42.86117414367103\n",
      "Epoch 185, Loss: 0.2758573678375661, Train MAPE: 40.37501526975084, Test MAPE: 42.877281751453715\n",
      "Epoch 186, Loss: 0.27572788049777347, Train MAPE: 40.64631312030485, Test MAPE: 43.19542854145449\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 26\u001B[0m\n\u001B[0;32m     22\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc4(x)\n\u001B[0;32m     23\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m x\n\u001B[1;32m---> 26\u001B[0m models \u001B[38;5;241m=\u001B[39m \u001B[43mtorchModelRun\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mFullyConnectedModel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\torchModelRun.py:221\u001B[0m, in \u001B[0;36mrun\u001B[1;34m(Model, data, linear_layers)\u001B[0m\n\u001B[0;32m    219\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    220\u001B[0m optimizer_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39moptim, optimizer)(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr)\n\u001B[1;32m--> 221\u001B[0m criterion \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(nn, loss_function)()\n\u001B[0;32m    222\u001B[0m \u001B[38;5;66;03m#criterion = mape_loss\u001B[39;00m\n\u001B[0;32m    223\u001B[0m train_loader, val_loader, test_loader, transform, y_transformer \u001B[38;5;241m=\u001B[39m getDataLoaders(data, y_column, batch_size, train_val_test_split, shufle)\n",
      "File \u001B[1;32mC:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\torchModelRun.py:106\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, device, epochs, train_loader, test_loader, criterion, optimizer, y_transformer)\u001B[0m\n\u001B[0;32m    103\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m    104\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m--> 106\u001B[0m     running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m    107\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m running_loss \u001B[38;5;241m/\u001B[39m (i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    108\u001B[0m endtime_train \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_traintime\n",
      "File \u001B[1;32mC:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\torchModelRun.py:151\u001B[0m, in \u001B[0;36mevaluate\u001B[1;34m(model, device, test_loader, train_loader, train_loss, endtime_train, criterion, y_transformer)\u001B[0m\n\u001B[0;32m    148\u001B[0m outputs = model(inputs)\n\u001B[0;32m    149\u001B[0m outputs = outputs.squeeze()\n\u001B[0;32m    150\u001B[0m #outputs = y_transformer.inverse(outputs)\n\u001B[1;32m--> 151\u001B[0m #labels = y_transformer.inverse(labels)\n\u001B[0;32m    152\u001B[0m outputs = torch.exp(outputs)\n\u001B[0;32m    153\u001B[0m labels = torch.exp(labels)\n",
      "File \u001B[1;32mC:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\torchModelRun.py:73\u001B[0m, in \u001B[0;36mmean_absolute_percentage_error\u001B[1;34m(preds, targets)\u001B[0m\n\u001B[0;32m      0\u001B[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from src import torchModelRun\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "inputs_nums = len(df.columns) - 1\n",
    "\n",
    "class FullyConnectedModel(nn.Module):\n",
    "    import torch.nn.functional as F\n",
    "    def __init__(self):\n",
    "        super(FullyConnectedModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs_nums, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "        self.fc4 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "models = torchModelRun.run(FullyConnectedModel, df, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T16:30:48.025861700Z",
     "start_time": "2024-11-11T16:19:26.329712100Z"
    }
   },
   "id": "1ec75e098d8fb34e",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    " index_nr = 64\n",
    "df_exclude = df.drop(columns=['price_cleaned'])\n",
    "datapoint = df_exclude.iloc[index_nr].values\n",
    "print(datapoint)\n",
    "datapoint = torch.tensor(datapoint, dtype=torch.float32).unsqueeze(0)\n",
    "transformer = models[0]['transformer']\n",
    "\n",
    "datapoint = transformer(datapoint)\n",
    "\n",
    "\n",
    "model = models[0]['model']\n",
    "model.eval()\n",
    "model = model.to('cpu')\n",
    "output = model(datapoint)\n",
    "print(f'output: {output}')\n",
    "transformer = models[0]['transformer']\n",
    "print(f'min: {transformer.}, max: {transformer.}')\n",
    "output = transformer.inverse(output)\n",
    "print(f'output inverst: {output}')\n",
    "output = torch.exp(output)\n",
    "print(f'output exp: {output[0][0]:.0f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-11T16:30:48.018859100Z"
    }
   },
   "id": "93d832963b9862d6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['price_cleaned'].iloc[index_nr]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-11T16:30:48.020856900Z"
    }
   },
   "id": "b1d875d245b45d93",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-11T16:30:48.021882800Z"
    }
   },
   "id": "c185cc492808f7ca",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = torchModelRun.getDataLoaders(df, 'price_cleaned', 1)\n",
    "\n",
    "counter = 5\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    counter -= 1\n",
    "    if counter == 0:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-11T16:30:48.023857500Z"
    }
   },
   "id": "e7803da5caab2ecd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.std().values.reshape(1, -1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-11T16:30:48.024858400Z"
    }
   },
   "id": "c0eae5eb7afd070d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T16:30:48.026862300Z",
     "start_time": "2024-11-11T16:30:48.025861700Z"
    }
   },
   "id": "a77dd87943f775e7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input = 1150000.0\n",
    "log = np.log(input+1)\n",
    "min = 0.6931471824645996\n",
    "max = 17.70733070373535\n",
    "output = (log - min) / (max - min)\n",
    "\n",
    "print(f'output: {output}')\n",
    "\n",
    "output = output * (max - min) + min\n",
    "output = np.exp(output) - 1\n",
    "print(f'output: {output}')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T16:30:48.027861400Z",
     "start_time": "2024-11-11T16:30:48.027861400Z"
    }
   },
   "id": "170efc538d930cca",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
