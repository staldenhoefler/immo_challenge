{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T06:21:51.556303500Z",
     "start_time": "2024-11-15T06:20:17.925254500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:39: DtypeWarning: Columns (3,4,5,6,11,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,45,46,47,49,50,107,110,114,115,116,119,120,121,124,125,126,128,131,132) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(filePath)\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:62: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Space extracted']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:67: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Plot_area_unified']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:73: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Availability']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:77: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['No. of rooms:']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "import os\n",
    "from src.dataPipeline import DataPipeline\n",
    "\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "dp = DataPipeline()\n",
    "df = dp.runPipeline(normalizeAndStandardize=False, imputer=imputer)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input nums: 81\n"
     ]
    }
   ],
   "source": [
    "inputs_nums = len(df.columns) - 1\n",
    "print(f'input nums: {inputs_nums}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-15T06:21:51.560460Z",
     "start_time": "2024-11-15T06:21:51.555382300Z"
    }
   },
   "id": "4cb72ead071d529e",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: denis-schatzmann. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.18.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\wandb\\run-20241115_072342-4llft0p3</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/4llft0p3' target=\"_blank\">MLP-bs16-lr0.0001</a></strong> to <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/4llft0p3' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/4llft0p3</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 71.48512347535456, Train MAPE: 228.30810715848168, Test MAPE: 229.85431636064902\n",
      "Epoch 2, Loss: 0.9901591422230949, Train MAPE: 76.12116872909999, Test MAPE: 76.82599197036919\n",
      "Epoch 3, Loss: 0.540850488902967, Train MAPE: 59.35464150199268, Test MAPE: 59.98922987459263\n",
      "Epoch 4, Loss: 0.4410339267790489, Train MAPE: 56.40339634128179, Test MAPE: 57.27292271318107\n",
      "Epoch 5, Loss: 0.3856259947348018, Train MAPE: 51.69134362111956, Test MAPE: 52.66603867395627\n",
      "Epoch 6, Loss: 0.3478740379428589, Train MAPE: 49.789888305152026, Test MAPE: 50.734631629739226\n",
      "Epoch 7, Loss: 0.31976177349100865, Train MAPE: 46.19609725418164, Test MAPE: 46.98389296696104\n",
      "Epoch 8, Loss: 0.29723573936111974, Train MAPE: 44.59978915769553, Test MAPE: 45.37005914856191\n",
      "Epoch 9, Loss: 0.2791165662134711, Train MAPE: 42.51996424739907, Test MAPE: 43.218195999262434\n",
      "Epoch 10, Loss: 0.2639170162136866, Train MAPE: 40.19170146020467, Test MAPE: 40.8277087010643\n",
      "Epoch 11, Loss: 0.2511614859282742, Train MAPE: 40.747358183161296, Test MAPE: 41.3714178092635\n",
      "Epoch 12, Loss: 0.24005000446123909, Train MAPE: 37.6738754649176, Test MAPE: 38.22528467141805\n",
      "Epoch 13, Loss: 0.23059293787751423, Train MAPE: 37.1011621268575, Test MAPE: 37.60186249451619\n",
      "Epoch 14, Loss: 0.22216640466025897, Train MAPE: 37.23667708933639, Test MAPE: 37.715344608058416\n",
      "Epoch 15, Loss: 0.2146658748709744, Train MAPE: 36.416109170118204, Test MAPE: 36.895110565127084\n",
      "Epoch 16, Loss: 0.20793415381004346, Train MAPE: 37.331453455031664, Test MAPE: 37.80724385140956\n",
      "Epoch 17, Loss: 0.2016750823309929, Train MAPE: 34.803741752244136, Test MAPE: 35.24061668512922\n",
      "Epoch 18, Loss: 0.19628217252023153, Train MAPE: 34.2095878039659, Test MAPE: 34.61892166722323\n",
      "Epoch 19, Loss: 0.1912841896739925, Train MAPE: 35.499164595014186, Test MAPE: 35.94445091919881\n",
      "Epoch 20, Loss: 0.18667963348786246, Train MAPE: 33.0233400048597, Test MAPE: 33.377738294930296\n",
      "Epoch 21, Loss: 0.18235193568345226, Train MAPE: 32.45490657060281, Test MAPE: 32.8008440025008\n",
      "Epoch 22, Loss: 0.1782590351277321, Train MAPE: 32.578767255305785, Test MAPE: 32.93675609018611\n",
      "Epoch 23, Loss: 0.1746382133236953, Train MAPE: 31.896744217877192, Test MAPE: 32.18709130853529\n",
      "Epoch 24, Loss: 0.17124545033431807, Train MAPE: 32.582866816827, Test MAPE: 32.949744546093704\n",
      "Epoch 25, Loss: 0.1679078585478055, Train MAPE: 31.689864429280032, Test MAPE: 32.04529084830448\n",
      "Epoch 26, Loss: 0.1649312017126543, Train MAPE: 30.461122814555182, Test MAPE: 30.792470062372786\n",
      "Epoch 27, Loss: 0.16202174211215595, Train MAPE: 31.53588496141443, Test MAPE: 31.89196382719895\n",
      "Epoch 28, Loss: 0.15946053564798512, Train MAPE: 30.472917906961413, Test MAPE: 30.80253857214332\n",
      "Epoch 29, Loss: 0.15687401727741998, Train MAPE: 30.128439575919483, Test MAPE: 30.45095953813458\n",
      "Epoch 30, Loss: 0.15439487152223047, Train MAPE: 29.61561502271965, Test MAPE: 29.936315825159067\n",
      "Epoch 31, Loss: 0.1521011085096793, Train MAPE: 30.55330413551367, Test MAPE: 30.9274651903759\n",
      "Epoch 32, Loss: 0.14986658514560355, Train MAPE: 29.390737635977317, Test MAPE: 29.740985172461734\n",
      "Epoch 33, Loss: 0.14774560880305268, Train MAPE: 29.112645158374526, Test MAPE: 29.471651767862255\n",
      "Epoch 34, Loss: 0.14567118107358196, Train MAPE: 28.77234757369454, Test MAPE: 29.147924119942033\n",
      "Epoch 35, Loss: 0.14378768841585113, Train MAPE: 29.000835243144635, Test MAPE: 29.375466438088836\n",
      "Epoch 36, Loss: 0.1417643102823488, Train MAPE: 29.43040425253966, Test MAPE: 29.867719533342967\n",
      "Epoch 37, Loss: 0.13989287372839246, Train MAPE: 28.5745860700525, Test MAPE: 28.972485407102155\n",
      "Epoch 38, Loss: 0.13813487790196363, Train MAPE: 28.44055436830164, Test MAPE: 28.86369418001723\n",
      "Epoch 39, Loss: 0.13638834634747568, Train MAPE: 28.066653496695615, Test MAPE: 28.481185248071665\n",
      "Epoch 40, Loss: 0.13466520884100167, Train MAPE: 28.854420938976432, Test MAPE: 29.331175471631045\n",
      "Epoch 41, Loss: 0.13315863929303187, Train MAPE: 27.790325781071516, Test MAPE: 28.216919043968463\n",
      "Epoch 42, Loss: 0.131467459082232, Train MAPE: 27.92397182716962, Test MAPE: 28.39687313430611\n",
      "Epoch 43, Loss: 0.1298768673760036, Train MAPE: 27.963257335176404, Test MAPE: 28.46367826498331\n",
      "Epoch 44, Loss: 0.12836656827668902, Train MAPE: 26.997897175563033, Test MAPE: 27.43414607358618\n",
      "Epoch 45, Loss: 0.12703701344962487, Train MAPE: 27.574364376891037, Test MAPE: 28.076791109253165\n",
      "Epoch 46, Loss: 0.12553307824265408, Train MAPE: 26.786971727702213, Test MAPE: 27.26091361502578\n",
      "Epoch 47, Loss: 0.12411995784648312, Train MAPE: 27.33308820642195, Test MAPE: 27.852333624244192\n",
      "Epoch 48, Loss: 0.12283015589400312, Train MAPE: 27.470614112349278, Test MAPE: 28.015331564278437\n",
      "Epoch 49, Loss: 0.12148656492136697, Train MAPE: 26.54308342316464, Test MAPE: 27.027605137149035\n",
      "Epoch 50, Loss: 0.12007182989937404, Train MAPE: 27.40057181031912, Test MAPE: 27.971079647312674\n",
      "Epoch 51, Loss: 0.11886754968986701, Train MAPE: 26.082525913461627, Test MAPE: 26.586082443880397\n",
      "Epoch 52, Loss: 0.11760068435154047, Train MAPE: 26.591570354033752, Test MAPE: 27.13246534793313\n",
      "Epoch 53, Loss: 0.11642011001791844, Train MAPE: 26.5444008021661, Test MAPE: 27.12325241191177\n",
      "Epoch 54, Loss: 0.11518741780508056, Train MAPE: 25.870699294575793, Test MAPE: 26.40628065642726\n",
      "Epoch 55, Loss: 0.11405071876550224, Train MAPE: 25.735477418250344, Test MAPE: 26.283568535728016\n",
      "Epoch 56, Loss: 0.11297614069893055, Train MAPE: 26.186380981598923, Test MAPE: 26.762397685726942\n",
      "Epoch 57, Loss: 0.11185938931558699, Train MAPE: 25.891844829914096, Test MAPE: 26.50624544867154\n",
      "Epoch 58, Loss: 0.11088524889004482, Train MAPE: 25.461209246890544, Test MAPE: 26.05468508475585\n",
      "Epoch 59, Loss: 0.10980953734407374, Train MAPE: 25.40961316493648, Test MAPE: 26.023646866224734\n",
      "Epoch 60, Loss: 0.1088316326269796, Train MAPE: 25.053722089911926, Test MAPE: 25.665470218293056\n",
      "Epoch 61, Loss: 0.10776388243533679, Train MAPE: 25.67153352058974, Test MAPE: 26.319816910900833\n",
      "Epoch 62, Loss: 0.10683524385023209, Train MAPE: 25.076742278306618, Test MAPE: 25.72911334129129\n",
      "Epoch 63, Loss: 0.10588890472507374, Train MAPE: 24.625127862763886, Test MAPE: 25.247612204131496\n",
      "Epoch 64, Loss: 0.10495233716852141, Train MAPE: 24.09391967173619, Test MAPE: 24.713274813246453\n",
      "Epoch 65, Loss: 0.10408559230121418, Train MAPE: 24.387715739691817, Test MAPE: 25.043317491524064\n",
      "Epoch 66, Loss: 0.10324001716662373, Train MAPE: 24.571485864121755, Test MAPE: 25.244045970083654\n",
      "Epoch 67, Loss: 0.1023723334127596, Train MAPE: 23.930740304326964, Test MAPE: 24.588390821698066\n",
      "Epoch 68, Loss: 0.10147421192538532, Train MAPE: 24.616455242709254, Test MAPE: 25.33225079971255\n",
      "Epoch 69, Loss: 0.10071872592718838, Train MAPE: 25.035223735030293, Test MAPE: 25.77212528798772\n",
      "Epoch 70, Loss: 0.09997568030021049, Train MAPE: 24.059816727130777, Test MAPE: 24.778363289960957\n",
      "Epoch 71, Loss: 0.09916980877654379, Train MAPE: 24.23738999791936, Test MAPE: 24.975516735822303\n",
      "Epoch 72, Loss: 0.09838691858658684, Train MAPE: 23.62214358083811, Test MAPE: 24.364610587956804\n",
      "Epoch 73, Loss: 0.09769686199331787, Train MAPE: 23.992663390341566, Test MAPE: 24.758081918475273\n",
      "Epoch 74, Loss: 0.09697637068552402, Train MAPE: 23.597853878200453, Test MAPE: 24.360279613071018\n",
      "Epoch 75, Loss: 0.09635299794225885, Train MAPE: 23.499855362443203, Test MAPE: 24.272233462425028\n",
      "Epoch 76, Loss: 0.0956335466494382, Train MAPE: 23.82710124753694, Test MAPE: 24.63426347345228\n",
      "Epoch 77, Loss: 0.09500734148009511, Train MAPE: 23.579671123683852, Test MAPE: 24.389288657469766\n",
      "Epoch 78, Loss: 0.09441514309649271, Train MAPE: 23.236852016339252, Test MAPE: 24.037728386363764\n",
      "Epoch 79, Loss: 0.0938076503174817, Train MAPE: 23.378790009627522, Test MAPE: 24.204076913124755\n",
      "Epoch 80, Loss: 0.09322770984087957, Train MAPE: 22.955131715461825, Test MAPE: 23.774637273444984\n",
      "Epoch 81, Loss: 0.09268386496279178, Train MAPE: 23.063083677026707, Test MAPE: 23.91616873540184\n",
      "Epoch 82, Loss: 0.09210752760446769, Train MAPE: 23.452762070689655, Test MAPE: 24.327969795899374\n",
      "Epoch 83, Loss: 0.09148843406341534, Train MAPE: 23.334193409803614, Test MAPE: 24.204217939998\n",
      "Epoch 84, Loss: 0.09100006186793543, Train MAPE: 22.62899311116878, Test MAPE: 23.50149248203555\n",
      "Epoch 85, Loss: 0.09044672481622815, Train MAPE: 22.480182110977537, Test MAPE: 23.348243402795315\n",
      "Epoch 86, Loss: 0.09000431043153927, Train MAPE: 22.51166231863108, Test MAPE: 23.391061041090225\n",
      "Epoch 87, Loss: 0.0894897100841582, Train MAPE: 22.892464719134118, Test MAPE: 23.811597001963648\n",
      "Epoch 88, Loss: 0.08897095129843927, Train MAPE: 22.66069624636569, Test MAPE: 23.598193069984173\n",
      "Epoch 89, Loss: 0.08851522502818877, Train MAPE: 22.51870518425624, Test MAPE: 23.463693943973702\n",
      "Epoch 90, Loss: 0.08807455723585556, Train MAPE: 22.565137706804137, Test MAPE: 23.52220964614459\n",
      "Epoch 91, Loss: 0.0876530586640964, Train MAPE: 22.319518556530845, Test MAPE: 23.2788028717041\n",
      "Epoch 92, Loss: 0.08722162078655292, Train MAPE: 22.251619418538315, Test MAPE: 23.216412982721437\n",
      "Epoch 93, Loss: 0.08679977678341125, Train MAPE: 22.422437024139374, Test MAPE: 23.41692143655828\n",
      "Epoch 94, Loss: 0.08638023861888386, Train MAPE: 22.52476300946361, Test MAPE: 23.524154717894806\n",
      "Epoch 95, Loss: 0.0859127210619237, Train MAPE: 21.914406972100615, Test MAPE: 22.9058965412593\n",
      "Epoch 96, Loss: 0.08553530337076459, Train MAPE: 21.787824766062137, Test MAPE: 22.775802648843932\n",
      "Epoch 97, Loss: 0.08518683420063808, Train MAPE: 22.510038624559588, Test MAPE: 23.537736589424455\n",
      "Epoch 98, Loss: 0.08481612925847219, Train MAPE: 22.617875761176574, Test MAPE: 23.680696966090878\n",
      "Epoch 99, Loss: 0.08443937118835783, Train MAPE: 22.266911541398397, Test MAPE: 23.315965926510163\n",
      "Epoch 100, Loss: 0.08408149361453257, Train MAPE: 22.13962974438617, Test MAPE: 23.2024553744729\n",
      "Epoch 101, Loss: 0.08370435790963335, Train MAPE: 21.836020865582093, Test MAPE: 22.886995224203645\n",
      "Epoch 102, Loss: 0.0833466748956123, Train MAPE: 22.045848540125963, Test MAPE: 23.128248116065716\n",
      "Epoch 103, Loss: 0.0830827675177604, Train MAPE: 22.095024730649456, Test MAPE: 23.190314468296094\n",
      "Epoch 104, Loss: 0.08272312285980409, Train MAPE: 22.07728806438062, Test MAPE: 23.165743908206164\n",
      "Epoch 105, Loss: 0.08238429648932051, Train MAPE: 22.17719042129736, Test MAPE: 23.278763281431235\n",
      "Epoch 106, Loss: 0.08207901767236274, Train MAPE: 22.052760298848725, Test MAPE: 23.17193667093913\n",
      "Epoch 107, Loss: 0.0817066515069937, Train MAPE: 22.04848277420234, Test MAPE: 23.166450679530588\n",
      "Epoch 108, Loss: 0.08146304985613688, Train MAPE: 22.002184106199532, Test MAPE: 23.13526762308289\n",
      "Epoch 109, Loss: 0.08112517631793982, Train MAPE: 21.720391541405128, Test MAPE: 22.848984407739163\n",
      "Epoch 110, Loss: 0.08086805327501673, Train MAPE: 21.389371292245926, Test MAPE: 22.50896625591877\n",
      "Epoch 111, Loss: 0.08053539669782295, Train MAPE: 21.516829684505296, Test MAPE: 22.65792728292531\n",
      "Epoch 112, Loss: 0.08031184411467493, Train MAPE: 21.662098605703644, Test MAPE: 22.802738379701346\n",
      "Epoch 113, Loss: 0.08000227219352815, Train MAPE: 21.60710937430509, Test MAPE: 22.760202290911327\n",
      "Epoch 114, Loss: 0.07972580925065913, Train MAPE: 21.73044532143144, Test MAPE: 22.902467588811998\n",
      "Epoch 115, Loss: 0.07943180036281922, Train MAPE: 21.63446543886472, Test MAPE: 22.805794346834965\n",
      "Epoch 116, Loss: 0.07925394020192432, Train MAPE: 21.36066717429449, Test MAPE: 22.52625949172681\n",
      "Epoch 117, Loss: 0.07895421028076488, Train MAPE: 21.706043034934815, Test MAPE: 22.90295480311602\n",
      "Epoch 118, Loss: 0.07873402962645057, Train MAPE: 21.07923630031392, Test MAPE: 22.24372127138335\n",
      "Epoch 119, Loss: 0.07846341084256725, Train MAPE: 21.179462188728017, Test MAPE: 22.370422129430075\n",
      "Epoch 120, Loss: 0.07821677375874149, Train MAPE: 21.28919298331095, Test MAPE: 22.48597894865891\n",
      "Epoch 121, Loss: 0.07799793301062696, Train MAPE: 21.513573394182888, Test MAPE: 22.73449675972891\n",
      "Epoch 122, Loss: 0.07778253017505943, Train MAPE: 21.309574615006706, Test MAPE: 22.533411431586604\n",
      "Epoch 123, Loss: 0.0775495772075876, Train MAPE: 21.403182848987964, Test MAPE: 22.626252726120054\n",
      "Epoch 124, Loss: 0.07734369281197212, Train MAPE: 21.21543564892455, Test MAPE: 22.4383187166119\n",
      "Epoch 125, Loss: 0.07707999585855356, Train MAPE: 21.357323309147688, Test MAPE: 22.582195340445214\n",
      "Epoch 126, Loss: 0.07688613228092715, Train MAPE: 20.852287118752646, Test MAPE: 22.08844335051789\n",
      "Epoch 127, Loss: 0.07662908593951684, Train MAPE: 20.918103446667853, Test MAPE: 22.162210482747163\n",
      "Epoch 128, Loss: 0.07642177643463344, Train MAPE: 21.393697297013045, Test MAPE: 22.65788741130025\n",
      "Epoch 129, Loss: 0.0762168920187285, Train MAPE: 21.225261660801714, Test MAPE: 22.485525935088994\n",
      "Epoch 130, Loss: 0.07598376697713326, Train MAPE: 21.07727093938899, Test MAPE: 22.34449837125581\n",
      "Epoch 131, Loss: 0.07586975939293264, Train MAPE: 21.189765117312437, Test MAPE: 22.461290903931832\n",
      "Epoch 132, Loss: 0.07564699357645166, Train MAPE: 20.873464977524083, Test MAPE: 22.147030505184013\n",
      "Epoch 133, Loss: 0.07544482009233558, Train MAPE: 20.686986222820337, Test MAPE: 21.950793470916164\n",
      "Epoch 134, Loss: 0.07522155032854809, Train MAPE: 20.543987953537147, Test MAPE: 21.809466577581063\n",
      "Epoch 135, Loss: 0.07504924172794487, Train MAPE: 20.760132645600596, Test MAPE: 22.049311747496155\n",
      "Epoch 136, Loss: 0.07482712313332814, Train MAPE: 20.760291372026717, Test MAPE: 22.050803659519474\n",
      "Epoch 137, Loss: 0.07466623759520448, Train MAPE: 20.566619367407473, Test MAPE: 21.86143150914218\n",
      "Epoch 138, Loss: 0.07451633224665605, Train MAPE: 21.078452930011547, Test MAPE: 22.394979480582634\n",
      "Epoch 139, Loss: 0.07431114251231587, Train MAPE: 20.384928047714162, Test MAPE: 21.68326054130934\n",
      "Epoch 140, Loss: 0.07413569165881248, Train MAPE: 20.857243425436923, Test MAPE: 22.18816837953882\n",
      "Epoch 141, Loss: 0.07395337217988326, Train MAPE: 20.43289616329672, Test MAPE: 21.733294337188603\n",
      "Epoch 142, Loss: 0.07378244842648192, Train MAPE: 20.507418243448434, Test MAPE: 21.827098630853996\n",
      "Epoch 143, Loss: 0.07364610175236931, Train MAPE: 20.672475742585135, Test MAPE: 22.019039417135303\n",
      "Epoch 144, Loss: 0.0734348551656833, Train MAPE: 20.942560400738813, Test MAPE: 22.296024256739123\n",
      "Epoch 145, Loss: 0.07330409601805794, Train MAPE: 20.804381702456013, Test MAPE: 22.168687250422334\n",
      "Epoch 146, Loss: 0.07316588171805083, Train MAPE: 20.958752643113396, Test MAPE: 22.330991821727533\n",
      "Epoch 147, Loss: 0.07298818249737599, Train MAPE: 20.401100072284674, Test MAPE: 21.745454214541848\n",
      "Epoch 148, Loss: 0.07288875118208726, Train MAPE: 20.350926354427465, Test MAPE: 21.706977219417176\n",
      "Epoch 149, Loss: 0.0727042053439516, Train MAPE: 20.250685015209342, Test MAPE: 21.598101308976098\n",
      "Epoch 150, Loss: 0.07256720533321254, Train MAPE: 20.572683281317882, Test MAPE: 21.976287385056303\n",
      "Epoch 151, Loss: 0.07240130792685345, Train MAPE: 20.193500217975387, Test MAPE: 21.552166313960633\n",
      "Epoch 152, Loss: 0.07225387437181585, Train MAPE: 20.144995687563377, Test MAPE: 21.512092904569545\n",
      "Epoch 153, Loss: 0.07214070345281355, Train MAPE: 20.28988149959312, Test MAPE: 21.677920769000874\n",
      "Epoch 154, Loss: 0.07195697067267942, Train MAPE: 20.53000783966006, Test MAPE: 21.951034472819945\n",
      "Epoch 155, Loss: 0.07181762233340785, Train MAPE: 20.1759305599209, Test MAPE: 21.566157216769984\n",
      "Epoch 156, Loss: 0.07163090512070766, Train MAPE: 20.311152586200894, Test MAPE: 21.7180889502339\n",
      "Epoch 157, Loss: 0.07150070515237142, Train MAPE: 20.442489078152327, Test MAPE: 21.861178887758218\n",
      "Epoch 158, Loss: 0.07136844333050521, Train MAPE: 20.381494522094727, Test MAPE: 21.80753017293996\n",
      "Epoch 159, Loss: 0.0712935297391873, Train MAPE: 20.329747342651896, Test MAPE: 21.75540934668647\n",
      "Epoch 160, Loss: 0.07109698627683426, Train MAPE: 20.515850093655025, Test MAPE: 21.961648992194984\n",
      "Epoch 161, Loss: 0.07102955029946989, Train MAPE: 20.128064436286202, Test MAPE: 21.568406631206646\n",
      "Epoch 162, Loss: 0.07087867310733204, Train MAPE: 20.102303085254, Test MAPE: 21.548730890412898\n",
      "Epoch 163, Loss: 0.07069558687133078, Train MAPE: 20.269437681108514, Test MAPE: 21.722619341707777\n",
      "Epoch 164, Loss: 0.07056664872810973, Train MAPE: 19.837329745635547, Test MAPE: 21.241084065930597\n",
      "Epoch 165, Loss: 0.07046514110370028, Train MAPE: 20.348949198297664, Test MAPE: 21.814564142190633\n",
      "Epoch 166, Loss: 0.07036065148977225, Train MAPE: 19.877654872941832, Test MAPE: 21.30381058367733\n",
      "Epoch 167, Loss: 0.0702476741965728, Train MAPE: 19.87970047600546, Test MAPE: 21.309773046851614\n",
      "Epoch 168, Loss: 0.07008998209123043, Train MAPE: 20.060513238093083, Test MAPE: 21.527060435649535\n",
      "Epoch 169, Loss: 0.0699868030017483, Train MAPE: 20.49565522622742, Test MAPE: 21.999528461032444\n",
      "Epoch 170, Loss: 0.06983838595310628, Train MAPE: 19.943902407945174, Test MAPE: 21.419002101795883\n",
      "Epoch 171, Loss: 0.06975200283586665, Train MAPE: 19.908990107926748, Test MAPE: 21.37430636453446\n",
      "Epoch 172, Loss: 0.06960135683406606, Train MAPE: 19.901124838100298, Test MAPE: 21.381214079728984\n",
      "Epoch 173, Loss: 0.06948638346210508, Train MAPE: 19.970532732750517, Test MAPE: 21.46248318683142\n",
      "Epoch 174, Loss: 0.06941058323651637, Train MAPE: 19.929855686820478, Test MAPE: 21.41606770804102\n",
      "Epoch 175, Loss: 0.06927394075988866, Train MAPE: 19.721065653821995, Test MAPE: 21.196239613938605\n",
      "Epoch 176, Loss: 0.06914395965592916, Train MAPE: 19.642876824390854, Test MAPE: 21.119400634619467\n",
      "Epoch 177, Loss: 0.06903221194158207, Train MAPE: 19.84328725125394, Test MAPE: 21.340022953077295\n",
      "Epoch 178, Loss: 0.06894285175574306, Train MAPE: 19.909400385841085, Test MAPE: 21.409477818514652\n",
      "Epoch 179, Loss: 0.06881016392925127, Train MAPE: 19.840117446299598, Test MAPE: 21.348546426414988\n",
      "Epoch 180, Loss: 0.06870723399847088, Train MAPE: 19.79119904683626, Test MAPE: 21.29115448144204\n",
      "Epoch 181, Loss: 0.06863864870029408, Train MAPE: 19.635728679704528, Test MAPE: 21.12648847733421\n",
      "Epoch 182, Loss: 0.06855392984488723, Train MAPE: 19.835830564124016, Test MAPE: 21.355088087790772\n",
      "Epoch 183, Loss: 0.06842611487847704, Train MAPE: 19.66119771904365, Test MAPE: 21.168577344024776\n",
      "Epoch 184, Loss: 0.06827487317043378, Train MAPE: 19.577329051346016, Test MAPE: 21.10210540011468\n",
      "Epoch 185, Loss: 0.06818642153575202, Train MAPE: 19.846369150843852, Test MAPE: 21.39183069280281\n",
      "Epoch 186, Loss: 0.06809741822504443, Train MAPE: 19.855548481927507, Test MAPE: 21.405402304112226\n",
      "Epoch 187, Loss: 0.06800364120751762, Train MAPE: 19.43259786713729, Test MAPE: 20.937573385421345\n",
      "Epoch 188, Loss: 0.06789840706057082, Train MAPE: 19.736108711330456, Test MAPE: 21.295018744194646\n",
      "Epoch 189, Loss: 0.06780374737515374, Train MAPE: 19.728715881976733, Test MAPE: 21.27750047778718\n",
      "Epoch 190, Loss: 0.0676193132742968, Train MAPE: 19.80558637141724, Test MAPE: 21.376618922441857\n",
      "Epoch 191, Loss: 0.06757942949486888, Train MAPE: 19.240957265160798, Test MAPE: 20.73439756663823\n",
      "Epoch 192, Loss: 0.06748194191229824, Train MAPE: 19.70586882981678, Test MAPE: 21.266991699335676\n",
      "Epoch 193, Loss: 0.06741974947836575, Train MAPE: 19.476317850078168, Test MAPE: 21.029957570335416\n",
      "Epoch 194, Loss: 0.06732279827509322, Train MAPE: 19.55276725710524, Test MAPE: 21.13150997088787\n",
      "Epoch 195, Loss: 0.06720349293252166, Train MAPE: 19.378786870640052, Test MAPE: 20.93913281466312\n",
      "Epoch 196, Loss: 0.06709225573295714, Train MAPE: 19.329149239815322, Test MAPE: 20.88026645631169\n",
      "Epoch 197, Loss: 0.0670122227159151, Train MAPE: 19.498307823334763, Test MAPE: 21.087197639933034\n",
      "Epoch 198, Loss: 0.0669033065409697, Train MAPE: 19.429433005653888, Test MAPE: 20.99631061407798\n",
      "Epoch 199, Loss: 0.06683413683207329, Train MAPE: 19.49393681848952, Test MAPE: 21.066666464239244\n",
      "Epoch 200, Loss: 0.06668551950889604, Train MAPE: 19.77132931103071, Test MAPE: 21.388004891260373\n",
      "Epoch 201, Loss: 0.06664947107834304, Train MAPE: 19.636953264274855, Test MAPE: 21.238857291210657\n",
      "Epoch 202, Loss: 0.0665484859861705, Train MAPE: 19.719220376311874, Test MAPE: 21.34043715466028\n",
      "Epoch 203, Loss: 0.06647213256256235, Train MAPE: 19.66707654607376, Test MAPE: 21.283358526412556\n",
      "Epoch 204, Loss: 0.0663862793017911, Train MAPE: 19.453439998260965, Test MAPE: 21.07063846661213\n",
      "Epoch 205, Loss: 0.06626679330585615, Train MAPE: 19.291310878636626, Test MAPE: 20.88837710924989\n",
      "Epoch 206, Loss: 0.06622787363548635, Train MAPE: 19.333531850593545, Test MAPE: 20.929151381569348\n",
      "Epoch 207, Loss: 0.0660959026657495, Train MAPE: 19.307043775044605, Test MAPE: 20.914546838665373\n",
      "Epoch 208, Loss: 0.06606541349876704, Train MAPE: 19.69667506469405, Test MAPE: 21.32513628700227\n",
      "Epoch 209, Loss: 0.06594381735625311, Train MAPE: 19.382102555762316, Test MAPE: 20.994223437546772\n",
      "Epoch 210, Loss: 0.06584129866734076, Train MAPE: 19.16826776758708, Test MAPE: 20.76024607223569\n",
      "Epoch 211, Loss: 0.06573432057467311, Train MAPE: 19.59455663565821, Test MAPE: 21.245775880484747\n",
      "Epoch 212, Loss: 0.06568771589204511, Train MAPE: 19.45707857483071, Test MAPE: 21.09720429241429\n",
      "Epoch 213, Loss: 0.06559842160866307, Train MAPE: 19.508716295229508, Test MAPE: 21.157655507668682\n",
      "Epoch 214, Loss: 0.06554612142595526, Train MAPE: 19.242469890919992, Test MAPE: 20.8694332933974\n",
      "Epoch 215, Loss: 0.0654231835047742, Train MAPE: 19.26425519177006, Test MAPE: 20.8805067932012\n",
      "Epoch 216, Loss: 0.06536795122304276, Train MAPE: 19.435580911023735, Test MAPE: 21.091117584842376\n",
      "Epoch 217, Loss: 0.06525605199153134, Train MAPE: 19.462950937814245, Test MAPE: 21.130806784063463\n",
      "Epoch 218, Loss: 0.06520456696430994, Train MAPE: 19.267082736453297, Test MAPE: 20.91138837529325\n",
      "Epoch 219, Loss: 0.06513811716022308, Train MAPE: 19.180700501453842, Test MAPE: 20.825719840681874\n",
      "Epoch 220, Loss: 0.065073417955947, Train MAPE: 19.320136198718618, Test MAPE: 20.994712500736632\n",
      "Epoch 221, Loss: 0.06493969648283543, Train MAPE: 19.395282046335428, Test MAPE: 21.08296952667821\n",
      "Epoch 222, Loss: 0.06487700002301888, Train MAPE: 19.162928260298496, Test MAPE: 20.819717502228603\n",
      "Epoch 223, Loss: 0.06477941220317282, Train MAPE: 19.075375862341026, Test MAPE: 20.742509553258902\n",
      "Epoch 224, Loss: 0.06471591263702309, Train MAPE: 19.85161005479149, Test MAPE: 21.55204405729798\n",
      "Epoch 225, Loss: 0.06462468737102367, Train MAPE: 19.185141037660273, Test MAPE: 20.87730064245933\n",
      "Epoch 226, Loss: 0.06459221473072628, Train MAPE: 19.542684166452, Test MAPE: 21.248815752080574\n",
      "Epoch 227, Loss: 0.06447894174216291, Train MAPE: 19.01124790096557, Test MAPE: 20.677973856871155\n",
      "Epoch 228, Loss: 0.06443488036844011, Train MAPE: 19.233880871437197, Test MAPE: 20.93222717489776\n",
      "Epoch 229, Loss: 0.06429057882175435, Train MAPE: 19.18974749033883, Test MAPE: 20.887916882832844\n",
      "Epoch 230, Loss: 0.06425872980416078, Train MAPE: 19.450834843022026, Test MAPE: 21.168796436996754\n",
      "Epoch 231, Loss: 0.06418405132670016, Train MAPE: 18.857695790745268, Test MAPE: 20.530578616935173\n",
      "Epoch 232, Loss: 0.06412550794141061, Train MAPE: 18.821726370177814, Test MAPE: 20.483252218399926\n",
      "Epoch 233, Loss: 0.06406019084166274, Train MAPE: 18.797835307519023, Test MAPE: 20.47548608304897\n",
      "Epoch 234, Loss: 0.06396814090693557, Train MAPE: 18.827609827511605, Test MAPE: 20.49715985922978\n",
      "Epoch 235, Loss: 0.06382222707389586, Train MAPE: 18.828143687627687, Test MAPE: 20.49715983730623\n",
      "Epoch 236, Loss: 0.0638207588300166, Train MAPE: 18.828154803465335, Test MAPE: 20.49715989942295\n",
      "Epoch 237, Loss: 0.06381450097629317, Train MAPE: 18.82720280425089, Test MAPE: 20.49715992500042\n",
      "Epoch 238, Loss: 0.06381497143259698, Train MAPE: 18.830328730128755, Test MAPE: 20.497160027310308\n",
      "Epoch 239, Loss: 0.06383380973988417, Train MAPE: 18.827322277332428, Test MAPE: 20.497160045579932\n",
      "Epoch 240, Loss: 0.06382097119299386, Train MAPE: 18.82745063796368, Test MAPE: 20.49716016250552\n",
      "Epoch 241, Loss: 0.06382497632295636, Train MAPE: 18.83008341821248, Test MAPE: 20.497160085773103\n",
      "Epoch 242, Loss: 0.06382413901752959, Train MAPE: 18.828658811655163, Test MAPE: 20.49716018808299\n",
      "Epoch 243, Loss: 0.06382869412695716, Train MAPE: 18.830243531260944, Test MAPE: 20.497160231930085\n",
      "Epoch 244, Loss: 0.0638191715036434, Train MAPE: 18.82811692074184, Test MAPE: 20.497160246545786\n",
      "Epoch 245, Loss: 0.06382199899005067, Train MAPE: 18.82869367517195, Test MAPE: 20.4971603086625\n",
      "Epoch 246, Loss: 0.06381715249095186, Train MAPE: 18.828886739816785, Test MAPE: 20.497160348855672\n",
      "Epoch 247, Loss: 0.06382280031767382, Train MAPE: 18.827639665722504, Test MAPE: 20.49716052424405\n",
      "Epoch 248, Loss: 0.06382277366916862, Train MAPE: 18.82766487088665, Test MAPE: 20.49716052059013\n",
      "Epoch 249, Loss: 0.06383646908283148, Train MAPE: 18.827478011982553, Test MAPE: 20.497160619246092\n",
      "Epoch 250, Loss: 0.0638206500902929, Train MAPE: 18.82810627945546, Test MAPE: 20.49716069963243\n",
      "Epoch 251, Loss: 0.06381682801923838, Train MAPE: 18.828461295920615, Test MAPE: 20.49716058636077\n",
      "Epoch 252, Loss: 0.06382209309113362, Train MAPE: 18.82914861576669, Test MAPE: 20.497160608284318\n",
      "Epoch 253, Loss: 0.06382480673831267, Train MAPE: 18.82874670787603, Test MAPE: 20.497160600976468\n",
      "Epoch 254, Loss: 0.06382267510077326, Train MAPE: 18.82993009975064, Test MAPE: 20.497160703286358\n",
      "Epoch 255, Loss: 0.06381683517426023, Train MAPE: 18.82827364335467, Test MAPE: 20.497160659439263\n",
      "Epoch 256, Loss: 0.06381694304695208, Train MAPE: 18.829033462107468, Test MAPE: 20.49716064116964\n",
      "Epoch 257, Loss: 0.06382703247693504, Train MAPE: 18.829734960887027, Test MAPE: 20.49716058636077\n",
      "Epoch 258, Loss: 0.06382148771270867, Train MAPE: 18.828229019991618, Test MAPE: 20.49716052789798\n",
      "Epoch 259, Loss: 0.06382378885095666, Train MAPE: 18.828081846008594, Test MAPE: 20.497160374433143\n",
      "Epoch 260, Loss: 0.06382633534761026, Train MAPE: 18.828853775876595, Test MAPE: 20.49716040366454\n",
      "Epoch 261, Loss: 0.06382916198037958, Train MAPE: 18.831179079364837, Test MAPE: 20.497160451165563\n",
      "Epoch 262, Loss: 0.06382238927582137, Train MAPE: 18.829114729240324, Test MAPE: 20.497160279431107\n",
      "Epoch 263, Loss: 0.06382625597892534, Train MAPE: 18.828677697698314, Test MAPE: 20.49716022096831\n",
      "Epoch 264, Loss: 0.06383078552207491, Train MAPE: 18.828075930119017, Test MAPE: 20.497160115004498\n",
      "Epoch 265, Loss: 0.06382282227563492, Train MAPE: 18.82916402039569, Test MAPE: 20.497159957885742\n",
      "Epoch 266, Loss: 0.06381399951221588, Train MAPE: 18.82878762900772, Test MAPE: 20.497159910384724\n",
      "Epoch 267, Loss: 0.0638181361426043, Train MAPE: 18.82907914063846, Test MAPE: 20.497159895769023\n",
      "Epoch 268, Loss: 0.0638256734504953, Train MAPE: 18.828164507535824, Test MAPE: 20.497159976155366\n",
      "Epoch 269, Loss: 0.06381941942653457, Train MAPE: 18.828466768347063, Test MAPE: 20.497160115004498\n",
      "Epoch 270, Loss: 0.06381421426912968, Train MAPE: 18.829668072474654, Test MAPE: 20.497160173467293\n",
      "Epoch 271, Loss: 0.06381464908337033, Train MAPE: 18.829306850725946, Test MAPE: 20.4971603378939\n",
      "Epoch 272, Loss: 0.06382763802010498, Train MAPE: 18.827347668568216, Test MAPE: 20.49716025750756\n",
      "Epoch 273, Loss: 0.06381915453178513, Train MAPE: 18.83087861435068, Test MAPE: 20.497160272123256\n",
      "Epoch 274, Loss: 0.06381834456241714, Train MAPE: 18.832304636332584, Test MAPE: 20.49716031597035\n",
      "Epoch 275, Loss: 0.06382248742900581, Train MAPE: 18.828215557456815, Test MAPE: 20.497160538859752\n",
      "Epoch 276, Loss: 0.06382253057968983, Train MAPE: 18.82803593934554, Test MAPE: 20.497160513282278\n",
      "Epoch 277, Loss: 0.06383042151781651, Train MAPE: 18.828014286915383, Test MAPE: 20.49716049866658\n",
      "Epoch 278, Loss: 0.06381932069830532, Train MAPE: 18.827268434965255, Test MAPE: 20.4971605607833\n",
      "Epoch 279, Loss: 0.06382092217399542, Train MAPE: 18.829079235731584, Test MAPE: 20.49716057905292\n",
      "Epoch 280, Loss: 0.06381637336327298, Train MAPE: 18.82867156922074, Test MAPE: 20.497160663093187\n",
      "Epoch 281, Loss: 0.06381490969117856, Train MAPE: 18.828945179113607, Test MAPE: 20.49716064847749\n",
      "Epoch 282, Loss: 0.06382076681635324, Train MAPE: 18.830627824064642, Test MAPE: 20.49716055347545\n",
      "Epoch 283, Loss: 0.06382303792279667, Train MAPE: 18.828837629246916, Test MAPE: 20.49716052059013\n",
      "Epoch 284, Loss: 0.06381326716050348, Train MAPE: 18.829859832789275, Test MAPE: 20.497160480396957\n",
      "Epoch 285, Loss: 0.06383130882267526, Train MAPE: 18.82905891597671, Test MAPE: 20.4971605607833\n",
      "Epoch 286, Loss: 0.0638228888567162, Train MAPE: 18.82696277366046, Test MAPE: 20.497160582706847\n",
      "Epoch 287, Loss: 0.06381871747987011, Train MAPE: 18.829053804721266, Test MAPE: 20.49716052059013\n",
      "Epoch 288, Loss: 0.06381790780695379, Train MAPE: 18.829959039605818, Test MAPE: 20.49716049866658\n",
      "Epoch 289, Loss: 0.06386853666388548, Train MAPE: 18.82818498318856, Test MAPE: 20.49716052059013\n",
      "Epoch 290, Loss: 0.06381923841689342, Train MAPE: 18.829476351020205, Test MAPE: 20.497160509628355\n",
      "Epoch 291, Loss: 0.0638269319949977, Train MAPE: 18.828121225534442, Test MAPE: 20.497160531551902\n",
      "Epoch 292, Loss: 0.0638302963794935, Train MAPE: 18.826758232006977, Test MAPE: 20.497160549821526\n",
      "Epoch 293, Loss: 0.06382161745942651, Train MAPE: 18.827013574419176, Test MAPE: 20.49716057905292\n",
      "Epoch 294, Loss: 0.06381648754270627, Train MAPE: 18.828263279118453, Test MAPE: 20.497160549821526\n",
      "Epoch 295, Loss: 0.06381926613333898, Train MAPE: 18.829625298855746, Test MAPE: 20.497160538859752\n",
      "Epoch 296, Loss: 0.06382054439187765, Train MAPE: 18.82879299536876, Test MAPE: 20.497160443857712\n",
      "Epoch 297, Loss: 0.06391243884752461, Train MAPE: 18.827295405752707, Test MAPE: 20.49716055347545\n",
      "Epoch 298, Loss: 0.06382312043547658, Train MAPE: 18.832304650047938, Test MAPE: 20.497160619246092\n",
      "Epoch 299, Loss: 0.06383812635661971, Train MAPE: 18.8284771874447, Test MAPE: 20.49716059366862\n",
      "Epoch 300, Loss: 0.06381893653711902, Train MAPE: 18.828860446567845, Test MAPE: 20.497160381740994\n",
      "Epoch 301, Loss: 0.06381906476318265, Train MAPE: 18.828028756614387, Test MAPE: 20.497160294046804\n",
      "Epoch 302, Loss: 0.06381876239676318, Train MAPE: 18.829449995594995, Test MAPE: 20.49716031597035\n",
      "Epoch 303, Loss: 0.06381629272038161, Train MAPE: 18.82816701013084, Test MAPE: 20.497160272123256\n",
      "Epoch 304, Loss: 0.06384615980853056, Train MAPE: 18.828738934013096, Test MAPE: 20.497160166159443\n",
      "Epoch 305, Loss: 0.06383782968885861, Train MAPE: 18.83056830811249, Test MAPE: 20.497160213660464\n",
      "Epoch 306, Loss: 0.06382456786166216, Train MAPE: 18.828673001560933, Test MAPE: 20.497160231930085\n",
      "Epoch 307, Loss: 0.06381139384755867, Train MAPE: 18.827268571204446, Test MAPE: 20.497160210006538\n",
      "Epoch 308, Loss: 0.06382165454981682, Train MAPE: 18.82846295867876, Test MAPE: 20.497160268469333\n",
      "Epoch 309, Loss: 0.06382445707624333, Train MAPE: 18.828114173099156, Test MAPE: 20.49716034520175\n",
      "Epoch 310, Loss: 0.06385883014320556, Train MAPE: 18.831596492463767, Test MAPE: 20.497160469435183\n",
      "Epoch 311, Loss: 0.06382980061853921, Train MAPE: 18.82975241138974, Test MAPE: 20.49716039635669\n",
      "Epoch 312, Loss: 0.06381739970128751, Train MAPE: 18.828157386523767, Test MAPE: 20.4971603378939\n",
      "Epoch 313, Loss: 0.06382191782301133, Train MAPE: 18.82852059379863, Test MAPE: 20.497160301354654\n",
      "Epoch 314, Loss: 0.0638313682389125, Train MAPE: 18.826563827829187, Test MAPE: 20.497160268469333\n",
      "Epoch 315, Loss: 0.06381741996715247, Train MAPE: 18.827786536139016, Test MAPE: 20.49716033058605\n",
      "Epoch 316, Loss: 0.06381905146482493, Train MAPE: 18.827172248269772, Test MAPE: 20.49716041097239\n",
      "Epoch 317, Loss: 0.063822115545562, Train MAPE: 18.828357617898504, Test MAPE: 20.49716031597035\n",
      "Epoch 318, Loss: 0.06381711840347412, Train MAPE: 18.828254169379992, Test MAPE: 20.497160286738954\n",
      "Epoch 319, Loss: 0.06382340805967582, Train MAPE: 18.82752733651195, Test MAPE: 20.497160367125296\n",
      "Epoch 320, Loss: 0.06381391764333613, Train MAPE: 18.832853893305632, Test MAPE: 20.497160199044764\n",
      "Epoch 321, Loss: 0.06381649106083757, Train MAPE: 18.828433188587457, Test MAPE: 20.497160096734877\n",
      "Epoch 322, Loss: 0.0638305126365242, Train MAPE: 18.829354533984585, Test MAPE: 20.49716007481133\n",
      "Epoch 323, Loss: 0.0638403528908398, Train MAPE: 18.828857478565133, Test MAPE: 20.497159950577892\n",
      "Epoch 324, Loss: 0.06382234653231196, Train MAPE: 18.829694244114116, Test MAPE: 20.49715998711714\n",
      "Epoch 325, Loss: 0.06382970024375308, Train MAPE: 18.829671667269068, Test MAPE: 20.497159968847516\n",
      "Epoch 326, Loss: 0.06382211592327, Train MAPE: 18.828497355416317, Test MAPE: 20.497159957885742\n",
      "Epoch 327, Loss: 0.06382291141650992, Train MAPE: 18.827928799110772, Test MAPE: 20.49715994692397\n",
      "Epoch 328, Loss: 0.06381498958758317, Train MAPE: 18.82839911736098, Test MAPE: 20.49715997250144\n",
      "Epoch 329, Loss: 0.06382254403788143, Train MAPE: 18.82818138016493, Test MAPE: 20.497160082119176\n",
      "Epoch 330, Loss: 0.06382481677302321, Train MAPE: 18.82978570632715, Test MAPE: 20.49715997980929\n",
      "Epoch 331, Loss: 0.06392883653069417, Train MAPE: 18.828165683398883, Test MAPE: 20.497159994424987\n",
      "Epoch 332, Loss: 0.0638193949603423, Train MAPE: 18.828947901154297, Test MAPE: 20.497160125966275\n",
      "Epoch 333, Loss: 0.06381535063178947, Train MAPE: 18.827928263754767, Test MAPE: 20.49716015519767\n",
      "Epoch 334, Loss: 0.06382327120927941, Train MAPE: 18.827760671723496, Test MAPE: 20.497160184429067\n",
      "Epoch 335, Loss: 0.0638176617645663, Train MAPE: 18.827336165957576, Test MAPE: 20.497160246545786\n",
      "Epoch 336, Loss: 0.06382016925639468, Train MAPE: 18.8279968336696, Test MAPE: 20.497160319624275\n",
      "Epoch 337, Loss: 0.06382209146511406, Train MAPE: 18.828830898208107, Test MAPE: 20.497160294046804\n",
      "Epoch 338, Loss: 0.06381967157564793, Train MAPE: 18.83122776978768, Test MAPE: 20.49716034520175\n",
      "Epoch 339, Loss: 0.0638250504965936, Train MAPE: 18.82805288375167, Test MAPE: 20.497160272123256\n",
      "Epoch 340, Loss: 0.0638408523038644, Train MAPE: 18.828409000188255, Test MAPE: 20.49716031597035\n",
      "Epoch 341, Loss: 0.06383816007621165, Train MAPE: 18.831428574921414, Test MAPE: 20.497160374433143\n",
      "Epoch 342, Loss: 0.06382179710021098, Train MAPE: 18.828054470161007, Test MAPE: 20.497160279431107\n",
      "Epoch 343, Loss: 0.06382300898572028, Train MAPE: 18.828138377499602, Test MAPE: 20.497160294046804\n",
      "Epoch 344, Loss: 0.06382865723568949, Train MAPE: 18.82817729847543, Test MAPE: 20.497160239237935\n",
      "Epoch 345, Loss: 0.06382228400690386, Train MAPE: 18.82941953562136, Test MAPE: 20.49716029039288\n",
      "Epoch 346, Loss: 0.06382760877362947, Train MAPE: 18.82965511146465, Test MAPE: 20.49716034520175\n",
      "Epoch 347, Loss: 0.0638221388464849, Train MAPE: 18.827674495408086, Test MAPE: 20.497160341547822\n",
      "Epoch 348, Loss: 0.06381495026576896, Train MAPE: 18.82808516375285, Test MAPE: 20.497160239237935\n",
      "Epoch 349, Loss: 0.06382069656891483, Train MAPE: 18.825946006884624, Test MAPE: 20.49716027577718\n",
      "Epoch 350, Loss: 0.06382273275794485, Train MAPE: 18.829862348642468, Test MAPE: 20.497160301354654\n",
      "Epoch 351, Loss: 0.06381214591777193, Train MAPE: 18.827624052620116, Test MAPE: 20.497160224622238\n",
      "Epoch 352, Loss: 0.06382555234195059, Train MAPE: 18.832259240337894, Test MAPE: 20.497160173467293\n",
      "Epoch 353, Loss: 0.063829805349265, Train MAPE: 18.829167641249278, Test MAPE: 20.497160166159443\n",
      "Epoch 354, Loss: 0.06382720928357008, Train MAPE: 18.828100212696985, Test MAPE: 20.497160246545786\n",
      "Epoch 355, Loss: 0.06381557362327354, Train MAPE: 18.828362884594632, Test MAPE: 20.4971603378939\n",
      "Epoch 356, Loss: 0.06382444678526282, Train MAPE: 18.82836611364626, Test MAPE: 20.497160513282278\n",
      "Epoch 357, Loss: 0.06381608493097506, Train MAPE: 18.828352278053817, Test MAPE: 20.497160436549862\n",
      "Epoch 358, Loss: 0.06381510788608746, Train MAPE: 18.828311194166584, Test MAPE: 20.497160480396957\n",
      "Epoch 359, Loss: 0.06382795931969627, Train MAPE: 18.83177087313834, Test MAPE: 20.497160568091147\n",
      "Epoch 360, Loss: 0.06383066737893987, Train MAPE: 18.828775296618133, Test MAPE: 20.4971605461676\n",
      "Epoch 361, Loss: 0.06382053923701166, Train MAPE: 18.831988202074, Test MAPE: 20.497160414626315\n",
      "Epoch 362, Loss: 0.06383125836204663, Train MAPE: 18.82916624594055, Test MAPE: 20.497160516936205\n",
      "Epoch 363, Loss: 0.06382486906995566, Train MAPE: 18.828155622729177, Test MAPE: 20.497160568091147\n",
      "Epoch 364, Loss: 0.06381232239313117, Train MAPE: 18.828768123944904, Test MAPE: 20.497160604630395\n",
      "Epoch 365, Loss: 0.06381988056514802, Train MAPE: 18.82854458652544, Test MAPE: 20.497160710594205\n",
      "Epoch 366, Loss: 0.06381947630596697, Train MAPE: 18.82921421447856, Test MAPE: 20.49716077636485\n",
      "Epoch 367, Loss: 0.06382327233927819, Train MAPE: 18.828490036445977, Test MAPE: 20.49716080194232\n",
      "Epoch 368, Loss: 0.06382764856869486, Train MAPE: 18.828470940557903, Test MAPE: 20.49716088963651\n",
      "Epoch 369, Loss: 0.06381433046479153, Train MAPE: 18.828708298482923, Test MAPE: 20.49716091886791\n",
      "Epoch 370, Loss: 0.06385126977458896, Train MAPE: 18.827772629683906, Test MAPE: 20.497160940791456\n",
      "Epoch 371, Loss: 0.06381975655280561, Train MAPE: 18.828966170006492, Test MAPE: 20.49716090425221\n",
      "Epoch 372, Loss: 0.06383006595350152, Train MAPE: 18.83076966368912, Test MAPE: 20.49716088963651\n",
      "Epoch 373, Loss: 0.06382547808169947, Train MAPE: 18.830438124710625, Test MAPE: 20.49716090425221\n",
      "Epoch 374, Loss: 0.06381864401700256, Train MAPE: 18.82883237032282, Test MAPE: 20.497161050409193\n",
      "Epoch 375, Loss: 0.06381568764376588, Train MAPE: 18.82905290773708, Test MAPE: 20.497161196566175\n",
      "Epoch 376, Loss: 0.06382251893949703, Train MAPE: 18.82835584358881, Test MAPE: 20.497161218489723\n",
      "Epoch 377, Loss: 0.06382222892046832, Train MAPE: 18.829013053659967, Test MAPE: 20.497161211181876\n",
      "Epoch 378, Loss: 0.06382245940539347, Train MAPE: 18.830259034183317, Test MAPE: 20.497161401185952\n",
      "Epoch 379, Loss: 0.06382358767992298, Train MAPE: 18.826876136966312, Test MAPE: 20.49716130252999\n",
      "Epoch 380, Loss: 0.06381594900163257, Train MAPE: 18.827715821599913, Test MAPE: 20.4971612148358\n",
      "Epoch 381, Loss: 0.06382406307232574, Train MAPE: 18.829667682044228, Test MAPE: 20.497161170988704\n",
      "Epoch 382, Loss: 0.06382051885863793, Train MAPE: 18.830577717760036, Test MAPE: 20.49716133541531\n",
      "Epoch 383, Loss: 0.06382332997591246, Train MAPE: 18.82859206725401, Test MAPE: 20.49716134272316\n",
      "Epoch 384, Loss: 0.06382680768709655, Train MAPE: 18.828417986945702, Test MAPE: 20.497161382916328\n",
      "Epoch 385, Loss: 0.06383581081189194, Train MAPE: 18.830252067697586, Test MAPE: 20.49716137560848\n",
      "Epoch 386, Loss: 0.06381647220222514, Train MAPE: 18.82764508054446, Test MAPE: 20.497161353684934\n",
      "Epoch 387, Loss: 0.06382226118637555, Train MAPE: 18.829022089335513, Test MAPE: 20.497161371954554\n",
      "Epoch 388, Loss: 0.06382284409787116, Train MAPE: 18.827536686269113, Test MAPE: 20.497161386570255\n",
      "Epoch 389, Loss: 0.06383335159257526, Train MAPE: 18.830169292790092, Test MAPE: 20.49716130983784\n",
      "Epoch 390, Loss: 0.06383760333907713, Train MAPE: 18.828718054671757, Test MAPE: 20.49716130983784\n",
      "Epoch 391, Loss: 0.06381431074896944, Train MAPE: 18.828956803790902, Test MAPE: 20.4971612148358\n",
      "Epoch 392, Loss: 0.06382202157841804, Train MAPE: 18.830187775601818, Test MAPE: 20.497161174642628\n",
      "Epoch 393, Loss: 0.0638247686576666, Train MAPE: 18.8280263532271, Test MAPE: 20.497161068678814\n",
      "Epoch 394, Loss: 0.06382310897386914, Train MAPE: 18.828613023401335, Test MAPE: 20.497161170988704\n",
      "Epoch 395, Loss: 0.06383656844414577, Train MAPE: 18.828408379339873, Test MAPE: 20.49716120752795\n",
      "Epoch 396, Loss: 0.06381470966710925, Train MAPE: 18.82890439742135, Test MAPE: 20.497161244067197\n",
      "Epoch 397, Loss: 0.06381794827350006, Train MAPE: 18.831179724900856, Test MAPE: 20.497161244067197\n",
      "Epoch 398, Loss: 0.06382412527181558, Train MAPE: 18.827759294701902, Test MAPE: 20.497161225797573\n",
      "Epoch 399, Loss: 0.06381862820594902, Train MAPE: 18.829686161655708, Test MAPE: 20.497161269644668\n",
      "Epoch 400, Loss: 0.06382176104516919, Train MAPE: 18.831616238002468, Test MAPE: 20.497161149065157\n",
      "Epoch 401, Loss: 0.06381917259837157, Train MAPE: 18.828957005406615, Test MAPE: 20.497161156373007\n",
      "Epoch 402, Loss: 0.06381751408252226, Train MAPE: 18.82666358417427, Test MAPE: 20.497161211181876\n",
      "Epoch 403, Loss: 0.06382273087654823, Train MAPE: 18.82887659411188, Test MAPE: 20.49716103213957\n",
      "Epoch 404, Loss: 0.06382202758692197, Train MAPE: 18.831042492721135, Test MAPE: 20.49716107964059\n",
      "Epoch 405, Loss: 0.06381979472542312, Train MAPE: 18.828530527372717, Test MAPE: 20.497161116179836\n",
      "Epoch 406, Loss: 0.06381443743562738, Train MAPE: 18.829441069185105, Test MAPE: 20.49716102483172\n",
      "Epoch 407, Loss: 0.06381655703294283, Train MAPE: 18.827737298016352, Test MAPE: 20.49716097002285\n",
      "Epoch 408, Loss: 0.06381417240336723, Train MAPE: 18.827802302396194, Test MAPE: 20.497160991946398\n",
      "Epoch 409, Loss: 0.06381753468144889, Train MAPE: 18.826885750058285, Test MAPE: 20.49716103944742\n",
      "Epoch 410, Loss: 0.06381944082016598, Train MAPE: 18.82807064422139, Test MAPE: 20.497161054063117\n",
      "Epoch 411, Loss: 0.0638159011077218, Train MAPE: 18.829508693654837, Test MAPE: 20.497161083294515\n",
      "Epoch 412, Loss: 0.06382309445309474, Train MAPE: 18.829057397686967, Test MAPE: 20.497161068678814\n",
      "Epoch 413, Loss: 0.06383170370746508, Train MAPE: 18.828442485769095, Test MAPE: 20.497161046755267\n",
      "Epoch 414, Loss: 0.06381682669770683, Train MAPE: 18.830690759711846, Test MAPE: 20.497161083294515\n",
      "Epoch 415, Loss: 0.06381778915217173, Train MAPE: 18.827966946997783, Test MAPE: 20.49716103944742\n",
      "Epoch 416, Loss: 0.0638249681553556, Train MAPE: 18.831738685030036, Test MAPE: 20.49716100290817\n",
      "Epoch 417, Loss: 0.06381709453822154, Train MAPE: 18.829661868105458, Test MAPE: 20.497160973676777\n",
      "Epoch 418, Loss: 0.0638179714422698, Train MAPE: 18.829613430502317, Test MAPE: 20.497161141757307\n",
      "Epoch 419, Loss: 0.06383062680167069, Train MAPE: 18.83043964254319, Test MAPE: 20.497161178296555\n",
      "Epoch 420, Loss: 0.06382242369189615, Train MAPE: 18.82830538845702, Test MAPE: 20.497161090602365\n",
      "Epoch 421, Loss: 0.06383314611227721, Train MAPE: 18.827676024212934, Test MAPE: 20.497161211181876\n",
      "Epoch 422, Loss: 0.06382435893109524, Train MAPE: 18.829106052907065, Test MAPE: 20.49716124041327\n",
      "Epoch 423, Loss: 0.06382466010992216, Train MAPE: 18.827767806450908, Test MAPE: 20.497161203874025\n",
      "Epoch 424, Loss: 0.06381615554451342, Train MAPE: 18.830013624888025, Test MAPE: 20.49716124041327\n",
      "Epoch 425, Loss: 0.06381415259691307, Train MAPE: 18.829322430911482, Test MAPE: 20.497161145411233\n",
      "Epoch 426, Loss: 0.06381913130854686, Train MAPE: 18.830323202383713, Test MAPE: 20.497161130795533\n",
      "Epoch 427, Loss: 0.06381281661289394, Train MAPE: 18.829359950635258, Test MAPE: 20.497161130795533\n",
      "Epoch 428, Loss: 0.06382406129540155, Train MAPE: 18.831685379847585, Test MAPE: 20.49716120752795\n",
      "Epoch 429, Loss: 0.06382323191469948, Train MAPE: 18.83007681289775, Test MAPE: 20.497161178296555\n",
      "Epoch 430, Loss: 0.06381334223778255, Train MAPE: 18.827477147915218, Test MAPE: 20.497161141757307\n",
      "Epoch 431, Loss: 0.06381472677156715, Train MAPE: 18.82936126090879, Test MAPE: 20.497161251375044\n",
      "Epoch 432, Loss: 0.06382843472281416, Train MAPE: 18.82894566006537, Test MAPE: 20.497161196566175\n",
      "Epoch 433, Loss: 0.06381328869343147, Train MAPE: 18.829318805486107, Test MAPE: 20.497161218489723\n",
      "Epoch 434, Loss: 0.0638292175373875, Train MAPE: 18.82788242978309, Test MAPE: 20.49716111983376\n",
      "Epoch 435, Loss: 0.06381874618478556, Train MAPE: 18.828778695740155, Test MAPE: 20.497161141757307\n",
      "Epoch 436, Loss: 0.06381900367092194, Train MAPE: 18.829448620402115, Test MAPE: 20.497161145411233\n",
      "Epoch 437, Loss: 0.06385562771418751, Train MAPE: 18.82862686722299, Test MAPE: 20.497161141757307\n",
      "Epoch 438, Loss: 0.06381438816393067, Train MAPE: 18.826910870187234, Test MAPE: 20.497161178296555\n",
      "Epoch 439, Loss: 0.06382112428260216, Train MAPE: 18.829520702819238, Test MAPE: 20.497161262336817\n",
      "Epoch 440, Loss: 0.0638228332470256, Train MAPE: 18.829084670669396, Test MAPE: 20.497161244067197\n",
      "Epoch 441, Loss: 0.06384592400890554, Train MAPE: 18.828674534937566, Test MAPE: 20.497161130795533\n",
      "Epoch 442, Loss: 0.06383688240968341, Train MAPE: 18.82878745710861, Test MAPE: 20.497161090602365\n",
      "Epoch 443, Loss: 0.06382699252560749, Train MAPE: 18.82719613355964, Test MAPE: 20.497161010216022\n",
      "Epoch 444, Loss: 0.06382417029764571, Train MAPE: 18.828359063954046, Test MAPE: 20.497160907906135\n",
      "Epoch 445, Loss: 0.06381778804047795, Train MAPE: 18.827103046077095, Test MAPE: 20.497161010216022\n",
      "Epoch 446, Loss: 0.06382436774383556, Train MAPE: 18.829329803371703, Test MAPE: 20.497161046755267\n",
      "Epoch 447, Loss: 0.06381595966317767, Train MAPE: 18.829517830824006, Test MAPE: 20.497161075986664\n",
      "Epoch 448, Loss: 0.06381929043166104, Train MAPE: 18.82776025706261, Test MAPE: 20.497161017523872\n",
      "Epoch 449, Loss: 0.06382303954792332, Train MAPE: 18.828687159464206, Test MAPE: 20.497160885982588\n",
      "Epoch 450, Loss: 0.06382276269509916, Train MAPE: 18.831230185518788, Test MAPE: 20.49716077636485\n",
      "Epoch 451, Loss: 0.06382154333400732, Train MAPE: 18.828569623447905, Test MAPE: 20.497160747133453\n",
      "Epoch 452, Loss: 0.06382290236312585, Train MAPE: 18.831619877143194, Test MAPE: 20.497160790980548\n",
      "Epoch 453, Loss: 0.06381671584978314, Train MAPE: 18.829832601867267, Test MAPE: 20.497160772710924\n",
      "Epoch 454, Loss: 0.0638134926762914, Train MAPE: 18.828010816016338, Test MAPE: 20.497160725209906\n",
      "Epoch 455, Loss: 0.06381825383213256, Train MAPE: 18.828193992804934, Test MAPE: 20.49716079463447\n",
      "Epoch 456, Loss: 0.06381439925229475, Train MAPE: 18.828998805235354, Test MAPE: 20.497160875020814\n",
      "Epoch 457, Loss: 0.06382433657756759, Train MAPE: 18.82958701290083, Test MAPE: 20.497160845789416\n",
      "Epoch 458, Loss: 0.06381394199523378, Train MAPE: 18.82704961837081, Test MAPE: 20.497160717902055\n",
      "Epoch 459, Loss: 0.06382689859596646, Train MAPE: 18.829947480305044, Test MAPE: 20.49716068136281\n",
      "Epoch 460, Loss: 0.06383490306526166, Train MAPE: 18.829688122494222, Test MAPE: 20.497160717902055\n",
      "Epoch 461, Loss: 0.06382403019654957, Train MAPE: 18.828498718722553, Test MAPE: 20.497160871366887\n",
      "Epoch 462, Loss: 0.06382685723202884, Train MAPE: 18.827334624351728, Test MAPE: 20.49716085675119\n",
      "Epoch 463, Loss: 0.06381796781520144, Train MAPE: 18.830015318734304, Test MAPE: 20.497160805596245\n",
      "Epoch 464, Loss: 0.06384421594276639, Train MAPE: 18.82904893805631, Test MAPE: 20.497160732517752\n",
      "Epoch 465, Loss: 0.06381491245121507, Train MAPE: 18.828860275125916, Test MAPE: 20.497160725209906\n",
      "Epoch 466, Loss: 0.06382612181668484, Train MAPE: 18.829607949846658, Test MAPE: 20.497160750787376\n",
      "Epoch 467, Loss: 0.06382354653028777, Train MAPE: 18.829467404951643, Test MAPE: 20.497160805596245\n",
      "Epoch 468, Loss: 0.06381333551850894, Train MAPE: 18.82749317293542, Test MAPE: 20.497160805596245\n",
      "Epoch 469, Loss: 0.0638173334095145, Train MAPE: 18.830895693624466, Test MAPE: 20.497160769057\n",
      "Epoch 470, Loss: 0.06382352076935174, Train MAPE: 18.829458055651656, Test MAPE: 20.497160769057\n",
      "Epoch 471, Loss: 0.06381630972974281, Train MAPE: 18.830711024605304, Test MAPE: 20.49716076174915\n",
      "Epoch 472, Loss: 0.06381824347507548, Train MAPE: 18.829353549222134, Test MAPE: 20.49716052789798\n",
      "Epoch 473, Loss: 0.0638197143307654, Train MAPE: 18.829932814476475, Test MAPE: 20.497160484050884\n",
      "Epoch 474, Loss: 0.06382111755618514, Train MAPE: 18.828827902774684, Test MAPE: 20.49716041097239\n",
      "Epoch 475, Loss: 0.06381374976418248, Train MAPE: 18.828021435358153, Test MAPE: 20.49716033058605\n",
      "Epoch 476, Loss: 0.06381596937107703, Train MAPE: 18.828580092835175, Test MAPE: 20.497160294046804\n",
      "Epoch 477, Loss: 0.063826068257154, Train MAPE: 18.82730351152722, Test MAPE: 20.49716046578126\n",
      "Epoch 478, Loss: 0.06381567884754472, Train MAPE: 18.828806134678373, Test MAPE: 20.497160590014694\n",
      "Epoch 479, Loss: 0.06381522302630992, Train MAPE: 18.828679020772846, Test MAPE: 20.49716061193824\n",
      "Epoch 480, Loss: 0.06382203458657458, Train MAPE: 18.827126107531335, Test MAPE: 20.497160608284318\n",
      "Epoch 481, Loss: 0.06382540010062268, Train MAPE: 18.829192567077374, Test MAPE: 20.497160703286358\n",
      "Epoch 482, Loss: 0.06382899782205159, Train MAPE: 18.828511385309618, Test MAPE: 20.497160885982588\n",
      "Epoch 483, Loss: 0.06381754616002194, Train MAPE: 18.830340721463198, Test MAPE: 20.49716077636485\n",
      "Epoch 484, Loss: 0.06383399252555658, Train MAPE: 18.827925418733063, Test MAPE: 20.497160871366887\n",
      "Epoch 485, Loss: 0.06381528979222703, Train MAPE: 18.83022035596926, Test MAPE: 20.49716088963651\n",
      "Epoch 486, Loss: 0.0638304732914939, Train MAPE: 18.827390540937678, Test MAPE: 20.497160739825603\n",
      "Epoch 487, Loss: 0.06381485955155708, Train MAPE: 18.829319556173175, Test MAPE: 20.497160754441303\n",
      "Epoch 488, Loss: 0.06382405585926365, Train MAPE: 18.827263259704825, Test MAPE: 20.497160790980548\n",
      "Epoch 489, Loss: 0.0638257674131746, Train MAPE: 18.828077175930385, Test MAPE: 20.497160838481566\n",
      "Epoch 490, Loss: 0.06381388532206744, Train MAPE: 18.8294906954523, Test MAPE: 20.497160765403077\n",
      "Epoch 491, Loss: 0.0638317166767793, Train MAPE: 18.83055693442618, Test MAPE: 20.497160714248132\n",
      "Epoch 492, Loss: 0.0638196350861973, Train MAPE: 18.82795892717283, Test MAPE: 20.497160769057\n",
      "Epoch 493, Loss: 0.06381408574661439, Train MAPE: 18.82841244091352, Test MAPE: 20.497160739825603\n",
      "Epoch 494, Loss: 0.06382765696220608, Train MAPE: 18.829904374317376, Test MAPE: 20.49716066674711\n",
      "Epoch 495, Loss: 0.06382444690670086, Train MAPE: 18.830347502791643, Test MAPE: 20.497160769057\n",
      "Epoch 496, Loss: 0.06383671958092332, Train MAPE: 18.831733865454463, Test MAPE: 20.497160812904095\n",
      "Epoch 497, Loss: 0.06381723187124483, Train MAPE: 18.82791272928708, Test MAPE: 20.49716088963651\n",
      "Epoch 498, Loss: 0.06382048931079966, Train MAPE: 18.829152999651168, Test MAPE: 20.497160959061077\n",
      "Epoch 499, Loss: 0.06385950571995797, Train MAPE: 18.829217891107927, Test MAPE: 20.497161010216022\n",
      "Epoch 500, Loss: 0.06382028280274295, Train MAPE: 18.828183890531985, Test MAPE: 20.497161054063117\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>█▅▄▄▄▄▃▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_mape</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>time_test</td><td>▇▅▄▅▂▂▁▃▂▂▃▂▂▃▂▂▃▂▁▃▂█▃▂▃▂▂▂▄▂▂▃▆▄▃▄▁▆▂▂</td></tr><tr><td>time_train</td><td>▃▄█▆▃▂▂▅▁▂▂▃▂▂▂▃▂▂▂▂▃▃▂▂▂▂▂▂▂▂▃▃▃▃▃▁▂▂▁▁</td></tr><tr><td>train_loss</td><td>█▆▆▆▆▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_mape</td><td>█▆▅▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>0.08104</td></tr><tr><td>test_mape</td><td>20.49716</td></tr><tr><td>time_test</td><td>1.4938</td></tr><tr><td>time_train</td><td>2.81541</td></tr><tr><td>train_loss</td><td>0.06382</td></tr><tr><td>train_mape</td><td>18.82818</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">MLP-bs16-lr0.0001</strong> at: <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/4llft0p3' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/4llft0p3</a><br/> View project at: <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20241115_072342-4llft0p3\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src import torchModelRun\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "inputs_nums = len(df.columns) - 1\n",
    "\n",
    "class FullyConnectedModel(nn.Module):\n",
    "    import torch.nn.functional as F\n",
    "    def __init__(self):\n",
    "        super(FullyConnectedModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs_nums, 76)\n",
    "        self.fc2 = nn.Linear(76, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "models = torchModelRun.run(FullyConnectedModel, df, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-15T07:06:06.431329900Z",
     "start_time": "2024-11-15T06:23:41.491207200Z"
    }
   },
   "id": "1ec75e098d8fb34e",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "largest_num = 6e+7\n",
    "transformer = models[0]['transform']\n",
    "model = models[0]['model']\n",
    "model.eval()\n",
    "model = model.to('cpu')\n",
    "\n",
    "df_exclude = df.drop(columns=['price_cleaned'])\n",
    "df_exclude_np = df_exclude.values\n",
    "input = transformer(torch.tensor(df_exclude_np, dtype=torch.float32))\n",
    "output = model(input)\n",
    "output_cliped = torch.clamp(output, 6.0, 18.0)\n",
    "output = torch.exp(output_cliped)\n",
    "\n",
    "plt.scatter(df['price_cleaned'], output.detach().numpy(), s=10, alpha=1)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.ylim([0, largest_num])\n",
    "plt.xlim([0, largest_num])\n",
    "plt.plot([0, largest_num], [0, largest_num], color='red')\n",
    "\n",
    "# Set the axes to display whole numbers\n",
    "plt.gca().xaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# rotate the x-axis labels\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-14T15:37:17.607515600Z"
    }
   },
   "id": "85a4ca400ab6d17e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullyConnectedModel(\n",
      "  (fc1): Linear(in_features=77, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FullyConnectedModel' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(models[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Model Summary with counts of parameters\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mmodels\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msummary\u001B[49m())\n",
      "File \u001B[1;32mC:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1929\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[0;32m   1930\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[1;32m-> 1931\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[0;32m   1932\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1933\u001B[0m )\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'FullyConnectedModel' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "print(models[0]['model'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T12:53:11.597652300Z",
     "start_time": "2024-11-14T12:53:11.562113300Z"
    }
   },
   "id": "9913115d1f91b6d7",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min outputs: 44824.0781, max outputs: 48190656.0000\n",
      "ss_res: 2525576731257844269056.0000, ss_tot: 61300058171113472.0000\n",
      "R^2-Wert: -41199.2344\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "trues = torch.tensor(df['price_cleaned'].values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "print(f'min outputs: {torch.min(output).item():.4f}, max outputs: {torch.max(output).item():.4f}')\n",
    "# Berechnung des R^2-Werts\n",
    "ss_res = torch.sum((trues - output) ** 2)\n",
    "ss_tot = torch.sum((trues - torch.mean(trues)) ** 2)\n",
    "\n",
    "print(f\"ss_res: {ss_res.item():.4f}, ss_tot: {ss_tot.item():.4f}\")\n",
    "\n",
    "r2_score = 1 - (ss_res / ss_tot)  # R^2-Formel\n",
    "print(f\"R^2-Wert: {r2_score.item():.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T12:49:18.800782400Z",
     "start_time": "2024-11-14T12:49:17.423119800Z"
    }
   },
   "id": "bf02cfe8b9f454a9",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 366, max value: 65659968\n"
     ]
    },
    {
     "data": {
      "text/plain": "     Floor  detail_responsive#surface_usable  Floor_space_merged  \\\n366    0.0                             800.0                 0.0   \n\n     gde_foreigners_percentage  gde_population  gde_social_help_quota  \\\n366                  28.144885          2043.0               0.689995   \n\n     gde_tax  price_cleaned  Space extracted  Plot_area_unified  ...  \\\n366     5.67       795000.0            200.0              980.0  ...   \n\n     type_unified_penthouse  type_unified_rustico  \\\n366                     0.0                   0.0   \n\n     type_unified_secondary-suite  type_unified_semi-detached-house  \\\n366                           0.0                               0.0   \n\n     type_unified_single-room  type_unified_stepped-apartment  \\\n366                       0.0                             0.0   \n\n     type_unified_stepped-house  type_unified_studio  \\\n366                         0.0                  0.0   \n\n     type_unified_terrace-house  type_unified_villa  \n366                         0.0                 0.0  \n\n[1 rows x 38 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Floor</th>\n      <th>detail_responsive#surface_usable</th>\n      <th>Floor_space_merged</th>\n      <th>gde_foreigners_percentage</th>\n      <th>gde_population</th>\n      <th>gde_social_help_quota</th>\n      <th>gde_tax</th>\n      <th>price_cleaned</th>\n      <th>Space extracted</th>\n      <th>Plot_area_unified</th>\n      <th>...</th>\n      <th>type_unified_penthouse</th>\n      <th>type_unified_rustico</th>\n      <th>type_unified_secondary-suite</th>\n      <th>type_unified_semi-detached-house</th>\n      <th>type_unified_single-room</th>\n      <th>type_unified_stepped-apartment</th>\n      <th>type_unified_stepped-house</th>\n      <th>type_unified_studio</th>\n      <th>type_unified_terrace-house</th>\n      <th>type_unified_villa</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>366</th>\n      <td>0.0</td>\n      <td>800.0</td>\n      <td>0.0</td>\n      <td>28.144885</td>\n      <td>2043.0</td>\n      <td>0.689995</td>\n      <td>5.67</td>\n      <td>795000.0</td>\n      <td>200.0</td>\n      <td>980.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 38 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get index of max prediction to get original data\n",
    "index = torch.argmax(output)\n",
    "\n",
    "print(f'index: {index}, max value: {output[index].item():.0f}')\n",
    "df.iloc[[int(index)]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T16:04:58.404884100Z",
     "start_time": "2024-11-13T16:04:58.375984200Z"
    }
   },
   "id": "2775014563c65865",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 0.00000000e+00 2.42000000e+02 9.25566343e+00\n",
      " 1.54500000e+03 2.23425863e+00 5.89000000e+00 1.56000000e+02\n",
      " 2.22000000e+02 5.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 8.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      " 0.00000000e+00]\n",
      "output: tensor([[13.5669]], grad_fn=<AddmmBackward0>)\n",
      "mean: tensor([2.1500e+00, 8.8294e-01, 2.4629e+01, 2.4678e+01, 1.3220e+04, 2.7662e+00,\n",
      "        6.2422e+00, 1.5121e+02, 4.6831e+02, 4.8767e+00, 2.1362e+02, 6.3205e+02,\n",
      "        1.0192e+01, 4.8822e-02, 1.1532e-01, 4.1612e-01, 3.1292e-02, 5.7949e-04,\n",
      "        2.8974e-04, 3.1485e-02, 2.4184e-01, 1.9316e-04, 3.2500e-02, 7.2919e-03,\n",
      "        4.8629e-01, 1.1590e-03, 1.8833e-03, 2.9361e-02, 3.0906e-03, 4.8291e-05,\n",
      "        3.8005e-02, 9.6581e-05, 9.8030e-03, 2.7526e-03, 4.3944e-03, 2.3228e-02,\n",
      "        5.4423e-02]), std: tensor([3.7403e+01, 1.4964e+01, 9.1374e+01, 1.0910e+01, 2.9369e+04, 2.2509e+00,\n",
      "        2.0956e+00, 1.3060e+02, 3.9246e+03, 1.9806e+00, 6.2011e+02, 9.2582e+02,\n",
      "        7.1058e+00, 2.1550e-01, 3.1941e-01, 4.9293e-01, 1.7411e-01, 2.4066e-02,\n",
      "        1.7020e-02, 1.7463e-01, 4.2821e-01, 1.3897e-02, 1.7733e-01, 8.5083e-02,\n",
      "        4.9982e-01, 3.4025e-02, 4.3358e-02, 1.6882e-01, 5.5508e-02, 6.9491e-03,\n",
      "        1.9121e-01, 9.8273e-03, 9.8526e-02, 5.2394e-02, 6.6146e-02, 1.5063e-01,\n",
      "        2.2686e-01])\n",
      "output exp: 779902\n"
     ]
    }
   ],
   "source": [
    "index_nr = 1\n",
    "df_exclude = df.drop(columns=['price_cleaned'])\n",
    "datapoint = df_exclude.iloc[index_nr].values\n",
    "print(datapoint)\n",
    "datapoint = torch.tensor(datapoint, dtype=torch.float32).unsqueeze(0)\n",
    "transformer = models[0]['transform']\n",
    "\n",
    "datapoint = transformer(datapoint)\n",
    "\n",
    "\n",
    "model = models[0]['model']\n",
    "model.eval()\n",
    "model = model.to('cpu')\n",
    "output = model(datapoint)\n",
    "print(f'output: {output}')\n",
    "transformer = models[0]['transform']\n",
    "print(f'mean: {transformer.mean}, std: {transformer.std}')\n",
    "#output = transformer.inverse(output)\n",
    "#print(f'output inverst: {output}')\n",
    "output = torch.exp(output)\n",
    "print(f'output exp: {output[0][0]:.0f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T16:04:59.333085200Z",
     "start_time": "2024-11-13T16:04:59.319061Z"
    }
   },
   "id": "93d832963b9862d6",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "np.float64(1420000.0)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['price_cleaned'].iloc[index_nr]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T16:05:00.309177500Z",
     "start_time": "2024-11-13T16:05:00.300843300Z"
    }
   },
   "id": "b1d875d245b45d93",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      Floor  detail_responsive#surface_usable  Floor_space_merged  \\\n0  4.000000                        121.247562          136.313355   \n1  2.861332                        224.500270          242.000000   \n2  2.000000                        129.453708          120.182857   \n3 -0.126916                        232.231722          257.000000   \n4  0.000000                        190.906461          177.781407   \n\n   gde_foreigners_percentage  gde_population  gde_social_help_quota  gde_tax  \\\n0                   9.255663          1545.0               2.234259     5.89   \n1                   9.255663          1545.0               2.234259     5.89   \n2                  21.358623         21036.0               3.549010     6.05   \n3                   9.255663          1545.0               2.234259     5.89   \n4                  15.901990          6081.0               1.708126     6.30   \n\n   price_cleaned  Space extracted  Plot_area_unified  ...  \\\n0      1150000.0            100.0        1340.042762  ...   \n1      1420000.0            156.0         222.000000  ...   \n2       720000.0             93.0         462.657966  ...   \n3      1430000.0            154.0         370.000000  ...   \n4       995000.0            142.0          32.776514  ...   \n\n   type_unified_penthouse  type_unified_rustico  type_unified_secondary-suite  \\\n0                     1.0                   0.0                           0.0   \n1                     0.0                   0.0                           0.0   \n2                     1.0                   0.0                           0.0   \n3                     0.0                   0.0                           0.0   \n4                     0.0                   0.0                           0.0   \n\n   type_unified_semi-detached-house  type_unified_single-room  \\\n0                               0.0                       0.0   \n1                               0.0                       0.0   \n2                               0.0                       0.0   \n3                               0.0                       0.0   \n4                               0.0                       0.0   \n\n   type_unified_stepped-apartment  type_unified_stepped-house  \\\n0                             0.0                         0.0   \n1                             0.0                         0.0   \n2                             0.0                         0.0   \n3                             0.0                         0.0   \n4                             0.0                         0.0   \n\n   type_unified_studio  type_unified_terrace-house  type_unified_villa  \n0                  0.0                         0.0                 0.0  \n1                  0.0                         1.0                 0.0  \n2                  0.0                         0.0                 0.0  \n3                  0.0                         0.0                 0.0  \n4                  0.0                         0.0                 0.0  \n\n[5 rows x 36 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Floor</th>\n      <th>detail_responsive#surface_usable</th>\n      <th>Floor_space_merged</th>\n      <th>gde_foreigners_percentage</th>\n      <th>gde_population</th>\n      <th>gde_social_help_quota</th>\n      <th>gde_tax</th>\n      <th>price_cleaned</th>\n      <th>Space extracted</th>\n      <th>Plot_area_unified</th>\n      <th>...</th>\n      <th>type_unified_penthouse</th>\n      <th>type_unified_rustico</th>\n      <th>type_unified_secondary-suite</th>\n      <th>type_unified_semi-detached-house</th>\n      <th>type_unified_single-room</th>\n      <th>type_unified_stepped-apartment</th>\n      <th>type_unified_stepped-house</th>\n      <th>type_unified_studio</th>\n      <th>type_unified_terrace-house</th>\n      <th>type_unified_villa</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.000000</td>\n      <td>121.247562</td>\n      <td>136.313355</td>\n      <td>9.255663</td>\n      <td>1545.0</td>\n      <td>2.234259</td>\n      <td>5.89</td>\n      <td>1150000.0</td>\n      <td>100.0</td>\n      <td>1340.042762</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.861332</td>\n      <td>224.500270</td>\n      <td>242.000000</td>\n      <td>9.255663</td>\n      <td>1545.0</td>\n      <td>2.234259</td>\n      <td>5.89</td>\n      <td>1420000.0</td>\n      <td>156.0</td>\n      <td>222.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.000000</td>\n      <td>129.453708</td>\n      <td>120.182857</td>\n      <td>21.358623</td>\n      <td>21036.0</td>\n      <td>3.549010</td>\n      <td>6.05</td>\n      <td>720000.0</td>\n      <td>93.0</td>\n      <td>462.657966</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.126916</td>\n      <td>232.231722</td>\n      <td>257.000000</td>\n      <td>9.255663</td>\n      <td>1545.0</td>\n      <td>2.234259</td>\n      <td>5.89</td>\n      <td>1430000.0</td>\n      <td>154.0</td>\n      <td>370.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>190.906461</td>\n      <td>177.781407</td>\n      <td>15.901990</td>\n      <td>6081.0</td>\n      <td>1.708126</td>\n      <td>6.30</td>\n      <td>995000.0</td>\n      <td>142.0</td>\n      <td>32.776514</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 36 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T14:56:58.844430600Z",
     "start_time": "2024-11-13T14:56:58.787871500Z"
    }
   },
   "id": "c185cc492808f7ca",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_loader, val_loader, test_loader \u001B[38;5;241m=\u001B[39m torchModelRun\u001B[38;5;241m.\u001B[39mgetDataLoaders(df, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprice_cleaned\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      3\u001B[0m counter \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = torchModelRun.getDataLoaders(df, 'price_cleaned', 1)\n",
    "\n",
    "counter = 5\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    counter -= 1\n",
    "    if counter == 0:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T14:56:59.246497500Z",
     "start_time": "2024-11-13T14:56:58.808427700Z"
    }
   },
   "id": "e7803da5caab2ecd",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.std().values.reshape(1, -1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-13T14:56:59.238985Z"
    }
   },
   "id": "c0eae5eb7afd070d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-13T14:56:59.240984400Z"
    }
   },
   "id": "a77dd87943f775e7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input = 1150000.0\n",
    "log = np.log(input+1)\n",
    "min = 0.6931471824645996\n",
    "max = 17.70733070373535\n",
    "output = (log - min) / (max - min)\n",
    "\n",
    "print(f'output: {output}')\n",
    "\n",
    "output = output * (max - min) + min\n",
    "output = np.exp(output) - 1\n",
    "print(f'output: {output}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-13T14:56:59.242490100Z"
    }
   },
   "id": "170efc538d930cca",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-13T14:56:59.244495800Z"
    }
   },
   "id": "4e9378e20110a7da"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
