{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-07T08:48:13.627471500Z",
     "start_time": "2024-11-07T08:48:06.484292800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:38: DtypeWarning: Columns (3,4,5,6,11,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,45,46,47,49,50,107,110,114,115,116,119,120,121,124,125,126,128,131,132) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(filePath)\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:61: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Space extracted']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Plot_area_unified']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:72: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Availability']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:76: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['No. of rooms:']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.dataPipeline import DataPipeline\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "dp = DataPipeline()\n",
    "df = dp.runPipeline(normalizeAndStandardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, dataframe, feature_columns, label_column):\n",
    "        self.dataframe = dataframe\n",
    "        self.features = dataframe[feature_columns].values\n",
    "        self.labels = dataframe[label_column].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "# Parameters\n",
    "feature_columns = [col for col in df.columns if col != 'price_cleaned']\n",
    "label_column = 'price_cleaned'\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = DataFrameDataset(df, feature_columns, label_column)\n",
    "\n",
    "# Set your desired split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.0\n",
    "test_ratio = 0.3\n",
    "\n",
    "# Calculate lengths for each split\n",
    "total_size = len(dataset)\n",
    "train_size = int(total_size * train_ratio)\n",
    "val_size = int(total_size * val_ratio)\n",
    "test_size = total_size - train_size - val_size  # Ensures all samples are used\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-07T09:12:06.523986900Z",
     "start_time": "2024-11-07T09:12:06.507729Z"
    }
   },
   "id": "ff77bd902f718e75",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "import wandb\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "\n",
    "\n",
    "class HelperClass:\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.wandb_run = None\n",
    "\n",
    "    def train(self, train_loader,test_loader, num_epochs):\n",
    "        self.model.to(self.device)\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            start_traintime = time.time()\n",
    "            self.model.train()\n",
    "            for i, (data) in enumerate(train_loader, 0):\n",
    "                inputs, labels = data\n",
    "                \n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = self.criterion(outputs.flatten(), labels.flatten())\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()                \n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            train_loss = running_loss / (i+1)\n",
    "            endtime_train = time.time() - start_traintime\n",
    "            test_accuracy, train_accuracy, train_loss, test_loss = self.evaluate(test_loader=test_loader, train_loader=train_loader, train_loss=train_loss, endtime_train=endtime_train)\n",
    "            print(f\"Epoch {epoch+1}, Loss: {train_loss}, Train Accuracy: {train_accuracy}, Test Accuracy: {test_accuracy}\")\n",
    "        print(\"Finished Training\")\n",
    "        wandb.finish()\n",
    "        return self.model\n",
    "\n",
    "\n",
    "    def evaluate(self, test_loader, train_loader, train_loss, endtime_train):\n",
    "        # Evaluate the model on test_loader\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        batches = 0\n",
    "        test_loss = 0\n",
    "        test_mape = 0\n",
    "        starttime_test = time.time()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                test_loss += self.criterion(outputs, labels).item()\n",
    "                test_mape += mean_absolute_percentage_error(labels, outputs).item()\n",
    "                batches += 1\n",
    "\n",
    "        # Calculate average loss and MAPE over all batches\n",
    "        avg_test_loss = test_loss / batches\n",
    "        avg_test_mape = test_mape / batches\n",
    "\n",
    "\n",
    "        batches = 0\n",
    "        train_mape = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                train_mape += mean_absolute_percentage_error(labels, outputs).item()\n",
    "                batches += 1\n",
    "        \n",
    "        train_mape = train_mape / batches\n",
    "        \n",
    "        endtime_test = time.time() - starttime_test\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"test_mape\": avg_test_mape,\n",
    "                \"train_mape\": train_mape,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"test_loss\": avg_test_loss,\n",
    "                \"time_train\": endtime_train,\n",
    "                \"time_test\": endtime_test,\n",
    "\n",
    "            }    \n",
    "        )\n",
    "        return avg_test_mape, train_mape, train_loss, test_loss\n",
    "\n",
    "\n",
    "def wandb_login(dict, name = None):\n",
    "    \n",
    "    # start a new wandb run to track this script\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"Immo-Challenge\",\n",
    "        name = name,\n",
    "        # track hyperparameters and run metadata\n",
    "        config=dict\n",
    "    )\n",
    "    \n",
    "def mean_absolute_percentage_error(preds, targets):\n",
    "    epsilon = 1e-10  # Small value to avoid division by zero\n",
    "    return torch.mean(torch.abs((targets - preds) / (targets + epsilon))) * 100\n",
    "    \n",
    "def get_run_hist(run_id):\n",
    "    api = wandb.Api()\n",
    "    run = api.run(path=f'Immo-Challenge/{run_id}')\n",
    "    history = run.history()\n",
    "    return history\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-07T09:12:06.802627700Z",
     "start_time": "2024-11-07T09:12:06.786834700Z"
    }
   },
   "id": "9238d2d0419855e8",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test_model(name: str, linear_layers: int, conv_layers: int):\n",
    "    learning_rates = [0.00001]\n",
    "    epochs = 500\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        fullyConnectedModel = FullyConnectedModel()\n",
    "        # Loss function\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = optim.SGD(fullyConnectedModel.parameters(), lr=lr)\n",
    "\n",
    "        model_class = HelperClass(fullyConnectedModel, criterion, optimizer)\n",
    "\n",
    "        \n",
    "        \n",
    "        dict = {\n",
    "            \"dataset\": \"CIFAR-10-Normalized\",\n",
    "            \"epochs\": epochs,\n",
    "            \"linear_layers\": linear_layers,\n",
    "            \"learning_rate\": lr,\n",
    "            \"architecture\": \"CNN\",\n",
    "            \"batch_size\": 32,\n",
    "            \"conv_layers\": conv_layers,\n",
    "        }\n",
    "\n",
    "        wandb_login(dict, name=f'CNN-Komplexität-{name}-lr{lr}')\n",
    "\n",
    "        trained_model = model_class.train(train_loader, test_loader, epochs)\n",
    "        return trained_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-07T09:12:07.302848400Z",
     "start_time": "2024-11-07T09:12:07.281481Z"
    }
   },
   "id": "8084ba63c86df6c3",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:s990wlab) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>█▇▆▅▄▃▂▁</td></tr><tr><td>test_mape</td><td>▁▁▁▁█▁▂▁</td></tr><tr><td>time_test</td><td>▇▄▁█▅▆▄▂</td></tr><tr><td>time_train</td><td>▇▃▁█▃▅▂▃</td></tr><tr><td>train_loss</td><td>█▇▆▅▄▃▂▁</td></tr><tr><td>train_mape</td><td>▁▁▄▂██▂▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>0.83708</td></tr><tr><td>test_mape</td><td>86.3086</td></tr><tr><td>time_test</td><td>0.71702</td></tr><tr><td>time_train</td><td>0.90914</td></tr><tr><td>train_loss</td><td>1.16984</td></tr><tr><td>train_mape</td><td>91.73158</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">CNN-Komplexität-FullyConnectedModel-lr1e-05</strong> at: <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/s990wlab' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/s990wlab</a><br/> View project at: <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20241107_101145-s990wlab\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:s990wlab). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.18.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\wandb\\run-20241107_101208-l32qdxz2</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/l32qdxz2' target=\"_blank\">CNN-Komplexität-FullyConnectedModel-lr1e-05</a></strong> to <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/l32qdxz2' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/l32qdxz2</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.240620796436228, Train Accuracy: 102.71235521512689, Test Accuracy: 95.5041805189483\n",
      "Epoch 2, Loss: 1.2325157917693617, Train Accuracy: 107.992464411963, Test Accuracy: 95.56263472109424\n",
      "Epoch 3, Loss: 1.2245824666004994, Train Accuracy: 113.82456724805435, Test Accuracy: 95.90549387250628\n",
      "Epoch 4, Loss: 1.2168330516527597, Train Accuracy: 119.91239586961609, Test Accuracy: 96.71853719438825\n",
      "Epoch 5, Loss: 1.209225345639353, Train Accuracy: 191.71360628557935, Test Accuracy: 98.69069730019083\n",
      "Epoch 6, Loss: 1.2017681444597714, Train Accuracy: 157.88529637710272, Test Accuracy: 107.28518972591478\n",
      "Epoch 7, Loss: 1.1944994800428592, Train Accuracy: 128.2380321677978, Test Accuracy: 117.13334247044155\n",
      "Epoch 8, Loss: 1.1874258502078787, Train Accuracy: 520.0602314654794, Test Accuracy: 101.25667673227738\n",
      "Epoch 9, Loss: 1.1802983554304298, Train Accuracy: 117.75761067997705, Test Accuracy: 102.28796238801917\n",
      "Epoch 10, Loss: 1.1731734658780975, Train Accuracy: 111.24226706607075, Test Accuracy: 97.04380732166524\n",
      "Epoch 11, Loss: 1.165956589754027, Train Accuracy: 112.96722887903125, Test Accuracy: 96.27022610878458\n",
      "Epoch 12, Loss: 1.1587638106172664, Train Accuracy: 124.70671786051611, Test Accuracy: 96.26799458873515\n",
      "Epoch 13, Loss: 1.151787462923472, Train Accuracy: 152.9947009097043, Test Accuracy: 97.56683306791345\n",
      "Epoch 14, Loss: 1.144825837601392, Train Accuracy: 115.47263190939337, Test Accuracy: 98.90398434230259\n",
      "Epoch 15, Loss: 1.1378615533757783, Train Accuracy: 136.0350324601522, Test Accuracy: 100.51621215197505\n",
      "Epoch 16, Loss: 1.1311591192509336, Train Accuracy: 114.39591317208121, Test Accuracy: 99.1568163657675\n",
      "Epoch 17, Loss: 1.1241805040128774, Train Accuracy: 115.64803355945278, Test Accuracy: 109.64915158797284\n",
      "Epoch 18, Loss: 1.1171996470548913, Train Accuracy: 148.00629753930824, Test Accuracy: 116.3646433694022\n",
      "Epoch 19, Loss: 1.1101720935259527, Train Accuracy: 124.09574684794153, Test Accuracy: 102.92362084680674\n",
      "Epoch 20, Loss: 1.1030844651015894, Train Accuracy: 116.44504418571356, Test Accuracy: 99.22012430307817\n",
      "Epoch 21, Loss: 1.0959371421852961, Train Accuracy: 130.26796942779742, Test Accuracy: 99.92462228268994\n",
      "Epoch 22, Loss: 1.0898747731741982, Train Accuracy: 116.84452767236489, Test Accuracy: 102.42418655084104\n",
      "Epoch 23, Loss: 1.0814481008496275, Train Accuracy: 120.23436136057914, Test Accuracy: 106.33036422729492\n",
      "Epoch 24, Loss: 1.0740222824203498, Train Accuracy: 142.15166057695185, Test Accuracy: 119.65798179470762\n",
      "Epoch 25, Loss: 1.0666059633264526, Train Accuracy: 118.14076026732864, Test Accuracy: 114.54800220411651\n",
      "Epoch 26, Loss: 1.0589547266111723, Train Accuracy: 119.27072826360457, Test Accuracy: 108.55699239458356\n",
      "Epoch 27, Loss: 1.0513764985753489, Train Accuracy: 130.22225464668523, Test Accuracy: 109.05573488741504\n",
      "Epoch 28, Loss: 1.0435860907288035, Train Accuracy: 401.61708770017395, Test Accuracy: 113.62026677812848\n",
      "Epoch 29, Loss: 1.0354253362774588, Train Accuracy: 298.8082711451424, Test Accuracy: 133.97907369963977\n",
      "Epoch 30, Loss: 1.0268414005376316, Train Accuracy: 201.5410050323286, Test Accuracy: 283.4797246504803\n",
      "Epoch 31, Loss: 1.017920966457812, Train Accuracy: 153.28705783902424, Test Accuracy: 373.0612173469699\n",
      "Epoch 32, Loss: 1.008629176965395, Train Accuracy: 167.27426556476618, Test Accuracy: 134.16085505972103\n",
      "Epoch 33, Loss: 0.9991902233237967, Train Accuracy: 227.43154370497822, Test Accuracy: 328.76783713515925\n",
      "Epoch 34, Loss: 0.9904063253518789, Train Accuracy: 275.4940628869789, Test Accuracy: 175.39044681860477\n",
      "Epoch 35, Loss: 0.9800064168642204, Train Accuracy: 202.40287097121225, Test Accuracy: 360.0414059113483\n",
      "Epoch 36, Loss: 0.9702466826160444, Train Accuracy: 169.614351385271, Test Accuracy: 162.02695163415402\n",
      "Epoch 37, Loss: 0.9604551378837267, Train Accuracy: 253.21738969731905, Test Accuracy: 159.87861277132617\n",
      "Epoch 38, Loss: 0.9503788024113723, Train Accuracy: 239.03570327925735, Test Accuracy: 206.30322357099882\n",
      "Epoch 39, Loss: 0.9403621149381839, Train Accuracy: 239.23494051843556, Test Accuracy: 199.23648248400008\n",
      "Epoch 40, Loss: 0.9299158400201791, Train Accuracy: 247.00989493238586, Test Accuracy: 179.88865249010982\n",
      "Epoch 41, Loss: 0.9194054993662242, Train Accuracy: 183.3441027635036, Test Accuracy: 172.30021721976144\n",
      "Epoch 42, Loss: 0.9087297944922644, Train Accuracy: 202.82647608249923, Test Accuracy: 415.1456226426728\n",
      "Epoch 43, Loss: 0.8977561366603724, Train Accuracy: 582.9357597384463, Test Accuracy: 553.8806882780425\n",
      "Epoch 44, Loss: 0.8866185899660871, Train Accuracy: 251.17975548007556, Test Accuracy: 284.21982537483683\n",
      "Epoch 45, Loss: 0.8752731816961351, Train Accuracy: 207.8383287392247, Test Accuracy: 22972.21339422343\n",
      "Epoch 46, Loss: 0.8638964119001498, Train Accuracy: 256.1644083519733, Test Accuracy: 365.62922012562655\n",
      "Epoch 47, Loss: 0.8525079158348738, Train Accuracy: 203.09925157809937, Test Accuracy: 206.58905093523921\n",
      "Epoch 48, Loss: 0.8410482946090416, Train Accuracy: 205.65881796738773, Test Accuracy: 162.76549121311731\n",
      "Epoch 49, Loss: 0.8295611214703784, Train Accuracy: 242.97868062526445, Test Accuracy: 159.28645299405468\n",
      "Epoch 50, Loss: 0.818095071701826, Train Accuracy: 165.09756181746135, Test Accuracy: 395.2438307976236\n",
      "Epoch 51, Loss: 0.8066500890388661, Train Accuracy: 261.26640457553987, Test Accuracy: 263.11231075987524\n",
      "Epoch 52, Loss: 0.7954367175983587, Train Accuracy: 503.8602785740692, Test Accuracy: 600.1152230866102\n",
      "Epoch 53, Loss: 0.784590766942475, Train Accuracy: 158.38000943206853, Test Accuracy: 194.04838303157263\n",
      "Epoch 54, Loss: 0.7726292791429915, Train Accuracy: 179.0806820324787, Test Accuracy: 183.86151876254957\n",
      "Epoch 55, Loss: 0.7613035836929549, Train Accuracy: 284.8822303851272, Test Accuracy: 1347.763438925451\n",
      "Epoch 56, Loss: 0.7501213689348956, Train Accuracy: 191.35868871342433, Test Accuracy: 248.32467067484953\n",
      "Epoch 57, Loss: 0.7390865523844989, Train Accuracy: 206.85206504671527, Test Accuracy: 379.39013588185213\n",
      "Epoch 58, Loss: 0.7282746328638063, Train Accuracy: 250.15953674984186, Test Accuracy: 204.97997307290836\n",
      "Epoch 59, Loss: 0.7175509070068663, Train Accuracy: 229.99164534449838, Test Accuracy: 279.73502419919384\n",
      "Epoch 60, Loss: 0.7070561849707798, Train Accuracy: 281.3689154994305, Test Accuracy: 237.52816118512834\n",
      "Epoch 61, Loss: 0.6967964310329449, Train Accuracy: 435.7832057209975, Test Accuracy: 604.3276902802137\n",
      "Epoch 62, Loss: 0.6867986960658555, Train Accuracy: 573.1473475817256, Test Accuracy: 206.85736426528618\n",
      "Epoch 63, Loss: 0.6770531216573125, Train Accuracy: 315.1045955432583, Test Accuracy: 217.66489624490544\n",
      "Epoch 64, Loss: 0.6675781253988944, Train Accuracy: 297.07148876231923, Test Accuracy: 265.0419879640852\n",
      "Epoch 65, Loss: 0.6583777994969024, Train Accuracy: 308.48724367738555, Test Accuracy: 204.7002265307368\n",
      "Epoch 66, Loss: 0.6494441405185157, Train Accuracy: 206.6779508110731, Test Accuracy: 169.4678551615501\n",
      "Epoch 67, Loss: 0.6408084280171413, Train Accuracy: 174.82050770064674, Test Accuracy: 187.41863772333886\n",
      "Epoch 68, Loss: 0.6324511922506668, Train Accuracy: 290.06518862701347, Test Accuracy: 165.27900088563257\n",
      "Epoch 69, Loss: 0.6244441795829964, Train Accuracy: 225.44345812307145, Test Accuracy: 190.19802879800602\n",
      "Epoch 70, Loss: 0.6167983374256779, Train Accuracy: 161.40273930155215, Test Accuracy: 187.2193759996064\n",
      "Epoch 71, Loss: 0.6094374138916918, Train Accuracy: 218.61534715481346, Test Accuracy: 244.94565159933907\n",
      "Epoch 72, Loss: 0.6024033589414942, Train Accuracy: 195.5918018165772, Test Accuracy: 268.5416753146113\n",
      "Epoch 73, Loss: 0.595685888424157, Train Accuracy: 197.05493932845033, Test Accuracy: 202.1244970633059\n",
      "Epoch 74, Loss: 0.5892554229773435, Train Accuracy: 182.71942120307907, Test Accuracy: 308.8163998662209\n",
      "Epoch 75, Loss: 0.5831334775923068, Train Accuracy: 193.5838062027612, Test Accuracy: 274.0670851882623\n",
      "Epoch 76, Loss: 0.5773292569904277, Train Accuracy: 213.04669234528322, Test Accuracy: 278.8875561539008\n",
      "Epoch 77, Loss: 0.5717349420951199, Train Accuracy: 251.660174054703, Test Accuracy: 222.05800982883997\n",
      "Epoch 78, Loss: 0.5664520151892787, Train Accuracy: 557.7665204417523, Test Accuracy: 148.55880262413802\n",
      "Epoch 79, Loss: 0.571341073077327, Train Accuracy: 170.65751381671663, Test Accuracy: 226.64373296620894\n",
      "Epoch 80, Loss: 0.5564112986592857, Train Accuracy: 174.65992439653994, Test Accuracy: 154.70989005419673\n",
      "Epoch 81, Loss: 0.5518850880416204, Train Accuracy: 170.81199880554067, Test Accuracy: 198.32232461656844\n",
      "Epoch 82, Loss: 0.5475080552779921, Train Accuracy: 201.2861875371286, Test Accuracy: 198.17953458124276\n",
      "Epoch 83, Loss: 0.5433188116950283, Train Accuracy: 445.6549640421794, Test Accuracy: 198.8750421757601\n",
      "Epoch 84, Loss: 0.5393468669362985, Train Accuracy: 241.44737474036685, Test Accuracy: 273.74783132514176\n",
      "Epoch 85, Loss: 0.5355260063042249, Train Accuracy: 233.36230360235635, Test Accuracy: 173.38311010477494\n",
      "Epoch 86, Loss: 0.5318289584861314, Train Accuracy: 214.33187333029784, Test Accuracy: 245.0013869538599\n",
      "Epoch 87, Loss: 0.528290817321862, Train Accuracy: 218.18964463407153, Test Accuracy: 184.149595494173\n",
      "Epoch 88, Loss: 0.5249266246297036, Train Accuracy: 181.2073591052834, Test Accuracy: 291.2934516011452\n",
      "Epoch 89, Loss: 0.5217186387580426, Train Accuracy: 202.95347544296564, Test Accuracy: 173.13601838325968\n",
      "Epoch 90, Loss: 0.5186147464982056, Train Accuracy: 200.3280873872519, Test Accuracy: 170.704286828333\n",
      "Epoch 91, Loss: 0.5156083271158064, Train Accuracy: 274.1107856616932, Test Accuracy: 138.97315943970972\n",
      "Epoch 92, Loss: 0.5128341045997898, Train Accuracy: 321.7072079030377, Test Accuracy: 202.1779650279454\n",
      "Epoch 93, Loss: 0.5099164745997111, Train Accuracy: 396.11220004626387, Test Accuracy: 215.73086372686893\n",
      "Epoch 94, Loss: 0.5072141833882575, Train Accuracy: 307.65809368409003, Test Accuracy: 185.20034307363082\n",
      "Epoch 95, Loss: 0.5045516371713743, Train Accuracy: 191.94126686669023, Test Accuracy: 179.08318712273422\n",
      "Epoch 96, Loss: 0.5019700639082335, Train Accuracy: 176.7230503491291, Test Accuracy: 221.00555924006872\n",
      "Epoch 97, Loss: 0.499443906112788, Train Accuracy: 159.02669318134465, Test Accuracy: 296.7126461924339\n",
      "Epoch 98, Loss: 0.4969985756632837, Train Accuracy: 206.81907738115908, Test Accuracy: 220.0022613564316\n",
      "Epoch 99, Loss: 0.494868254590641, Train Accuracy: 178.87079091145122, Test Accuracy: 167.94684188219966\n",
      "Epoch 100, Loss: 0.4922227436044616, Train Accuracy: 182.4192345204969, Test Accuracy: 259.4625085713912\n",
      "Epoch 101, Loss: 0.48990455860831456, Train Accuracy: 178.15394276758775, Test Accuracy: 259.23815834278963\n",
      "Epoch 102, Loss: 0.4874928384332608, Train Accuracy: 146.7479532855345, Test Accuracy: 149.56369524585958\n",
      "Epoch 103, Loss: 0.48530251166938154, Train Accuracy: 228.16090388058052, Test Accuracy: 207.00712145591268\n",
      "Epoch 104, Loss: 0.48304206316118586, Train Accuracy: 426.738285248337, Test Accuracy: 184.29047582587418\n",
      "Epoch 105, Loss: 0.48082347557761784, Train Accuracy: 215.55048503082742, Test Accuracy: 169.24916578798877\n",
      "Epoch 106, Loss: 0.4786114412069269, Train Accuracy: 185.0807679758552, Test Accuracy: 386.5651464851535\n",
      "Epoch 107, Loss: 0.4764374254645344, Train Accuracy: 453.91402510428065, Test Accuracy: 164.30726228441512\n",
      "Epoch 108, Loss: 0.47427304165643647, Train Accuracy: 267.18076945498944, Test Accuracy: 152.68050664784957\n",
      "Epoch 109, Loss: 0.47209955188058544, Train Accuracy: 246.9500145025274, Test Accuracy: 287.21877641093977\n",
      "Epoch 110, Loss: 0.46994040048624136, Train Accuracy: 193.17568738507495, Test Accuracy: 186.84765881908183\n",
      "Epoch 111, Loss: 0.4678389896644805, Train Accuracy: 185.7462468053371, Test Accuracy: 166.43882940253434\n",
      "Epoch 112, Loss: 0.46567669110303883, Train Accuracy: 206.71045926050232, Test Accuracy: 917.5025444420016\n",
      "Epoch 113, Loss: 0.4635938535881569, Train Accuracy: 245.73354435593086, Test Accuracy: 182.15177160379838\n",
      "Epoch 114, Loss: 0.46148866778738895, Train Accuracy: 175.96022259067394, Test Accuracy: 154.39362504530925\n",
      "Epoch 115, Loss: 0.4758335845616261, Train Accuracy: 143.67960453333427, Test Accuracy: 161.99794321644063\n",
      "Epoch 116, Loss: 0.45752580807527987, Train Accuracy: 176.5434118058019, Test Accuracy: 157.07371042212662\n",
      "Epoch 117, Loss: 0.45510362982148755, Train Accuracy: 179.58785102769113, Test Accuracy: 191.44316420263175\n",
      "Epoch 118, Loss: 0.45306693888177657, Train Accuracy: 160.65420159938822, Test Accuracy: 209.3975949190101\n",
      "Epoch 119, Loss: 0.4511024726022858, Train Accuracy: 147.80176135173772, Test Accuracy: 220.4930919335813\n",
      "Epoch 120, Loss: 0.44886135028398044, Train Accuracy: 130.19566055564962, Test Accuracy: 1171.3235329024646\n",
      "Epoch 121, Loss: 0.44692020061328047, Train Accuracy: 130.77471154210866, Test Accuracy: 162.6446723354106\n",
      "Epoch 122, Loss: 0.4449226283223757, Train Accuracy: 136.50951246098825, Test Accuracy: 157.6341367838334\n",
      "Epoch 123, Loss: 0.4428978701304498, Train Accuracy: 15381.407978650517, Test Accuracy: 171.5496445480658\n",
      "Epoch 124, Loss: 0.44089206963058164, Train Accuracy: 158.84635366861355, Test Accuracy: 168.00634172011394\n",
      "Epoch 125, Loss: 0.43887364935548673, Train Accuracy: 161.33205748543438, Test Accuracy: 206.73722039436808\n",
      "Epoch 126, Loss: 0.43692225544393487, Train Accuracy: 158.8588654499346, Test Accuracy: 300.5157766342163\n",
      "Epoch 127, Loss: 0.434893117209334, Train Accuracy: 149.2336338468923, Test Accuracy: 541.4983449663434\n",
      "Epoch 128, Loss: 0.4329268331934965, Train Accuracy: 153.03788380862846, Test Accuracy: 211.67582875855115\n",
      "Epoch 129, Loss: 0.4309227853968609, Train Accuracy: 212.68959319617608, Test Accuracy: 187.66239637258101\n",
      "Epoch 130, Loss: 0.4301408639292822, Train Accuracy: 188.8623092868396, Test Accuracy: 224.69943770583794\n",
      "Epoch 131, Loss: 0.4269529508260696, Train Accuracy: 436.1684913384836, Test Accuracy: 188.89304816966154\n",
      "Epoch 132, Loss: 0.42492953060791955, Train Accuracy: 169.41594041619834, Test Accuracy: 276.4957405401736\n",
      "Epoch 133, Loss: 0.42304115473033954, Train Accuracy: 202.50963212729022, Test Accuracy: 272.3630223955427\n",
      "Epoch 134, Loss: 0.42107011885249773, Train Accuracy: 151.0343360900879, Test Accuracy: 206.22554202955595\n",
      "Epoch 135, Loss: 0.41911741522654355, Train Accuracy: 217.99184495354115, Test Accuracy: 330.5028990920709\n",
      "Epoch 136, Loss: 0.41717993242959583, Train Accuracy: 400.2127446815274, Test Accuracy: 157.56971826358716\n",
      "Epoch 137, Loss: 0.41516863593028724, Train Accuracy: 226.61788962967287, Test Accuracy: 170.71778780100297\n",
      "Epoch 138, Loss: 0.41324161052039243, Train Accuracy: 165.23995498390116, Test Accuracy: 176.1661597271355\n",
      "Epoch 139, Loss: 0.4112782080059655, Train Accuracy: 989.9636782723392, Test Accuracy: 240.27239715809725\n",
      "Epoch 140, Loss: 0.4093166402331197, Train Accuracy: 223.18505763798888, Test Accuracy: 216.0060570191364\n",
      "Epoch 141, Loss: 0.40741315150747576, Train Accuracy: 206.95291149381475, Test Accuracy: 170.28209614267155\n",
      "Epoch 142, Loss: 0.4054213132468792, Train Accuracy: 172.98396297095903, Test Accuracy: 156.86178304711166\n",
      "Epoch 143, Loss: 0.4035362846037695, Train Accuracy: 310.69914572818016, Test Accuracy: 222.66085809590865\n",
      "Epoch 144, Loss: 0.4016216251121822, Train Accuracy: 177.5015485584084, Test Accuracy: 212.84287855576497\n",
      "Epoch 145, Loss: 0.400632895843617, Train Accuracy: 182.69059964603616, Test Accuracy: 211.13382281089315\n",
      "Epoch 146, Loss: 0.39771040409438607, Train Accuracy: 165.84246366237915, Test Accuracy: 280.2030800216052\n",
      "Epoch 147, Loss: 0.3958053370266215, Train Accuracy: 446.48890738622885, Test Accuracy: 205.48675396977637\n",
      "Epoch 148, Loss: 0.39388728635518094, Train Accuracy: 182.40543091688428, Test Accuracy: 548.7041974359629\n",
      "Epoch 149, Loss: 0.3919902044955714, Train Accuracy: 331.0046522445178, Test Accuracy: 205.9642265475526\n",
      "Epoch 150, Loss: 0.390086718741193, Train Accuracy: 195.10525176300783, Test Accuracy: 201.34772049650854\n",
      "Epoch 151, Loss: 0.38820340701774236, Train Accuracy: 393.09276291995394, Test Accuracy: 163.53986012205786\n",
      "Epoch 152, Loss: 0.38632598743494806, Train Accuracy: 200.0731805179521, Test Accuracy: 1094.7238266224763\n",
      "Epoch 153, Loss: 0.38443322950977277, Train Accuracy: 285.7845050970366, Test Accuracy: 161.77732239937296\n",
      "Epoch 154, Loss: 0.38256954401935284, Train Accuracy: 137.38680128352127, Test Accuracy: 160.88909014877007\n",
      "Epoch 155, Loss: 0.38070660449729526, Train Accuracy: 137.0847962721693, Test Accuracy: 179.15851682546187\n",
      "Epoch 156, Loss: 0.37882804850663254, Train Accuracy: 216.34231643342866, Test Accuracy: 249.1018061735192\n",
      "Epoch 157, Loss: 0.3769912400702728, Train Accuracy: 288.4838977788679, Test Accuracy: 180.5178432659227\n",
      "Epoch 158, Loss: 0.3751241202849316, Train Accuracy: 144.26420381241346, Test Accuracy: 139.09499660803348\n",
      "Epoch 159, Loss: 0.3733015036107504, Train Accuracy: 165.0601357975465, Test Accuracy: 137.6603853459261\n",
      "Epoch 160, Loss: 0.3714330955775336, Train Accuracy: 175.56732090922884, Test Accuracy: 135.16995675223214\n",
      "Epoch 161, Loss: 0.36951539817946827, Train Accuracy: 183.94719549967894, Test Accuracy: 138.41798430073018\n",
      "Epoch 162, Loss: 0.3678150406936702, Train Accuracy: 292.6681858371593, Test Accuracy: 151.3645019920505\n",
      "Epoch 163, Loss: 0.3663533466258769, Train Accuracy: 202.81668700170204, Test Accuracy: 176.86619639883236\n",
      "Epoch 164, Loss: 0.36418838365126854, Train Accuracy: 174.4870734934734, Test Accuracy: 290.575668179259\n",
      "Epoch 165, Loss: 0.36239422823066053, Train Accuracy: 163.72907273774595, Test Accuracy: 143.35262284960066\n",
      "Epoch 166, Loss: 0.3605958889207968, Train Accuracy: 390.0653913401931, Test Accuracy: 143.876186565477\n",
      "Epoch 167, Loss: 0.3587865170676779, Train Accuracy: 381.0723650491994, Test Accuracy: 165.364859931323\n",
      "Epoch 168, Loss: 0.3573474834476449, Train Accuracy: 210.15870523244078, Test Accuracy: 301.4672990526472\n",
      "Epoch 169, Loss: 0.35832913678813993, Train Accuracy: 183.02190690489255, Test Accuracy: 214.34935760498047\n",
      "Epoch 170, Loss: 0.35347851072295405, Train Accuracy: 207.26876843575576, Test Accuracy: 138.5680452852833\n",
      "Epoch 171, Loss: 0.3516881328411869, Train Accuracy: 153.0306896151286, Test Accuracy: 158.4591204000979\n",
      "Epoch 172, Loss: 0.34992931116702486, Train Accuracy: 154.35532159617483, Test Accuracy: 225.42416919007593\n",
      "Epoch 173, Loss: 0.3481718723588833, Train Accuracy: 184.13369559615654, Test Accuracy: 159.72774398570158\n",
      "Epoch 174, Loss: 0.34641839397706387, Train Accuracy: 200.13673973918222, Test Accuracy: 179.56299279660595\n",
      "Epoch 175, Loss: 0.34469627505942724, Train Accuracy: 294.4238639055248, Test Accuracy: 525.5877871221426\n",
      "Epoch 176, Loss: 0.342994981480526, Train Accuracy: 286.352776446019, Test Accuracy: 158.23466853706205\n",
      "Epoch 177, Loss: 0.34123796116941785, Train Accuracy: 341.98947456576894, Test Accuracy: 138.66291011109644\n",
      "Epoch 178, Loss: 0.339527958715681, Train Accuracy: 238.67005177504123, Test Accuracy: 141.3901237565644\n",
      "Epoch 179, Loss: 0.3378174656123754, Train Accuracy: 183.73416329473582, Test Accuracy: 834.2433634193576\n",
      "Epoch 180, Loss: 0.3364257995885974, Train Accuracy: 544.2147967966694, Test Accuracy: 135.8012884490344\n",
      "Epoch 181, Loss: 0.3344648910525379, Train Accuracy: 167.82305355614594, Test Accuracy: 131.52279624160454\n",
      "Epoch 182, Loss: 0.3327696194509386, Train Accuracy: 205.32949222255849, Test Accuracy: 137.13544047608667\n",
      "Epoch 183, Loss: 0.331114937607362, Train Accuracy: 173.92945849295518, Test Accuracy: 208.5538004661093\n",
      "Epoch 184, Loss: 0.3294679311019291, Train Accuracy: 225.0258532050402, Test Accuracy: 1190.0217954090663\n",
      "Epoch 185, Loss: 0.3278100877860217, Train Accuracy: 162.41437671586252, Test Accuracy: 211.68667518849276\n",
      "Epoch 186, Loss: 0.3264791271863281, Train Accuracy: 236.10161722045498, Test Accuracy: 153.25436882096895\n",
      "Epoch 187, Loss: 0.32458876370996464, Train Accuracy: 145.06225720328368, Test Accuracy: 139.29573424981564\n",
      "Epoch 188, Loss: 0.3229964889669526, Train Accuracy: 137.23511762639887, Test Accuracy: 139.20467250201168\n",
      "Epoch 189, Loss: 0.32136751027804283, Train Accuracy: 156.45722473945867, Test Accuracy: 144.5742283062059\n",
      "Epoch 190, Loss: 0.3197928589266364, Train Accuracy: 220.8866979173289, Test Accuracy: 163.12789200763314\n",
      "Epoch 191, Loss: 0.31822940278380574, Train Accuracy: 172.12484196134938, Test Accuracy: 193.51248094986897\n",
      "Epoch 192, Loss: 0.3166496709678974, Train Accuracy: 184.65532531571336, Test Accuracy: 265.5460612822552\n",
      "Epoch 193, Loss: 0.31510304177997134, Train Accuracy: 159.9022621054618, Test Accuracy: 269.3628220071598\n",
      "Epoch 194, Loss: 0.3135558552757268, Train Accuracy: 182.1658617891942, Test Accuracy: 176.30809768365353\n",
      "Epoch 195, Loss: 0.3120380375978559, Train Accuracy: 178.88088291009615, Test Accuracy: 303.53439798160474\n",
      "Epoch 196, Loss: 0.3104960614029742, Train Accuracy: 164.28296889674482, Test Accuracy: 277.3965985434396\n",
      "Epoch 197, Loss: 0.3090083249579683, Train Accuracy: 240.5188823758382, Test Accuracy: 166.07779300456144\n",
      "Epoch 198, Loss: 0.30753432893086247, Train Accuracy: 191.632399277301, Test Accuracy: 156.6952092501582\n",
      "Epoch 199, Loss: 0.30603545804047466, Train Accuracy: 180.18951479872155, Test Accuracy: 177.0699606720282\n",
      "Epoch 200, Loss: 0.30459698838491556, Train Accuracy: 224.48553671304714, Test Accuracy: 170.33526130598418\n",
      "Epoch 201, Loss: 0.3031127785385486, Train Accuracy: 168.53061589344324, Test Accuracy: 167.2327021968608\n",
      "Epoch 202, Loss: 0.30185715476187513, Train Accuracy: 184.1028743051074, Test Accuracy: 146.84504047705204\n",
      "Epoch 203, Loss: 0.30021995803493096, Train Accuracy: 250.97246594919417, Test Accuracy: 206.5565483132187\n",
      "Epoch 204, Loss: 0.29881844093413773, Train Accuracy: 144.21382365654543, Test Accuracy: 216.18364608531095\n",
      "Epoch 205, Loss: 0.2974137453456371, Train Accuracy: 180.71111072231434, Test Accuracy: 355.4265357815489\n",
      "Epoch 206, Loss: 0.29601440580864385, Train Accuracy: 160.70472782393776, Test Accuracy: 221.1251501161225\n",
      "Epoch 207, Loss: 0.2946590224466403, Train Accuracy: 162.26460753593196, Test Accuracy: 160.3650089186065\n",
      "Epoch 208, Loss: 0.29329763413040083, Train Accuracy: 207.47901238706493, Test Accuracy: 168.08516214331803\n",
      "Epoch 209, Loss: 0.29193803720026945, Train Accuracy: 1254.3571633770898, Test Accuracy: 207.99892262050085\n",
      "Epoch 210, Loss: 0.2906443598047874, Train Accuracy: 182.29586294495564, Test Accuracy: 1386.0119483324945\n",
      "Epoch 211, Loss: 0.28928398160127355, Train Accuracy: 219.9248867160121, Test Accuracy: 175.94314851566236\n",
      "Epoch 212, Loss: 0.2887627838930437, Train Accuracy: 433.5971316080907, Test Accuracy: 1386.6915725980487\n",
      "Epoch 213, Loss: 0.286677172623287, Train Accuracy: 179.2863179964362, Test Accuracy: 153.8321215376562\n",
      "Epoch 214, Loss: 0.285439163368986, Train Accuracy: 165.90514301992872, Test Accuracy: 170.0311084864091\n",
      "Epoch 215, Loss: 0.2841332096482181, Train Accuracy: 207.98620288272917, Test Accuracy: 148.06137764210604\n",
      "Epoch 216, Loss: 0.28287282347968523, Train Accuracy: 204.6244863180415, Test Accuracy: 243.16158678093734\n",
      "Epoch 217, Loss: 0.28162016837166814, Train Accuracy: 213.10646207379565, Test Accuracy: 151.2084954709423\n",
      "Epoch 218, Loss: 0.2805508820857665, Train Accuracy: 198.41238990132607, Test Accuracy: 157.3167809272299\n",
      "Epoch 219, Loss: 0.27915512089855987, Train Accuracy: 144.2449546914132, Test Accuracy: 472.2898244273906\n",
      "Epoch 220, Loss: 0.2783442369134408, Train Accuracy: 137.62934443935, Test Accuracy: 188.27315355806934\n",
      "Epoch 221, Loss: 0.27674916534301475, Train Accuracy: 244.46033005328357, Test Accuracy: 178.37253512168417\n",
      "Epoch 222, Loss: 0.27555725014008403, Train Accuracy: 146.56293610253533, Test Accuracy: 171.18357872476383\n",
      "Epoch 223, Loss: 0.2743930548777971, Train Accuracy: 543.3199753291758, Test Accuracy: 174.65603069383272\n",
      "Epoch 224, Loss: 0.2732190033604332, Train Accuracy: 142.09782043223308, Test Accuracy: 272.3922759464809\n",
      "Epoch 225, Loss: 0.2720642177656518, Train Accuracy: 235.5761019166241, Test Accuracy: 246.46069711568404\n",
      "Epoch 226, Loss: 0.2709589738819618, Train Accuracy: 156.7281072019748, Test Accuracy: 821.2469186782837\n",
      "Epoch 227, Loss: 0.2698175839960567, Train Accuracy: 194.59363256241613, Test Accuracy: 167.8317878684219\n",
      "Epoch 228, Loss: 0.26923510177658067, Train Accuracy: 150.71966836541174, Test Accuracy: 188.68994995039336\n",
      "Epoch 229, Loss: 0.2675790399704251, Train Accuracy: 156.9818067821945, Test Accuracy: 189.8691216099019\n",
      "Epoch 230, Loss: 0.26650957636743033, Train Accuracy: 260.1043359518573, Test Accuracy: 148.47518385673055\n",
      "Epoch 231, Loss: 0.26565012061831905, Train Accuracy: 150.53630814667082, Test Accuracy: 171.6804703887628\n",
      "Epoch 232, Loss: 0.2646808922749331, Train Accuracy: 172.5023077332478, Test Accuracy: 245.75860936301095\n",
      "Epoch 233, Loss: 0.2633461928799039, Train Accuracy: 176.65669216473202, Test Accuracy: 199.6943439366866\n",
      "Epoch 234, Loss: 0.2625534974468599, Train Accuracy: 341.0408285796251, Test Accuracy: 149.39834186008997\n",
      "Epoch 235, Loss: 0.26128264522909533, Train Accuracy: 1590.2624682587136, Test Accuracy: 282.8958512131049\n",
      "Epoch 236, Loss: 0.260272945360947, Train Accuracy: 147.36066464313532, Test Accuracy: 140.1175752756547\n",
      "Epoch 237, Loss: 0.25926736646826803, Train Accuracy: 167.08247776156702, Test Accuracy: 136.21961412624438\n",
      "Epoch 238, Loss: 0.2583294129243663, Train Accuracy: 226.8730460235796, Test Accuracy: 124.4375163681653\n",
      "Epoch 239, Loss: 0.25731960041000135, Train Accuracy: 182.00567796402478, Test Accuracy: 117.63833221124143\n",
      "Epoch 240, Loss: 0.2563547636402661, Train Accuracy: 194.49584218031467, Test Accuracy: 119.19177469915273\n",
      "Epoch 241, Loss: 0.25539845204555495, Train Accuracy: 140.94127397933727, Test Accuracy: 141.0307005278918\n",
      "Epoch 242, Loss: 0.2545763791978962, Train Accuracy: 275.7894437443506, Test Accuracy: 133.66016421026114\n",
      "Epoch 243, Loss: 0.2535179777621624, Train Accuracy: 154.0061543921673, Test Accuracy: 165.34919228845712\n",
      "Epoch 244, Loss: 0.25266931732888265, Train Accuracy: 200.99775099806504, Test Accuracy: 138.76499499106893\n",
      "Epoch 245, Loss: 0.2517325514448599, Train Accuracy: 218.03790432619058, Test Accuracy: 123.59991404474998\n",
      "Epoch 246, Loss: 0.25097044175939076, Train Accuracy: 333.5475122714721, Test Accuracy: 153.47429481817753\n",
      "Epoch 247, Loss: 0.2514718287701859, Train Accuracy: 164.54403244118723, Test Accuracy: 343.05533977430696\n",
      "Epoch 248, Loss: 0.24906857485049436, Train Accuracy: 188.99998079295753, Test Accuracy: 127.4445097981667\n",
      "Epoch 249, Loss: 0.24820638326133238, Train Accuracy: 159.26348429957417, Test Accuracy: 173.58175614415381\n",
      "Epoch 250, Loss: 0.24736705579422008, Train Accuracy: 153.79023059870013, Test Accuracy: 154.3776169212497\n",
      "Epoch 251, Loss: 0.24750464520451798, Train Accuracy: 161.4294558552214, Test Accuracy: 131.89498321377502\n",
      "Epoch 252, Loss: 0.24571915435669964, Train Accuracy: 172.8520458396728, Test Accuracy: 154.21392004830497\n",
      "Epoch 253, Loss: 0.24491180775659588, Train Accuracy: 261.01199318282715, Test Accuracy: 111.45717249111253\n",
      "Epoch 254, Loss: 0.2441116290236263, Train Accuracy: 173.68635458914926, Test Accuracy: 110.1769102446887\n",
      "Epoch 255, Loss: 0.24330785741450783, Train Accuracy: 152.64799172299175, Test Accuracy: 112.42690294616077\n",
      "Epoch 256, Loss: 0.24252720869653727, Train Accuracy: 145.6915932119023, Test Accuracy: 129.5020655028674\n",
      "Epoch 257, Loss: 0.24194343912331653, Train Accuracy: 141.4386270301869, Test Accuracy: 255.11688212959135\n",
      "Epoch 258, Loss: 0.24098823311421777, Train Accuracy: 150.93668260303056, Test Accuracy: 242.08583617696956\n",
      "Epoch 259, Loss: 0.24022886829136889, Train Accuracy: 152.92972709939568, Test Accuracy: 147.1635350207893\n",
      "Epoch 260, Loss: 0.23986382375693427, Train Accuracy: 187.27259243186768, Test Accuracy: 123.22808067166075\n",
      "Epoch 261, Loss: 0.23875682586238514, Train Accuracy: 405.7105810386608, Test Accuracy: 223.37091315522486\n",
      "Epoch 262, Loss: 0.23801735328322918, Train Accuracy: 239.08604012537316, Test Accuracy: 128.31328339479407\n",
      "Epoch 263, Loss: 0.23732188248424027, Train Accuracy: 170.39466560747744, Test Accuracy: 115.08477670319226\n",
      "Epoch 264, Loss: 0.23662920266540652, Train Accuracy: 234.66695808894787, Test Accuracy: 108.88823077143455\n",
      "Epoch 265, Loss: 0.23729105470029543, Train Accuracy: 178.16687733756635, Test Accuracy: 191.32818056612598\n",
      "Epoch 266, Loss: 0.23525337534355453, Train Accuracy: 233.4931625015626, Test Accuracy: 122.79571996416364\n",
      "Epoch 267, Loss: 0.23457377071694127, Train Accuracy: 139.92447951168668, Test Accuracy: 111.70660912260718\n",
      "Epoch 268, Loss: 0.2339117716156207, Train Accuracy: 145.4707233587553, Test Accuracy: 105.76616680378817\n",
      "Epoch 269, Loss: 0.23327051968752835, Train Accuracy: 144.03104459065577, Test Accuracy: 105.39770937939079\n",
      "Epoch 270, Loss: 0.23260827046009347, Train Accuracy: 164.85846557450242, Test Accuracy: 126.68130755911068\n",
      "Epoch 271, Loss: 0.23197402350586194, Train Accuracy: 180.0504184614386, Test Accuracy: 140.49324131011963\n",
      "Epoch 272, Loss: 0.2313498616518139, Train Accuracy: 127.08844493306701, Test Accuracy: 106.91272867942342\n",
      "Epoch 273, Loss: 0.23073180639174823, Train Accuracy: 131.9020366919119, Test Accuracy: 109.5788294733787\n",
      "Epoch 274, Loss: 0.23049008204750474, Train Accuracy: 133.3929564060737, Test Accuracy: 254.9875443322318\n",
      "Epoch 275, Loss: 0.22950230421308893, Train Accuracy: 190.6549412917256, Test Accuracy: 269.9754906089938\n",
      "Epoch 276, Loss: 0.2289345661459462, Train Accuracy: 146.34833438130386, Test Accuracy: 143.77674513447042\n",
      "Epoch 277, Loss: 0.2283464498587128, Train Accuracy: 166.43905754005883, Test Accuracy: 146.19147004886548\n",
      "Epoch 278, Loss: 0.22790563691211804, Train Accuracy: 189.49582793154394, Test Accuracy: 129.29134880766577\n",
      "Epoch 279, Loss: 0.22720373865559337, Train Accuracy: 1159.411528714637, Test Accuracy: 124.03038274025431\n",
      "Epoch 280, Loss: 0.2266689363726429, Train Accuracy: 165.45248893053318, Test Accuracy: 111.83558380360506\n",
      "Epoch 281, Loss: 0.22608658506397944, Train Accuracy: 192.38230486145687, Test Accuracy: 108.72431875734912\n",
      "Epoch 282, Loss: 0.22555259671887581, Train Accuracy: 147.77254023645847, Test Accuracy: 184.7509522535363\n",
      "Epoch 283, Loss: 0.2250154205397713, Train Accuracy: 250.81728661034248, Test Accuracy: 118.34787828095105\n",
      "Epoch 284, Loss: 0.22450101589356067, Train Accuracy: 158.24142840863317, Test Accuracy: 115.45820082450399\n",
      "Epoch 285, Loss: 0.22397152036306178, Train Accuracy: 133.20352808077956, Test Accuracy: 138.6180877685547\n",
      "Epoch 286, Loss: 0.22347646748598185, Train Accuracy: 159.15075981590888, Test Accuracy: 127.17552782564746\n",
      "Epoch 287, Loss: 0.2229572350780339, Train Accuracy: 125.50173706455355, Test Accuracy: 180.58209662534753\n",
      "Epoch 288, Loss: 0.22246374711964922, Train Accuracy: 129.81547264249372, Test Accuracy: 108.38733410348698\n",
      "Epoch 289, Loss: 0.22197029164986415, Train Accuracy: 155.24084074492006, Test Accuracy: 167.50172346465442\n",
      "Epoch 290, Loss: 0.22148168498152945, Train Accuracy: 124.57996913484202, Test Accuracy: 115.24081588278011\n",
      "Epoch 291, Loss: 0.22101449835291478, Train Accuracy: 171.74814649118636, Test Accuracy: 196.6514393164187\n",
      "Epoch 292, Loss: 0.22053539064555905, Train Accuracy: 117.1274142860062, Test Accuracy: 126.19956183920101\n",
      "Epoch 293, Loss: 0.22008231066725742, Train Accuracy: 120.76182384616176, Test Accuracy: 122.38358892713275\n",
      "Epoch 294, Loss: 0.21961545026991047, Train Accuracy: 109.79754823563657, Test Accuracy: 145.75381407445792\n",
      "Epoch 295, Loss: 0.2191553949516467, Train Accuracy: 103.96483363312235, Test Accuracy: 125.90225714080188\n",
      "Epoch 296, Loss: 0.21873157070664367, Train Accuracy: 135.94893734721177, Test Accuracy: 298.08496961788256\n",
      "Epoch 297, Loss: 0.21828939671871705, Train Accuracy: 122.04862563719635, Test Accuracy: 173.44472254539022\n",
      "Epoch 298, Loss: 0.21787592595039623, Train Accuracy: 130.32498105506147, Test Accuracy: 193.5479921224166\n",
      "Epoch 299, Loss: 0.21744434854537412, Train Accuracy: 263.8010510083622, Test Accuracy: 153.3969711381562\n",
      "Epoch 300, Loss: 0.21703557384809727, Train Accuracy: 135.05226995021562, Test Accuracy: 131.18058518001013\n",
      "Epoch 301, Loss: 0.2166191027545588, Train Accuracy: 112.27342621450612, Test Accuracy: 153.75504577403166\n",
      "Epoch 302, Loss: 0.21622469744296577, Train Accuracy: 115.50644697193505, Test Accuracy: 470.53204587041114\n",
      "Epoch 303, Loss: 0.21593652487502188, Train Accuracy: 187.11005904951034, Test Accuracy: 209.67756697596337\n",
      "Epoch 304, Loss: 0.2154237894118199, Train Accuracy: 108.10146563006178, Test Accuracy: 112.28794265279964\n",
      "Epoch 305, Loss: 0.21504075810439574, Train Accuracy: 111.77682424165488, Test Accuracy: 107.86332434050891\n",
      "Epoch 306, Loss: 0.21464474015707186, Train Accuracy: 105.96796839607623, Test Accuracy: 105.23329546013657\n",
      "Epoch 307, Loss: 0.2142802740536891, Train Accuracy: 115.02118944965254, Test Accuracy: 129.13041325004733\n",
      "Epoch 308, Loss: 0.21394081193957828, Train Accuracy: 114.27903590630129, Test Accuracy: 113.58556620928707\n",
      "Epoch 309, Loss: 0.21428937243941673, Train Accuracy: 393.29425073035173, Test Accuracy: 118.39263577363928\n",
      "Epoch 310, Loss: 0.21319563437510014, Train Accuracy: 137.25812363572402, Test Accuracy: 103.15068957270408\n",
      "Epoch 311, Loss: 0.2128350058727423, Train Accuracy: 137.87963657671304, Test Accuracy: 109.64810258515027\n",
      "Epoch 312, Loss: 0.21269393092453381, Train Accuracy: 147.8894089379509, Test Accuracy: 121.79733759043168\n",
      "Epoch 313, Loss: 0.2121210939666619, Train Accuracy: 145.87769120631646, Test Accuracy: 123.2765973733396\n",
      "Epoch 314, Loss: 0.21178077259539124, Train Accuracy: 148.24940995731814, Test Accuracy: 541.5268803226704\n",
      "Epoch 315, Loss: 0.21143576251526305, Train Accuracy: 163.82081257592665, Test Accuracy: 139.06188665117537\n",
      "Epoch 316, Loss: 0.21108972298819978, Train Accuracy: 266.1102768712284, Test Accuracy: 221.87893227168493\n",
      "Epoch 317, Loss: 0.2107811783953963, Train Accuracy: 146.01331608144147, Test Accuracy: 177.32643055429264\n",
      "Epoch 318, Loss: 0.21044237608155256, Train Accuracy: 154.04610984017603, Test Accuracy: 140.8704051192926\n",
      "Epoch 319, Loss: 0.21010185693848177, Train Accuracy: 120.25851775154764, Test Accuracy: 431.9543671705285\n",
      "Epoch 320, Loss: 0.21027403691461558, Train Accuracy: 130.7109498789848, Test Accuracy: 200.45961924961634\n",
      "Epoch 321, Loss: 0.20972791365480267, Train Accuracy: 151.132921264782, Test Accuracy: 128.0061717714582\n",
      "Epoch 322, Loss: 0.20914239003638893, Train Accuracy: 147.14293451956118, Test Accuracy: 122.63276565318205\n",
      "Epoch 323, Loss: 0.20882431247232877, Train Accuracy: 120.44102703403331, Test Accuracy: 315.7754656538671\n",
      "Epoch 324, Loss: 0.20853786001744104, Train Accuracy: 212.31690483176735, Test Accuracy: 124.76765325118085\n",
      "Epoch 325, Loss: 0.20924375344911506, Train Accuracy: 954.5374198345961, Test Accuracy: 256.0998164196404\n",
      "Epoch 326, Loss: 0.2079089275608145, Train Accuracy: 210.6564951923796, Test Accuracy: 186.55453290744703\n",
      "Epoch 327, Loss: 0.2076125765816437, Train Accuracy: 221.92818592831134, Test Accuracy: 2008.2822650014139\n",
      "Epoch 328, Loss: 0.20730906095522858, Train Accuracy: 134.91932123964673, Test Accuracy: 145.08760257643095\n",
      "Epoch 329, Loss: 0.20700781329893364, Train Accuracy: 423.10016784834914, Test Accuracy: 123.15501779439498\n",
      "Epoch 330, Loss: 0.20672072951210138, Train Accuracy: 882.3784040455223, Test Accuracy: 114.87043281477325\n",
      "Epoch 331, Loss: 0.20643801797085587, Train Accuracy: 203.7404347259054, Test Accuracy: 119.95245365220673\n",
      "Epoch 332, Loss: 0.20619027587000271, Train Accuracy: 135.32026235801646, Test Accuracy: 443.4094572261888\n",
      "Epoch 333, Loss: 0.20588367042198974, Train Accuracy: 120.31329664985773, Test Accuracy: 129.9538411120979\n",
      "Epoch 334, Loss: 0.2056113608707256, Train Accuracy: 148.50649762310135, Test Accuracy: 129.07311116432658\n",
      "Epoch 335, Loss: 0.20534261134891268, Train Accuracy: 146.8482174445555, Test Accuracy: 176.69988801527995\n",
      "Epoch 336, Loss: 0.20507053361239658, Train Accuracy: 130.80892459360203, Test Accuracy: 351.3918950411738\n",
      "Epoch 337, Loss: 0.20516994525016136, Train Accuracy: 138.5534129987921, Test Accuracy: 120.34970053847955\n",
      "Epoch 338, Loss: 0.20454620214228067, Train Accuracy: 164.4361752566415, Test Accuracy: 116.7289593560355\n",
      "Epoch 339, Loss: 0.20432409245512678, Train Accuracy: 404.3867391840895, Test Accuracy: 105.95437281472343\n",
      "Epoch 340, Loss: 0.20402203984273276, Train Accuracy: 130.4286434666072, Test Accuracy: 118.38814058109206\n",
      "Epoch 341, Loss: 0.2049617085298136, Train Accuracy: 126.39900517202832, Test Accuracy: 198.32114149599659\n",
      "Epoch 342, Loss: 0.2035100635831984, Train Accuracy: 167.04558453048605, Test Accuracy: 107.51031326761051\n",
      "Epoch 343, Loss: 0.203253320075166, Train Accuracy: 116.02956189211923, Test Accuracy: 122.74895989165014\n",
      "Epoch 344, Loss: 0.203030507529615, Train Accuracy: 118.0016594433941, Test Accuracy: 112.2916011031793\n",
      "Epoch 345, Loss: 0.2027661066901762, Train Accuracy: 162.1751447130867, Test Accuracy: 102.74570916623486\n",
      "Epoch 346, Loss: 0.20257320151413655, Train Accuracy: 147.14507663484736, Test Accuracy: 109.66501539580676\n",
      "Epoch 347, Loss: 0.20252498583480943, Train Accuracy: 230.93906063689184, Test Accuracy: 262.1639818074752\n",
      "Epoch 348, Loss: 0.20204926148990343, Train Accuracy: 116.98517839861647, Test Accuracy: 121.12659127371651\n",
      "Epoch 349, Loss: 0.20182533153277746, Train Accuracy: 125.3513289103101, Test Accuracy: 123.83526568510095\n",
      "Epoch 350, Loss: 0.20157760488739568, Train Accuracy: 114.53768889178855, Test Accuracy: 136.58356795019034\n",
      "Epoch 351, Loss: 0.20141682252634308, Train Accuracy: 124.28084720302724, Test Accuracy: 114.45161406847896\n",
      "Epoch 352, Loss: 0.20113104829744027, Train Accuracy: 113.24988101817326, Test Accuracy: 172.4784557965337\n",
      "Epoch 353, Loss: 0.20090774211959192, Train Accuracy: 110.2769855190419, Test Accuracy: 113.45916111615239\n",
      "Epoch 354, Loss: 0.20111578529195237, Train Accuracy: 113.9247364319649, Test Accuracy: 107.4185765908689\n",
      "Epoch 355, Loss: 0.20044812758981007, Train Accuracy: 123.98129385148931, Test Accuracy: 115.5402503305552\n",
      "Epoch 356, Loss: 0.2002311745766526, Train Accuracy: 133.17990174909127, Test Accuracy: 206.16313079911836\n",
      "Epoch 357, Loss: 0.20004480558445503, Train Accuracy: 360.2769009446159, Test Accuracy: 136.84163735837353\n",
      "Epoch 358, Loss: 0.19979692982098077, Train Accuracy: 174.07614081067643, Test Accuracy: 120.26180370486513\n",
      "Epoch 359, Loss: 0.19958334801009403, Train Accuracy: 174.83241463124884, Test Accuracy: 113.69581121327926\n",
      "Epoch 360, Loss: 0.1993761930311062, Train Accuracy: 154.35836801591535, Test Accuracy: 142.17783622352445\n",
      "Epoch 361, Loss: 0.199165400433511, Train Accuracy: 235.2688183690578, Test Accuracy: 161.4338614016163\n",
      "Epoch 362, Loss: 0.2026409024863832, Train Accuracy: 140.09276910527268, Test Accuracy: 1181.3209241555662\n",
      "Epoch 363, Loss: 0.19875968228801433, Train Accuracy: 142.1455736974136, Test Accuracy: 185.84937811870964\n",
      "Epoch 364, Loss: 0.19852798267958494, Train Accuracy: 178.49537752180706, Test Accuracy: 122.78007773963772\n",
      "Epoch 365, Loss: 0.19835549753762505, Train Accuracy: 140.2366499619098, Test Accuracy: 215.8015190630543\n",
      "Epoch 366, Loss: 0.19823460786295669, Train Accuracy: 192.63556659456415, Test Accuracy: 126.19278927238621\n",
      "Epoch 367, Loss: 0.1982533252386364, Train Accuracy: 135.17206895690518, Test Accuracy: 122.2672392981393\n",
      "Epoch 368, Loss: 0.19824150077595132, Train Accuracy: 149.77313808382732, Test Accuracy: 120.1188832886365\n",
      "Epoch 369, Loss: 0.19754849852182998, Train Accuracy: 181.76655545224247, Test Accuracy: 141.10910045857332\n",
      "Epoch 370, Loss: 0.19734617778409835, Train Accuracy: 136.8653682733782, Test Accuracy: 117.97227538361841\n",
      "Epoch 371, Loss: 0.19715685739754588, Train Accuracy: 187.78956619028972, Test Accuracy: 267.3544120204692\n",
      "Epoch 372, Loss: 0.19694885266822493, Train Accuracy: 129.0457793805479, Test Accuracy: 242.1896468571254\n",
      "Epoch 373, Loss: 0.196778618679657, Train Accuracy: 173.59449511805562, Test Accuracy: 373.83354136408593\n",
      "Epoch 374, Loss: 0.19658898068306646, Train Accuracy: 114.23433220777783, Test Accuracy: 130.9518913346894\n",
      "Epoch 375, Loss: 0.19641030068087434, Train Accuracy: 139.15895601435355, Test Accuracy: 127.4770342768455\n",
      "Epoch 376, Loss: 0.2005516469266665, Train Accuracy: 121.06815396565577, Test Accuracy: 108.699746054046\n",
      "Epoch 377, Loss: 0.1959885820323302, Train Accuracy: 170.62300020547613, Test Accuracy: 102.29866526078204\n",
      "Epoch 378, Loss: 0.1958287436427463, Train Accuracy: 126.9651704867507, Test Accuracy: 127.09261061220752\n",
      "Epoch 379, Loss: 0.19564009666406187, Train Accuracy: 145.92321072000382, Test Accuracy: 117.01034450531006\n",
      "Epoch 380, Loss: 0.19545577819351131, Train Accuracy: 137.77455806940858, Test Accuracy: 218.0889919631335\n",
      "Epoch 381, Loss: 0.19527908211830736, Train Accuracy: 137.7082076625699, Test Accuracy: 109.60927840641567\n",
      "Epoch 382, Loss: 0.19895672588863897, Train Accuracy: 405.92766112780413, Test Accuracy: 100.35018350640122\n",
      "Epoch 383, Loss: 0.19489487326577934, Train Accuracy: 114.14957473053713, Test Accuracy: 100.52977715706339\n",
      "Epoch 384, Loss: 0.1949988799607809, Train Accuracy: 251.44291439098131, Test Accuracy: 133.73063321016272\n",
      "Epoch 385, Loss: 0.1945737850092172, Train Accuracy: 225.99332503103844, Test Accuracy: 244.93361889586157\n",
      "Epoch 386, Loss: 0.194381802172667, Train Accuracy: 125.41696168661639, Test Accuracy: 133.508896282741\n",
      "Epoch 387, Loss: 0.19422342009940216, Train Accuracy: 7774.228657111036, Test Accuracy: 163.3748651037411\n",
      "Epoch 388, Loss: 0.19404531249326107, Train Accuracy: 113.94397125411086, Test Accuracy: 108.27097100627665\n",
      "Epoch 389, Loss: 0.19386714186952567, Train Accuracy: 145.214834718266, Test Accuracy: 133.483333023227\n",
      "Epoch 390, Loss: 0.19538497355719722, Train Accuracy: 107.69466544554061, Test Accuracy: 103.9726349188357\n",
      "Epoch 391, Loss: 0.1935631619415835, Train Accuracy: 117.95587699940221, Test Accuracy: 122.31303872867507\n",
      "Epoch 392, Loss: 0.19339580054718927, Train Accuracy: 123.85183504634665, Test Accuracy: 117.23317872261515\n",
      "Epoch 393, Loss: 0.19324220936639433, Train Accuracy: 105.94724064165445, Test Accuracy: 96.51032745594881\n",
      "Epoch 394, Loss: 0.19325233110139062, Train Accuracy: 127.730983839515, Test Accuracy: 98.68190866587113\n",
      "Epoch 395, Loss: 0.19290555420349997, Train Accuracy: 130.4594414625439, Test Accuracy: 547.8126328721338\n",
      "Epoch 396, Loss: 0.1934791092436834, Train Accuracy: 105.62554688735395, Test Accuracy: 104.91156879736452\n",
      "Epoch 397, Loss: 0.19259285639881826, Train Accuracy: 113.72559613785023, Test Accuracy: 125.74807165106948\n",
      "Epoch 398, Loss: 0.19243764142858383, Train Accuracy: 134.75130424249093, Test Accuracy: 109.58961576345015\n",
      "Epoch 399, Loss: 0.19284499103419434, Train Accuracy: 114.76466212491759, Test Accuracy: 111.87249092179901\n",
      "Epoch 400, Loss: 0.19211053877204853, Train Accuracy: 111.96397062673088, Test Accuracy: 111.31332323502521\n",
      "Epoch 401, Loss: 0.19278469767433712, Train Accuracy: 120.78433745069107, Test Accuracy: 98.79156192468137\n",
      "Epoch 402, Loss: 0.19181320488481443, Train Accuracy: 125.80652409726733, Test Accuracy: 120.74606728067204\n",
      "Epoch 403, Loss: 0.1916682521682192, Train Accuracy: 136.27711653761583, Test Accuracy: 112.39805009413739\n",
      "Epoch 404, Loss: 0.1920750385672017, Train Accuracy: 134.53116810504403, Test Accuracy: 111.00251355463145\n",
      "Epoch 405, Loss: 0.19132308808870663, Train Accuracy: 134.14983189288583, Test Accuracy: 94.71482656439956\n",
      "Epoch 406, Loss: 0.1911910395536631, Train Accuracy: 167.47297498172952, Test Accuracy: 92.2067311150687\n",
      "Epoch 407, Loss: 0.19292179885253113, Train Accuracy: 116.94948813273557, Test Accuracy: 93.93117519300812\n",
      "Epoch 408, Loss: 0.19091416092658786, Train Accuracy: 136.22978508446357, Test Accuracy: 108.00266328149912\n",
      "Epoch 409, Loss: 0.1907591677415521, Train Accuracy: 132.29276595334778, Test Accuracy: 112.12131801916628\n",
      "Epoch 410, Loss: 0.1906169358899723, Train Accuracy: 174.01773787483867, Test Accuracy: 111.36442641822659\n",
      "Epoch 411, Loss: 0.19046582010853833, Train Accuracy: 229.7861209260035, Test Accuracy: 112.72560024261475\n",
      "Epoch 412, Loss: 0.19032513705964652, Train Accuracy: 338.08586803411237, Test Accuracy: 135.50187118685974\n",
      "Epoch 413, Loss: 0.19025625280640546, Train Accuracy: 119.99654283252274, Test Accuracy: 109.06548974951919\n",
      "Epoch 414, Loss: 0.1900292617743692, Train Accuracy: 430.88439658434174, Test Accuracy: 116.43127495901925\n",
      "Epoch 415, Loss: 0.18987456240224565, Train Accuracy: 109.6130363581217, Test Accuracy: 95.83601389125901\n",
      "Epoch 416, Loss: 0.18972708690041346, Train Accuracy: 2094.0541464223384, Test Accuracy: 94.50502000536237\n",
      "Epoch 417, Loss: 0.18959785622383593, Train Accuracy: 236.33149050622853, Test Accuracy: 96.7833692784212\n",
      "Epoch 418, Loss: 0.18943559144680033, Train Accuracy: 166.40529042417162, Test Accuracy: 173.19401024798958\n",
      "Epoch 419, Loss: 0.18930877413278693, Train Accuracy: 149.79771007646357, Test Accuracy: 125.32589143636275\n",
      "Epoch 420, Loss: 0.18919720865085035, Train Accuracy: 113.8565122909045, Test Accuracy: 110.70451146729138\n",
      "Epoch 421, Loss: 0.18901128129133984, Train Accuracy: 103.37625937597495, Test Accuracy: 96.91412427474042\n",
      "Epoch 422, Loss: 0.18888973229293685, Train Accuracy: 108.80848560291516, Test Accuracy: 100.90367766789028\n",
      "Epoch 423, Loss: 0.18874128536001272, Train Accuracy: 120.15181473412711, Test Accuracy: 110.34746135011011\n",
      "Epoch 424, Loss: 0.18862064984266358, Train Accuracy: 300.0142842299046, Test Accuracy: 107.97605931029028\n",
      "Epoch 425, Loss: 0.1884733115840516, Train Accuracy: 139.71664627375696, Test Accuracy: 122.84118333154795\n",
      "Epoch 426, Loss: 0.18834862452310142, Train Accuracy: 140.00789115621956, Test Accuracy: 2432.815028930197\n",
      "Epoch 427, Loss: 0.18821487461850536, Train Accuracy: 146.13093797070192, Test Accuracy: 119.7140102581102\n",
      "Epoch 428, Loss: 0.1880689878903199, Train Accuracy: 264.3379644815457, Test Accuracy: 128.1092547786479\n",
      "Epoch 429, Loss: 0.18792291942839173, Train Accuracy: 123.660555474346, Test Accuracy: 148.60364123753138\n",
      "Epoch 430, Loss: 0.18780319654959493, Train Accuracy: 173.63153523584947, Test Accuracy: 105.94400291053616\n",
      "Epoch 431, Loss: 0.1876674320587493, Train Accuracy: 142.1576252025379, Test Accuracy: 109.36086522316447\n",
      "Epoch 432, Loss: 0.18752639180485411, Train Accuracy: 110.3772139768371, Test Accuracy: 112.03959601266044\n",
      "Epoch 433, Loss: 0.18738057217855886, Train Accuracy: 120.66177446628295, Test Accuracy: 101.31667364860067\n",
      "Epoch 434, Loss: 0.1872670108934404, Train Accuracy: 110.65088580108576, Test Accuracy: 105.00687233282595\n",
      "Epoch 435, Loss: 0.18714367189287684, Train Accuracy: 148.87672896771252, Test Accuracy: 134.08213442199084\n",
      "Epoch 436, Loss: 0.18701263748545452, Train Accuracy: 104.24743056349473, Test Accuracy: 111.8259969244198\n",
      "Epoch 437, Loss: 0.18688373467772873, Train Accuracy: 142.33325713677272, Test Accuracy: 118.39656098034916\n",
      "Epoch 438, Loss: 0.18674416778735506, Train Accuracy: 112.46896008380915, Test Accuracy: 148.69140948081503\n",
      "Epoch 439, Loss: 0.18660240782948548, Train Accuracy: 162.33033507866725, Test Accuracy: 117.13194076382383\n",
      "Epoch 440, Loss: 0.18647675400687277, Train Accuracy: 122.28503429654913, Test Accuracy: 101.51021073789013\n",
      "Epoch 441, Loss: 0.1863739586830612, Train Accuracy: 153.2456185739463, Test Accuracy: 102.5442086044623\n",
      "Epoch 442, Loss: 0.18624099853287576, Train Accuracy: 160.27721227232684, Test Accuracy: 128.27436883108956\n",
      "Epoch 443, Loss: 0.18611391108886582, Train Accuracy: 110.74031560217526, Test Accuracy: 116.82192343108508\n",
      "Epoch 444, Loss: 0.18597191895252993, Train Accuracy: 138.4431220791272, Test Accuracy: 110.05189510267608\n",
      "Epoch 445, Loss: 0.18585435815591872, Train Accuracy: 149.73551984323714, Test Accuracy: 117.98465711243298\n",
      "Epoch 446, Loss: 0.18574504909274744, Train Accuracy: 215.6171178838617, Test Accuracy: 114.46258525459133\n",
      "Epoch 447, Loss: 0.18561537272482295, Train Accuracy: 241.02255400108822, Test Accuracy: 116.00750323704311\n",
      "Epoch 448, Loss: 0.18549364171025529, Train Accuracy: 129.73174658176413, Test Accuracy: 160.8139689503884\n",
      "Epoch 449, Loss: 0.18536666294058513, Train Accuracy: 121.01779997844405, Test Accuracy: 167.67952895651058\n",
      "Epoch 450, Loss: 0.18522789343874146, Train Accuracy: 106.55788524302143, Test Accuracy: 778.1190156158136\n",
      "Epoch 451, Loss: 0.18512689158487536, Train Accuracy: 108.40665341466992, Test Accuracy: 163.62891839475049\n",
      "Epoch 452, Loss: 0.18500803746242298, Train Accuracy: 129.35290745207203, Test Accuracy: 186.31793329667073\n",
      "Epoch 453, Loss: 0.1848663419265508, Train Accuracy: 122.83887783025496, Test Accuracy: 792.7536980375952\n",
      "Epoch 454, Loss: 0.18475494493222927, Train Accuracy: 121.09172028890063, Test Accuracy: 121.72914431046466\n",
      "Epoch 455, Loss: 0.18463318409292226, Train Accuracy: 116.43925151365852, Test Accuracy: 129.41508174429134\n",
      "Epoch 456, Loss: 0.1844981478321758, Train Accuracy: 154.6559988727194, Test Accuracy: 162.10794371001575\n",
      "Epoch 457, Loss: 0.18487752177008718, Train Accuracy: 174.02188350752616, Test Accuracy: 232.5205069561394\n",
      "Epoch 458, Loss: 0.1842868991615964, Train Accuracy: 113.76385851135922, Test Accuracy: 160.00285582639734\n",
      "Epoch 459, Loss: 0.18417206865529523, Train Accuracy: 151.629253220506, Test Accuracy: 142.67902934794523\n",
      "Epoch 460, Loss: 0.1840548381235866, Train Accuracy: 244.1988591000079, Test Accuracy: 119.41621309397172\n",
      "Epoch 461, Loss: 0.18398121066179526, Train Accuracy: 135.2594415009413, Test Accuracy: 127.5574502361064\n",
      "Epoch 462, Loss: 0.1838206500569012, Train Accuracy: 151.39461436365573, Test Accuracy: 166.01984895978654\n",
      "Epoch 463, Loss: 0.18370550617685694, Train Accuracy: 132.50854230083573, Test Accuracy: 131.62975901000354\n",
      "Epoch 464, Loss: 0.1835749495399533, Train Accuracy: 175.16766881629727, Test Accuracy: 129.76404861528047\n",
      "Epoch 465, Loss: 0.18346902289421704, Train Accuracy: 210.97707787645203, Test Accuracy: 140.53036115607438\n",
      "Epoch 466, Loss: 0.18334174799708408, Train Accuracy: 244.11021903307224, Test Accuracy: 166.05077833058883\n",
      "Epoch 467, Loss: 0.1832248801520885, Train Accuracy: 243.73529261415845, Test Accuracy: 134.22463257458745\n",
      "Epoch 468, Loss: 0.18312734613985768, Train Accuracy: 130.22929386497847, Test Accuracy: 166.254040990557\n",
      "Epoch 469, Loss: 0.18301812640067264, Train Accuracy: 1081.1127434444636, Test Accuracy: 117.5154190258104\n",
      "Epoch 470, Loss: 0.1829056705315587, Train Accuracy: 123.13447579982765, Test Accuracy: 115.25885739618418\n",
      "Epoch 471, Loss: 0.18280085997301393, Train Accuracy: 155.18511082254824, Test Accuracy: 156.9858759860603\n",
      "Epoch 472, Loss: 0.18268277880957828, Train Accuracy: 170.95022231588217, Test Accuracy: 135.0421407466032\n",
      "Epoch 473, Loss: 0.18256757062649928, Train Accuracy: 108.8019988145557, Test Accuracy: 130.24978520918864\n",
      "Epoch 474, Loss: 0.18245557597196402, Train Accuracy: 129.93088931327836, Test Accuracy: 108.7220438159242\n",
      "Epoch 475, Loss: 0.18234239896673798, Train Accuracy: 128.50877777700612, Test Accuracy: 117.75315977602588\n",
      "Epoch 476, Loss: 0.1822292862614898, Train Accuracy: 8943.493106132524, Test Accuracy: 111.71082222218416\n",
      "Epoch 477, Loss: 0.1821139508927612, Train Accuracy: 224.3000764575516, Test Accuracy: 154.5758929739193\n",
      "Epoch 478, Loss: 0.18201280573048756, Train Accuracy: 116.90031608397904, Test Accuracy: 105.69440503023108\n",
      "Epoch 479, Loss: 0.18257760861480946, Train Accuracy: 153.5634886190719, Test Accuracy: 213.99173006719474\n",
      "Epoch 480, Loss: 0.1817743246732952, Train Accuracy: 115.94231991590505, Test Accuracy: 105.25917713009581\n",
      "Epoch 481, Loss: 0.1816782057733648, Train Accuracy: 118.73455486673234, Test Accuracy: 102.11524288021788\n",
      "Epoch 482, Loss: 0.18156629079072462, Train Accuracy: 121.21397246312782, Test Accuracy: 100.64555929145034\n",
      "Epoch 483, Loss: 0.181452183277067, Train Accuracy: 218.45459988550232, Test Accuracy: 121.07646013765918\n",
      "Epoch 484, Loss: 0.3446766862531623, Train Accuracy: 139.7617688168582, Test Accuracy: 130.25473522653385\n",
      "Epoch 485, Loss: 0.18126312722971652, Train Accuracy: 233.50590909127334, Test Accuracy: 118.43721921103341\n",
      "Epoch 486, Loss: 0.18112675807921774, Train Accuracy: 145.54704532581556, Test Accuracy: 113.66725719218351\n",
      "Epoch 487, Loss: 0.180997720050383, Train Accuracy: 155.29466954571413, Test Accuracy: 124.84693505812665\n",
      "Epoch 488, Loss: 0.18084550710370523, Train Accuracy: 105.9805499270917, Test Accuracy: 524.0780109094113\n",
      "Epoch 489, Loss: 0.180738520914489, Train Accuracy: 115.78684278859612, Test Accuracy: 127.3868688661225\n",
      "Epoch 490, Loss: 0.18062170037665531, Train Accuracy: 131.92262743860158, Test Accuracy: 96.36716770639225\n",
      "Epoch 491, Loss: 0.18050950355277573, Train Accuracy: 146.222722502193, Test Accuracy: 94.4974172747865\n",
      "Epoch 492, Loss: 0.1803952689910271, Train Accuracy: 123.98628217334038, Test Accuracy: 107.71039055804818\n",
      "Epoch 493, Loss: 0.1802999939865288, Train Accuracy: 121.895979708081, Test Accuracy: 134.788951445599\n",
      "Epoch 494, Loss: 0.18017385905319153, Train Accuracy: 150.5516107087584, Test Accuracy: 107.04673120926837\n",
      "Epoch 495, Loss: 0.18010404289861084, Train Accuracy: 193.79301852470414, Test Accuracy: 101.30895427781708\n",
      "Epoch 496, Loss: 0.17994502559775502, Train Accuracy: 152.27011746963734, Test Accuracy: 104.47425606785988\n",
      "Epoch 497, Loss: 0.1798607247669649, Train Accuracy: 261.1893638009837, Test Accuracy: 125.75258404867989\n",
      "Epoch 498, Loss: 0.1797501363626455, Train Accuracy: 113.30063847013845, Test Accuracy: 118.99092041716284\n",
      "Epoch 499, Loss: 0.17962582124518572, Train Accuracy: 155.10240373674054, Test Accuracy: 110.4202046102407\n",
      "Epoch 500, Loss: 0.17953900053905614, Train Accuracy: 100.35746634919482, Test Accuracy: 99.20888741162358\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>███▇▇▆▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_mape</td><td>▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁█▁▁▁▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>time_test</td><td>▅▃█▆▃▃▂▁▁▅▇▂▂▁▁▂▂▅▂▂▂▃▃▃▂▃▄▂▂▂▄▃▃▃▂▂▃▂▂▂</td></tr><tr><td>time_train</td><td>▃▃▂▄▆▃▄█▁▁▂▂▃▂▆▃▂▄▂▃▁▄▃▃▂▂▂▂▂▂▂▅▃▂▂▃▃▂▃▂</td></tr><tr><td>train_loss</td><td>█▇▇▇▆▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_mape</td><td>▁▇▅▂▃▃▅▃█▄▃▂▃▃▂▂▃▄▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▃▄▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>0.12557</td></tr><tr><td>test_mape</td><td>99.20889</td></tr><tr><td>time_test</td><td>0.71234</td></tr><tr><td>time_train</td><td>0.88318</td></tr><tr><td>train_loss</td><td>0.17954</td></tr><tr><td>train_mape</td><td>100.35747</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">CNN-Komplexität-FullyConnectedModel-lr1e-05</strong> at: <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/l32qdxz2' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/l32qdxz2</a><br/> View project at: <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20241107_101208-l32qdxz2\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class FullyConnectedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullyConnectedModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(109, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "trained_model = test_model(\"FullyConnectedModel\", 3, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-07T09:25:50.171998600Z",
     "start_time": "2024-11-07T09:12:08.046853500Z"
    }
   },
   "id": "35794667631fe43c",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:38: DtypeWarning: Columns (3,4,5,6,11,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,45,46,47,49,50,107,110,114,115,116,119,120,121,124,125,126,128,131,132) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(filePath)\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:61: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Space extracted']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Plot_area_unified']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:72: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Availability']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:76: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['No. of rooms:']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.dataPipeline import DataPipeline\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "dp = DataPipeline()\n",
    "df = dp.runPipeline(normalizeAndStandardize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-07T16:34:04.195967Z",
     "start_time": "2024-11-07T16:33:56.858052Z"
    }
   },
   "id": "ebd5e6c5a57853ef",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.18.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\wandb\\run-20241107_174522-m1m0rtc8</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/m1m0rtc8' target=\"_blank\">MLP-bs16-lr1e-06</a></strong> to <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/m1m0rtc8' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/m1m0rtc8</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 155.7348597628044, Train MAPE: 184.50389505827073, Test MAPE: 167.94256217634998\n",
      "Epoch 2, Loss: 101.19729408161753, Train MAPE: 337.4542379553, Test MAPE: 230.37973963711912\n",
      "Epoch 3, Loss: 80.20769925496951, Train MAPE: 501.98705294125375, Test MAPE: 333.8668213694488\n",
      "Epoch 4, Loss: 75.79869537408376, Train MAPE: 706.670733443614, Test MAPE: 704.2004773370151\n",
      "Epoch 5, Loss: 70.88791314890378, Train MAPE: 467.58272961116364, Test MAPE: 442.568129139385\n",
      "Epoch 6, Loss: 65.86295672779687, Train MAPE: 538.8632627063836, Test MAPE: 374.4633936936828\n",
      "Epoch 7, Loss: 50.538664928447254, Train MAPE: 193.35206386021204, Test MAPE: 179.84531689055578\n",
      "Epoch 8, Loss: 77.6649203218184, Train MAPE: 708.1845845600103, Test MAPE: 1023.4720443108073\n",
      "Epoch 9, Loss: 61.485677391319236, Train MAPE: 2600.879924229213, Test MAPE: 315.40630262564883\n",
      "Epoch 10, Loss: 59.52022589201667, Train MAPE: 422.58447939826107, Test MAPE: 255.0069189619744\n",
      "Epoch 11, Loss: 60.26153136351193, Train MAPE: 340.99867189879154, Test MAPE: 172.71544081589272\n",
      "Epoch 12, Loss: 57.24572242506398, Train MAPE: 189.3220360500815, Test MAPE: 140.21548458808226\n",
      "Epoch 13, Loss: 56.08352690895132, Train MAPE: 1129.8568575366232, Test MAPE: 237.60717685560613\n",
      "Epoch 14, Loss: 55.844396975902214, Train MAPE: 373.9637497554918, Test MAPE: 203.66325611721052\n",
      "Epoch 15, Loss: 53.91166087216408, Train MAPE: 2141.94392560381, Test MAPE: 906.52160305173\n",
      "Epoch 16, Loss: 55.215033555419424, Train MAPE: 1461.9617183800513, Test MAPE: 389.77976726115435\n",
      "Epoch 17, Loss: 51.69434624344596, Train MAPE: 507.7687004668601, Test MAPE: 228.74941263162313\n",
      "Epoch 18, Loss: 55.30887663010097, Train MAPE: 756.9514196986548, Test MAPE: 375.0995209463712\n",
      "Epoch 19, Loss: 52.62444170003655, Train MAPE: 226.921285456459, Test MAPE: 140.70368681100138\n",
      "Epoch 20, Loss: 51.243512582230316, Train MAPE: 239.84291270503834, Test MAPE: 116.53680606637421\n",
      "Epoch 21, Loss: 53.510794216697306, Train MAPE: 154.91366728703105, Test MAPE: 77.60200567994538\n",
      "Epoch 22, Loss: 59.0241310364448, Train MAPE: 241.3756285735539, Test MAPE: 93.71062650077644\n",
      "Epoch 23, Loss: 52.28218175912292, Train MAPE: 130.46634256942616, Test MAPE: 78.89721317948967\n",
      "Epoch 24, Loss: 51.52858909093065, Train MAPE: 172.1433571092463, Test MAPE: 178.18314877049676\n",
      "Epoch 25, Loss: 49.5224408222182, Train MAPE: 179.18023033635842, Test MAPE: 95.65658766465168\n",
      "Epoch 26, Loss: 48.99918034515582, Train MAPE: 281.48037643012015, Test MAPE: 161.86245166661638\n",
      "Epoch 27, Loss: 50.25299880511466, Train MAPE: 163.90393822328014, Test MAPE: 279.71225958674347\n",
      "Epoch 28, Loss: 52.06285747425668, Train MAPE: 218.44686179375947, Test MAPE: 88.65263002220242\n",
      "Epoch 29, Loss: 48.53673823110666, Train MAPE: 124.38998734150957, Test MAPE: 79.47092016629333\n",
      "Epoch 30, Loss: 48.71893751655535, Train MAPE: 134.3503118032234, Test MAPE: 123.38683666945417\n",
      "Epoch 31, Loss: 50.02143045514108, Train MAPE: 161.98991723645796, Test MAPE: 121.58881830027276\n",
      "Epoch 32, Loss: 47.90053332983476, Train MAPE: 145.32887106339518, Test MAPE: 123.13260828900611\n",
      "Epoch 33, Loss: 49.08013095174517, Train MAPE: 12739.75511244045, Test MAPE: 11252.9944394868\n",
      "Epoch 34, Loss: 51.969689436064016, Train MAPE: 314.55018892658524, Test MAPE: 98.94561040447132\n",
      "Epoch 35, Loss: 47.046081396214454, Train MAPE: 270.14575495953073, Test MAPE: 220.49697840898887\n",
      "Epoch 36, Loss: 49.76819580570506, Train MAPE: 402.1517225578213, Test MAPE: 115.52848365023675\n",
      "Epoch 37, Loss: 48.91340133864951, Train MAPE: 615.737810585437, Test MAPE: 70.22989247356794\n",
      "Epoch 38, Loss: 47.6536610494524, Train MAPE: 690.873861998634, Test MAPE: 320.50868229482364\n",
      "Epoch 39, Loss: 53.102189616526076, Train MAPE: 105.54890666220584, Test MAPE: 86.88447773821966\n",
      "Epoch 40, Loss: 48.01894086170745, Train MAPE: 126.79930279536079, Test MAPE: 66.75779236870251\n",
      "Epoch 41, Loss: 45.274546401041235, Train MAPE: 1009.3389318215653, Test MAPE: 985.7732872506211\n",
      "Epoch 42, Loss: 50.91554248572996, Train MAPE: 71.6594583407573, Test MAPE: 60.38555744644326\n",
      "Epoch 43, Loss: 35.1751546227692, Train MAPE: 184.17609530601612, Test MAPE: 176.82572697314265\n",
      "Epoch 44, Loss: 61.28617383055353, Train MAPE: 88.83485939735877, Test MAPE: 50.88025227725734\n",
      "Epoch 45, Loss: 50.226224318332434, Train MAPE: 113.77870107782881, Test MAPE: 69.47797916036\n",
      "Epoch 46, Loss: 47.642623767528185, Train MAPE: 65.99963214321996, Test MAPE: 51.06585156004091\n",
      "Epoch 47, Loss: 48.18600141590645, Train MAPE: 71.53577275609925, Test MAPE: 56.4014483151308\n",
      "Epoch 48, Loss: 49.534269665540236, Train MAPE: 64.27310274648529, Test MAPE: 54.047968176132876\n",
      "Epoch 49, Loss: 51.17170973855034, Train MAPE: 79.68674145254627, Test MAPE: 49.94214742393786\n",
      "Epoch 50, Loss: 48.42723017970987, Train MAPE: 77.92386787209735, Test MAPE: 46.20050663874981\n",
      "Epoch 51, Loss: 49.6933270274507, Train MAPE: 129.9642504815973, Test MAPE: 59.892717733693765\n",
      "Epoch 52, Loss: 45.45940065806801, Train MAPE: 175.44493439979314, Test MAPE: 70.87964665113282\n",
      "Epoch 53, Loss: 50.81827777035513, Train MAPE: 244.88340978345843, Test MAPE: 61.53787499963095\n",
      "Epoch 54, Loss: 47.84465645052214, Train MAPE: 91.910571993826, Test MAPE: 55.04876411554914\n",
      "Epoch 55, Loss: 48.237915520814326, Train MAPE: 83.75458525628395, Test MAPE: 61.93839863479366\n",
      "Epoch 56, Loss: 48.18732054567291, Train MAPE: 942.0664987817821, Test MAPE: 83.49330615768945\n",
      "Epoch 57, Loss: 39.6291806069225, Train MAPE: 255.5938880313277, Test MAPE: 247.173517219865\n",
      "Epoch 58, Loss: 52.87272547589738, Train MAPE: 613.331878095208, Test MAPE: 265.76999469917854\n",
      "Epoch 59, Loss: 47.1122266382179, Train MAPE: 98.16667096806853, Test MAPE: 49.93271385892598\n",
      "Epoch 60, Loss: 47.95301119558877, Train MAPE: 217.5902943269404, Test MAPE: 51.71711617319977\n",
      "Epoch 61, Loss: 46.67822090925672, Train MAPE: 182.61190688438634, Test MAPE: 46.5560636118454\n",
      "Epoch 62, Loss: 46.94754383079386, Train MAPE: 81.44002199527287, Test MAPE: 64.06657810046755\n",
      "Epoch 63, Loss: 47.38444215887916, Train MAPE: 154.41998417928372, Test MAPE: 77.42514481672382\n",
      "Epoch 64, Loss: 48.061974176395886, Train MAPE: 158.68061830087683, Test MAPE: 59.62798908295759\n",
      "Epoch 65, Loss: 46.26165635302334, Train MAPE: 124.04624366085916, Test MAPE: 117.98705473850514\n",
      "Epoch 66, Loss: 49.28684144703563, Train MAPE: 719.3060643359776, Test MAPE: 44.84963819076275\n",
      "Epoch 67, Loss: 45.54076848265537, Train MAPE: 151.2984077219538, Test MAPE: 100.45024528357261\n",
      "Epoch 68, Loss: 47.658211254319205, Train MAPE: 256.79493261555126, Test MAPE: 110.71751758787367\n",
      "Epoch 69, Loss: 47.35559511344705, Train MAPE: 166.0194722572886, Test MAPE: 41.54036781340267\n",
      "Epoch 70, Loss: 46.16682966643532, Train MAPE: 76.4012971508423, Test MAPE: 41.81974519532302\n",
      "Epoch 71, Loss: 47.283889700559506, Train MAPE: 102.4071412272787, Test MAPE: 70.88681852383633\n",
      "Epoch 72, Loss: 47.75108979395229, Train MAPE: 92.74026202058289, Test MAPE: 67.82875384772875\n",
      "Epoch 73, Loss: 47.17933721807522, Train MAPE: 110.65635017294898, Test MAPE: 42.32362723259177\n",
      "Epoch 74, Loss: 46.906500336536396, Train MAPE: 439.8475934312762, Test MAPE: 53.07035620527706\n",
      "Epoch 75, Loss: 47.51639959944746, Train MAPE: 98.74994758791571, Test MAPE: 49.31850710317093\n",
      "Epoch 76, Loss: 46.7411019430636, Train MAPE: 2144.2320763175576, Test MAPE: 232.0598693839435\n",
      "Epoch 77, Loss: 46.77575074420565, Train MAPE: 78.08721542724142, Test MAPE: 60.36725910230615\n",
      "Epoch 78, Loss: 38.5549270181505, Train MAPE: 270.0534697264747, Test MAPE: 230.91561397099403\n",
      "Epoch 79, Loss: 53.147980998483625, Train MAPE: 274.1866712999938, Test MAPE: 318.2450885114999\n",
      "Epoch 80, Loss: 40.81568407446632, Train MAPE: 442.3477015728461, Test MAPE: 436.1375338382648\n",
      "Epoch 81, Loss: 52.438225606143874, Train MAPE: 65.00677348486187, Test MAPE: 34.3227235352856\n",
      "Epoch 82, Loss: 47.7841908064236, Train MAPE: 578.4514554573271, Test MAPE: 521.0433006432778\n",
      "Epoch 83, Loss: 45.60601135171157, Train MAPE: 193.15933163168333, Test MAPE: 177.72181388884212\n",
      "Epoch 84, Loss: 46.65524285408817, Train MAPE: 136.33515278422135, Test MAPE: 76.30705593006822\n",
      "Epoch 85, Loss: 45.97608314498844, Train MAPE: 372.21625494516934, Test MAPE: 51.00713395387277\n",
      "Epoch 86, Loss: 44.911172311235134, Train MAPE: 299.7213821191687, Test MAPE: 187.71774859081282\n",
      "Epoch 87, Loss: 49.35043628469524, Train MAPE: 239.19394855737, Test MAPE: 74.44719557195788\n",
      "Epoch 88, Loss: 46.06141756113515, Train MAPE: 92.46844441395821, Test MAPE: 89.78699415899328\n",
      "Epoch 89, Loss: 50.174421206302405, Train MAPE: 179.3208913327635, Test MAPE: 41.07446408545834\n",
      "Epoch 90, Loss: 43.40082256763116, Train MAPE: 85.55137925500037, Test MAPE: 71.06018208909309\n",
      "Epoch 91, Loss: 47.82527377975753, Train MAPE: 79.44758155321915, Test MAPE: 29.769283034335608\n",
      "Epoch 92, Loss: 47.239968326610665, Train MAPE: 44.68682826931159, Test MAPE: 28.6268295428762\n",
      "Epoch 93, Loss: 45.70994217169479, Train MAPE: 53.77769272027971, Test MAPE: 28.029746136446107\n",
      "Epoch 94, Loss: 45.843092845075994, Train MAPE: 52.96844520310542, Test MAPE: 36.55187340577444\n",
      "Epoch 95, Loss: 44.9121482243131, Train MAPE: 1329.6449815342319, Test MAPE: 150.45553860719176\n",
      "Epoch 96, Loss: 48.70476982754692, Train MAPE: 87.27931974946824, Test MAPE: 57.16123874708154\n",
      "Epoch 97, Loss: 44.10884538570049, Train MAPE: 49.201980210390666, Test MAPE: 45.06875199239373\n",
      "Epoch 98, Loss: 50.60429389113319, Train MAPE: 91.99054378562555, Test MAPE: 48.999739628642\n",
      "Epoch 99, Loss: 43.94767061513085, Train MAPE: 51.80083708795125, Test MAPE: 51.073183204022406\n",
      "Epoch 100, Loss: 46.194933392457514, Train MAPE: 75.91118592303872, Test MAPE: 36.42517506893567\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▄▃▃▂▁▁▁▁▁▁</td></tr><tr><td>test_mape</td><td>▃▆▄▃▂▂█▂▂▂▁▂▂▂▃▁▃▁▁▁▁▁▁▁▁▁▂▁▂▁▃▄▅▁▂▁▁▁▁▁</td></tr><tr><td>time_test</td><td>█▃█▂▁▁▁▃▁▂▄▄▁▂▁▁▁▁▁▁▄▁▁▁▁▂▃▁▁▁▄▃▅▄▂▄▂▄▂▂</td></tr><tr><td>time_train</td><td>▅▇▂█▂▁▁▁▁▅▄▆▁▁▁▂▄▁▅▁▃▃▅▂▂▂▃▃▅▇▅▄▄▂▂▂▅▂▂▂</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▂</td></tr><tr><td>train_mape</td><td>▂▃▃▂█▂▂▂▂▁▂▂▃▃▅▁▁▁▁▂▂▂▂▁▂▂▁▁▁▁▂▁▂▃▃▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>11.43581</td></tr><tr><td>test_mape</td><td>36.42518</td></tr><tr><td>time_test</td><td>1.05907</td></tr><tr><td>time_train</td><td>2.26135</td></tr><tr><td>train_loss</td><td>46.19493</td></tr><tr><td>train_mape</td><td>75.91119</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">MLP-bs16-lr1e-06</strong> at: <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/m1m0rtc8' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/m1m0rtc8</a><br/> View project at: <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20241107_174522-m1m0rtc8\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src import torchModelRun\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class FullyConnectedModel(nn.Module):\n",
    "    import torch.nn.functional as F\n",
    "    def __init__(self):\n",
    "        super(FullyConnectedModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(109, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "models = torchModelRun.run(FullyConnectedModel, df, 3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-07T16:51:04.937223900Z",
     "start_time": "2024-11-07T16:45:22.458006600Z"
    }
   },
   "id": "1ec75e098d8fb34e",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.06252029 -0.16141141 -0.15081525 -0.15818127 -0.16118794  0.07330458\n",
      "  0.01860683 -0.05420477 -0.15818149 -0.16125587 -0.210203   -0.1607161\n",
      " -0.10421573 -0.31844458 -0.30211263 -0.2543027  -0.26567123 -0.19613857\n",
      " -0.13621395  0.21723425 -0.16156815  0.10853035 -0.29733819 -0.26276898\n",
      " -0.23044499 -0.04922958 -0.13332496 -0.07904496 -0.16249442 -0.15641817\n",
      " -0.15930265 -0.0863231  -0.27647055 -0.12237486 -0.06379804 -0.24325078\n",
      " -0.01523485 -0.13062437  0.01332896 -0.10886941 -0.06211918 -0.35097407\n",
      " -0.12816296 -0.10699056 -0.18118609 -0.37021714 -0.36131536 -0.17176833\n",
      " -0.14970704 -0.22814913 -0.28677628 -0.30713192 -0.32764697 -0.14018795\n",
      " -0.1119663  -0.11196448 -0.15238566 -0.15870255 -0.16169256 -0.16107613\n",
      " -0.15992203 -0.05349623 -0.25859084  0.08188449 -0.15569916 -0.01211535\n",
      " -0.04884041 -0.09895917 -0.01184354 -0.00692593 -0.10208501 -0.09608129\n",
      " -0.03761226 -0.01421981 -0.00692593 -0.00692593 -0.01839305 -0.02363568\n",
      " -0.05873742 -0.09777585 -0.01209366 -0.09965065 -0.0504576  -0.05276182\n",
      " -0.02603295 -0.12575228 -0.0876858  -0.01160629 -0.00927651 -0.07966063\n",
      " -0.23669942 -0.01026758 -0.11945548 -0.0348824  -0.17484111 -0.01796556\n",
      " -0.02594998  1.62483156 -0.04698762 -0.00692593 -0.13180543 -0.00692593\n",
      " -0.07130911 -0.03338843 -0.02923016 -0.02793807 -0.14050177 -0.00830236\n",
      " -0.22299811]\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[0.0823]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exclude = df.drop(columns=['price_cleaned'])\n",
    "datapoint = df_exclude.iloc[0].values\n",
    "print(datapoint)\n",
    "\n",
    "model = models[0]\n",
    "model.eval()\n",
    "model = model.to('cpu')\n",
    "model(torch.tensor(datapoint, dtype=torch.float32).unsqueeze(0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-07T17:04:34.353154900Z",
     "start_time": "2024-11-07T17:04:34.303969100Z"
    }
   },
   "id": "93d832963b9862d6",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "np.float64(0.08544731460740697)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['price_cleaned'].iloc[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-07T17:05:06.568169700Z",
     "start_time": "2024-11-07T17:05:06.561134400Z"
    }
   },
   "id": "b1d875d245b45d93",
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
