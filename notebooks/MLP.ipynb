{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-18T09:01:41.981930500Z",
     "start_time": "2024-12-18T08:59:34.461088100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:43: DtypeWarning: Columns (3,4,5,6,11,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,45,46,47,49,50,107,110,114,115,116,119,120,121,124,125,126,128,131,132) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(filePath)\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:70: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Space extracted']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:75: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Plot_area_unified']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:81: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Availability']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "import os\n",
    "from src.dataPipeline import DataPipeline\n",
    "\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "dp = DataPipeline()\n",
    "df = dp.runPipeline(normalizeAndStandardize=False, imputer=imputer)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input nums: 109\n"
     ]
    }
   ],
   "source": [
    "inputs_nums = len(df.columns) - 1\n",
    "print(f'input nums: {inputs_nums}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-18T09:01:42.008870900Z",
     "start_time": "2024-12-18T09:01:41.984162100Z"
    }
   },
   "id": "4cb72ead071d529e",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       Floor  detail_responsive#surface_usable  Floor_space_merged  \\\n0        4.0                             187.2               159.2   \n1        2.4                             191.8               242.0   \n2        2.0                             162.0               112.6   \n3        2.2                             191.8               257.0   \n4        0.0                             197.6               228.0   \n...      ...                               ...                 ...   \n20791    0.8                              67.8               100.8   \n20792    3.8                             207.2               249.2   \n20793    0.0                             233.2               318.0   \n20794    1.8                             167.8               149.8   \n20795    1.0                             361.2               122.6   \n\n       ForestDensityM  ForestDensityS  RiversAndLakesL  RiversAndLakesM  \\\n0            0.286451        0.090908         0.082170         0.001811   \n1            0.286451        0.090908         0.082170         0.001811   \n2            0.095877        0.001911         0.154274         0.188229   \n3            0.286451        0.090908         0.082170         0.001811   \n4            0.279276        0.145835         0.109586         0.141473   \n...               ...             ...              ...              ...   \n20791        0.385885        0.097278         0.000000         0.000000   \n20792        0.000000        0.000000         0.013997         0.000000   \n20793        0.000000        0.000000         0.000000         0.000000   \n20794        0.151363        0.038351         0.026686         0.021426   \n20795        0.153670        0.113892         0.055729         0.034357   \n\n       RiversAndLakesS  distanceToTrainStation  gde_area_forest_percentage  \\\n0             0.011871                3.038467                   51.449275   \n1             0.011871                3.038467                   51.449275   \n2             0.000000                0.909587                   32.197891   \n3             0.011871                3.038467                   51.449275   \n4             0.091805                1.460245                   49.705635   \n...                ...                     ...                         ...   \n20791         0.000000                1.375319                   29.028213   \n20792         0.000000                1.067011                   30.599295   \n20793         0.000000                0.230859                   30.599295   \n20794         0.000000                0.357339                   26.718547   \n20795         0.000000                0.530367                   45.357143   \n\n       ...  Swimming pool  View  Washing machine  Waste water connection  \\\n0      ...            0.0   0.0              0.0                     0.0   \n1      ...            0.0   0.0              0.0                     0.0   \n2      ...            0.0   0.0              0.0                     0.0   \n3      ...            0.0   0.0              0.0                     0.0   \n4      ...            0.0   0.0              0.0                     0.0   \n...    ...            ...   ...              ...                     ...   \n20791  ...            0.0   0.0              0.0                     0.0   \n20792  ...            0.0   1.0              1.0                     0.0   \n20793  ...            0.0   0.0              0.0                     0.0   \n20794  ...            0.0   1.0              0.0                     0.0   \n20795  ...            0.0   0.0              0.0                     0.0   \n\n       Water connection  Wheelchair access  With a summer house  covered  \\\n0                   0.0                0.0                  0.0      0.0   \n1                   0.0                0.0                  0.0      0.0   \n2                   0.0                0.0                  0.0      0.0   \n3                   0.0                0.0                  0.0      0.0   \n4                   0.0                0.0                  0.0      0.0   \n...                 ...                ...                  ...      ...   \n20791               0.0                0.0                  0.0      0.0   \n20792               0.0                1.0                  0.0      0.0   \n20793               0.0                0.0                  0.0      0.0   \n20794               0.0                1.0                  0.0      0.0   \n20795               0.0                0.0                  0.0      0.0   \n\n       Availability_Immediately  Availability_On request  \n0                           0.0                      1.0  \n1                           0.0                      1.0  \n2                           1.0                      0.0  \n3                           0.0                      1.0  \n4                           0.0                      1.0  \n...                         ...                      ...  \n20791                       0.0                      0.0  \n20792                       0.0                      0.0  \n20793                       0.0                      0.0  \n20794                       0.0                      0.0  \n20795                       0.0                      0.0  \n\n[20796 rows x 110 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Floor</th>\n      <th>detail_responsive#surface_usable</th>\n      <th>Floor_space_merged</th>\n      <th>ForestDensityM</th>\n      <th>ForestDensityS</th>\n      <th>RiversAndLakesL</th>\n      <th>RiversAndLakesM</th>\n      <th>RiversAndLakesS</th>\n      <th>distanceToTrainStation</th>\n      <th>gde_area_forest_percentage</th>\n      <th>...</th>\n      <th>Swimming pool</th>\n      <th>View</th>\n      <th>Washing machine</th>\n      <th>Waste water connection</th>\n      <th>Water connection</th>\n      <th>Wheelchair access</th>\n      <th>With a summer house</th>\n      <th>covered</th>\n      <th>Availability_Immediately</th>\n      <th>Availability_On request</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.0</td>\n      <td>187.2</td>\n      <td>159.2</td>\n      <td>0.286451</td>\n      <td>0.090908</td>\n      <td>0.082170</td>\n      <td>0.001811</td>\n      <td>0.011871</td>\n      <td>3.038467</td>\n      <td>51.449275</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.4</td>\n      <td>191.8</td>\n      <td>242.0</td>\n      <td>0.286451</td>\n      <td>0.090908</td>\n      <td>0.082170</td>\n      <td>0.001811</td>\n      <td>0.011871</td>\n      <td>3.038467</td>\n      <td>51.449275</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>162.0</td>\n      <td>112.6</td>\n      <td>0.095877</td>\n      <td>0.001911</td>\n      <td>0.154274</td>\n      <td>0.188229</td>\n      <td>0.000000</td>\n      <td>0.909587</td>\n      <td>32.197891</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.2</td>\n      <td>191.8</td>\n      <td>257.0</td>\n      <td>0.286451</td>\n      <td>0.090908</td>\n      <td>0.082170</td>\n      <td>0.001811</td>\n      <td>0.011871</td>\n      <td>3.038467</td>\n      <td>51.449275</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>197.6</td>\n      <td>228.0</td>\n      <td>0.279276</td>\n      <td>0.145835</td>\n      <td>0.109586</td>\n      <td>0.141473</td>\n      <td>0.091805</td>\n      <td>1.460245</td>\n      <td>49.705635</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20791</th>\n      <td>0.8</td>\n      <td>67.8</td>\n      <td>100.8</td>\n      <td>0.385885</td>\n      <td>0.097278</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.375319</td>\n      <td>29.028213</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20792</th>\n      <td>3.8</td>\n      <td>207.2</td>\n      <td>249.2</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.013997</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.067011</td>\n      <td>30.599295</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20793</th>\n      <td>0.0</td>\n      <td>233.2</td>\n      <td>318.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.230859</td>\n      <td>30.599295</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20794</th>\n      <td>1.8</td>\n      <td>167.8</td>\n      <td>149.8</td>\n      <td>0.151363</td>\n      <td>0.038351</td>\n      <td>0.026686</td>\n      <td>0.021426</td>\n      <td>0.000000</td>\n      <td>0.357339</td>\n      <td>26.718547</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20795</th>\n      <td>1.0</td>\n      <td>361.2</td>\n      <td>122.6</td>\n      <td>0.153670</td>\n      <td>0.113892</td>\n      <td>0.055729</td>\n      <td>0.034357</td>\n      <td>0.000000</td>\n      <td>0.530367</td>\n      <td>45.357143</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>20796 rows Ã— 110 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-18T09:01:42.059513800Z",
     "start_time": "2024-12-18T09:01:41.991486900Z"
    }
   },
   "id": "2384d723e80bcf2e",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: denis-schatzmann. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.18.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\wandb\\run-20241218_093356-zlpn57jh</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/zlpn57jh' target=\"_blank\">MLP-bs16-lr1e-05</a></strong> to <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/zlpn57jh' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/zlpn57jh</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 188.30583114624022, Train MAPE: 103432389.37307692, Test MAPE: 102706437.18461539\n",
      "Epoch 2, Loss: 176.92242802840013, Train MAPE: 63027433.125, Test MAPE: 62600942.93076923\n",
      "Epoch 3, Loss: 159.32121144808255, Train MAPE: 22491451.30096154, Test MAPE: 22360558.915384617\n",
      "Epoch 4, Loss: 105.09867888230544, Train MAPE: 145687.79105694112, Test MAPE: 145772.50787259615\n",
      "Epoch 5, Loss: 16.773785338034997, Train MAPE: 1242.786384978661, Test MAPE: 1266.4391717764047\n",
      "Epoch 6, Loss: 8.201833857137423, Train MAPE: 790.0660057141231, Test MAPE: 802.0024293165941\n",
      "Epoch 7, Loss: 5.992067961853284, Train MAPE: 470.5810819772574, Test MAPE: 476.3439805837778\n",
      "Epoch 8, Loss: 4.595311076652545, Train MAPE: 338.61658974427445, Test MAPE: 341.5693550109863\n",
      "Epoch 9, Loss: 3.619505343786799, Train MAPE: 248.7800445483281, Test MAPE: 250.2769607250507\n",
      "Epoch 10, Loss: 2.9367134598871836, Train MAPE: 204.60998409711397, Test MAPE: 205.16182760091928\n",
      "Epoch 11, Loss: 2.4358076425125965, Train MAPE: 175.31227323825541, Test MAPE: 175.0542277262761\n",
      "Epoch 12, Loss: 2.0599705362262632, Train MAPE: 146.76125139456528, Test MAPE: 146.07011127471924\n",
      "Epoch 13, Loss: 1.7742599891928525, Train MAPE: 130.7709014415741, Test MAPE: 129.6375174155602\n",
      "Epoch 14, Loss: 1.5533543175802782, Train MAPE: 116.6700116781088, Test MAPE: 115.21789744450496\n",
      "Epoch 15, Loss: 1.381376323218529, Train MAPE: 104.57120710152846, Test MAPE: 102.90738108708308\n",
      "Epoch 16, Loss: 1.2462262308081755, Train MAPE: 100.43289716427142, Test MAPE: 98.59574111058161\n",
      "Epoch 17, Loss: 1.1387150087178899, Train MAPE: 95.18972912935111, Test MAPE: 93.24572448730468\n",
      "Epoch 18, Loss: 1.0522744006692217, Train MAPE: 89.46496551587032, Test MAPE: 87.47701216477614\n",
      "Epoch 19, Loss: 0.9810203391055649, Train MAPE: 86.81745149539067, Test MAPE: 84.72199135560255\n",
      "Epoch 20, Loss: 0.9223496133318314, Train MAPE: 82.97922387306507, Test MAPE: 80.86136608857375\n",
      "Epoch 21, Loss: 0.8736073900730564, Train MAPE: 81.42350371434138, Test MAPE: 79.22707778490506\n",
      "Epoch 22, Loss: 0.8317014366961443, Train MAPE: 78.17915497743166, Test MAPE: 75.97365000798152\n",
      "Epoch 23, Loss: 0.7969483512525375, Train MAPE: 77.9553827395806, Test MAPE: 75.67799650338979\n",
      "Epoch 24, Loss: 0.765828541931338, Train MAPE: 74.5014053087968, Test MAPE: 72.30087100542508\n",
      "Epoch 25, Loss: 0.7395153468188185, Train MAPE: 72.45225848234617, Test MAPE: 70.27417141107412\n",
      "Epoch 26, Loss: 0.7151502590626478, Train MAPE: 74.25773207774529, Test MAPE: 71.97301002649161\n",
      "Epoch 27, Loss: 0.694616414563587, Train MAPE: 70.29228371106662, Test MAPE: 68.1421536225539\n",
      "Epoch 28, Loss: 0.6755126191016573, Train MAPE: 70.0778058217122, Test MAPE: 67.89619109813984\n",
      "Epoch 29, Loss: 0.6585881072096527, Train MAPE: 68.94591261973748, Test MAPE: 66.83067363592295\n",
      "Epoch 30, Loss: 0.6429409134918107, Train MAPE: 68.78560337653526, Test MAPE: 66.672245993981\n",
      "Epoch 31, Loss: 0.6287771216163841, Train MAPE: 68.00915194841532, Test MAPE: 65.94965282586905\n",
      "Epoch 32, Loss: 0.6156346860556648, Train MAPE: 66.22275290672596, Test MAPE: 64.27050837736863\n",
      "Epoch 33, Loss: 0.6034024060990375, Train MAPE: 65.34523037580344, Test MAPE: 63.44553283544687\n",
      "Epoch 34, Loss: 0.5920938947739509, Train MAPE: 64.02638976573944, Test MAPE: 62.21426957937388\n",
      "Epoch 35, Loss: 0.5812988539799475, Train MAPE: 63.2866123181123, Test MAPE: 61.5306843317472\n",
      "Epoch 36, Loss: 0.5711591056189858, Train MAPE: 65.10179474537189, Test MAPE: 63.27899885911208\n",
      "Epoch 37, Loss: 0.561907049590865, Train MAPE: 63.28927748203277, Test MAPE: 61.58113048993624\n",
      "Epoch 38, Loss: 0.5527675108769192, Train MAPE: 62.30269156969511, Test MAPE: 60.67427896352915\n",
      "Epoch 39, Loss: 0.5442788951886961, Train MAPE: 61.53106266168447, Test MAPE: 59.96152531550481\n",
      "Epoch 40, Loss: 0.5362121085421397, Train MAPE: 60.44776890277863, Test MAPE: 58.95338375384991\n",
      "Epoch 41, Loss: 0.5285310200845393, Train MAPE: 61.501182609338024, Test MAPE: 59.981444696279674\n",
      "Epoch 42, Loss: 0.5210162387516063, Train MAPE: 60.351388199512776, Test MAPE: 58.91757813967191\n",
      "Epoch 43, Loss: 0.5138581763093288, Train MAPE: 60.41087457950299, Test MAPE: 59.00126799803514\n",
      "Epoch 44, Loss: 0.5077509462260283, Train MAPE: 58.507613429656395, Test MAPE: 57.21720179777879\n",
      "Epoch 45, Loss: 0.5006573588587344, Train MAPE: 58.994873829988336, Test MAPE: 57.706091345273535\n",
      "Epoch 46, Loss: 0.49438402558175415, Train MAPE: 58.40693815671481, Test MAPE: 57.16825416271503\n",
      "Epoch 47, Loss: 0.48824894978449895, Train MAPE: 57.65042338187878, Test MAPE: 56.47381014457116\n",
      "Epoch 48, Loss: 0.4823299154137763, Train MAPE: 57.68078804199512, Test MAPE: 56.52429389953613\n",
      "Epoch 49, Loss: 0.47652834239057623, Train MAPE: 57.326917114624614, Test MAPE: 56.21968195988582\n",
      "Epoch 50, Loss: 0.47111168447881935, Train MAPE: 57.408934908646806, Test MAPE: 56.327681922912596\n",
      "Epoch 51, Loss: 0.46568315196065946, Train MAPE: 55.693489105884844, Test MAPE: 54.700598760751575\n",
      "Epoch 52, Loss: 0.46057378790126396, Train MAPE: 55.505420734332155, Test MAPE: 54.54171254818256\n",
      "Epoch 53, Loss: 0.4554024909766248, Train MAPE: 54.88809470763573, Test MAPE: 53.96760027225201\n",
      "Epoch 54, Loss: 0.4505919284545458, Train MAPE: 55.49853926438551, Test MAPE: 54.584114544208234\n",
      "Epoch 55, Loss: 0.4458609096132792, Train MAPE: 55.62968791814951, Test MAPE: 54.73524938730093\n",
      "Epoch 56, Loss: 0.44130334036711316, Train MAPE: 54.80348002360417, Test MAPE: 53.961446923476\n",
      "Epoch 57, Loss: 0.4366450080504784, Train MAPE: 53.81698947319617, Test MAPE: 53.02559842329759\n",
      "Epoch 58, Loss: 0.43226496012021715, Train MAPE: 53.524609143917374, Test MAPE: 52.76735924940843\n",
      "Epoch 59, Loss: 0.4279523696893683, Train MAPE: 53.612439337143535, Test MAPE: 52.87898344626794\n",
      "Epoch 60, Loss: 0.4238049027366707, Train MAPE: 53.77803978919983, Test MAPE: 53.053405548976016\n",
      "Epoch 61, Loss: 0.419778662490157, Train MAPE: 52.91447686598851, Test MAPE: 52.23778001345121\n",
      "Epoch 62, Loss: 0.4157791810611693, Train MAPE: 52.97407222069227, Test MAPE: 52.310920693324164\n",
      "Epoch 63, Loss: 0.41192039163926475, Train MAPE: 52.660532402992246, Test MAPE: 52.0286779330327\n",
      "Epoch 64, Loss: 0.4081051057491165, Train MAPE: 52.02778509763571, Test MAPE: 51.438556832533614\n",
      "Epoch 65, Loss: 0.40437348282251223, Train MAPE: 52.184400393412666, Test MAPE: 51.62470293045044\n",
      "Epoch 66, Loss: 0.4007347845329115, Train MAPE: 51.1479016157297, Test MAPE: 50.63300412984995\n",
      "Epoch 67, Loss: 0.3972632558586506, Train MAPE: 51.792908873924844, Test MAPE: 51.27864698263315\n",
      "Epoch 68, Loss: 0.3936831640652739, Train MAPE: 51.15784484973321, Test MAPE: 50.690522105877214\n",
      "Epoch 69, Loss: 0.3902893664673544, Train MAPE: 50.90801125673147, Test MAPE: 50.4610962500939\n",
      "Epoch 70, Loss: 0.3869766583236364, Train MAPE: 51.067467852739185, Test MAPE: 50.6344755246089\n",
      "Epoch 71, Loss: 0.38373010886761433, Train MAPE: 50.54490319582132, Test MAPE: 50.159089675316444\n",
      "Epoch 72, Loss: 0.3805285725456018, Train MAPE: 50.653973533557014, Test MAPE: 50.28209922497089\n",
      "Epoch 73, Loss: 0.37742344150319695, Train MAPE: 49.798958782049326, Test MAPE: 49.469772705665\n",
      "Epoch 74, Loss: 0.37433773230474726, Train MAPE: 49.501941899152904, Test MAPE: 49.20076485413772\n",
      "Epoch 75, Loss: 0.37119523885731515, Train MAPE: 48.61713787408976, Test MAPE: 48.33497401017409\n",
      "Epoch 76, Loss: 0.36843139249831436, Train MAPE: 49.3835850073741, Test MAPE: 49.12062197465163\n",
      "Epoch 77, Loss: 0.36544517072347493, Train MAPE: 48.48415726331564, Test MAPE: 48.24821049616887\n",
      "Epoch 78, Loss: 0.3625482959887729, Train MAPE: 48.28506421309251, Test MAPE: 48.078165853940526\n",
      "Epoch 79, Loss: 0.35968142863577945, Train MAPE: 47.97729744544396, Test MAPE: 47.79504687235906\n",
      "Epoch 80, Loss: 0.3569795400763933, Train MAPE: 48.624013229516834, Test MAPE: 48.44776530632606\n",
      "Epoch 81, Loss: 0.35431253884990627, Train MAPE: 47.61120922932258, Test MAPE: 47.46968140235314\n",
      "Epoch 82, Loss: 0.3516488553383029, Train MAPE: 48.16465032467475, Test MAPE: 48.03207798004151\n",
      "Epoch 83, Loss: 0.3490549578689612, Train MAPE: 47.695699836657596, Test MAPE: 47.59607327534602\n",
      "Epoch 84, Loss: 0.346512039954989, Train MAPE: 47.568807611098656, Test MAPE: 47.488165209843565\n",
      "Epoch 85, Loss: 0.34401429015784885, Train MAPE: 47.057853115521944, Test MAPE: 46.993160922710715\n",
      "Epoch 86, Loss: 0.34158312323001716, Train MAPE: 47.351007931049054, Test MAPE: 47.308064695504996\n",
      "Epoch 87, Loss: 0.33910854406869756, Train MAPE: 46.37670988486364, Test MAPE: 46.352785308544455\n",
      "Epoch 88, Loss: 0.3367538101971149, Train MAPE: 46.649761260472815, Test MAPE: 46.64669134433453\n",
      "Epoch 89, Loss: 0.3344525106752721, Train MAPE: 46.483903492414036, Test MAPE: 46.495995022700384\n",
      "Epoch 90, Loss: 0.3321702976030512, Train MAPE: 45.94195898587887, Test MAPE: 45.96669280712421\n",
      "Epoch 91, Loss: 0.3299364509096799, Train MAPE: 46.323728719124425, Test MAPE: 46.368041801452634\n",
      "Epoch 92, Loss: 0.32765316714604314, Train MAPE: 45.08883259846614, Test MAPE: 45.137362223405106\n",
      "Epoch 93, Loss: 0.32548598611297513, Train MAPE: 45.12790453800788, Test MAPE: 45.191378270662746\n",
      "Epoch 94, Loss: 0.32348883698622766, Train MAPE: 45.45757588973412, Test MAPE: 45.54218513782208\n",
      "Epoch 95, Loss: 0.3213573541778785, Train MAPE: 45.361052043621356, Test MAPE: 45.462972193497876\n",
      "Epoch 96, Loss: 0.31919028389339266, Train MAPE: 45.10841472882491, Test MAPE: 45.22079608623798\n",
      "Epoch 97, Loss: 0.31722631241338184, Train MAPE: 44.917646490610565, Test MAPE: 45.04622386785654\n",
      "Epoch 98, Loss: 0.3151839680324953, Train MAPE: 44.21907017964583, Test MAPE: 44.347350590045636\n",
      "Epoch 99, Loss: 0.3132612338540359, Train MAPE: 44.78972442516914, Test MAPE: 44.94314222335815\n",
      "Epoch 100, Loss: 0.31126602878794074, Train MAPE: 43.910872468581566, Test MAPE: 44.07330647248488\n",
      "Epoch 101, Loss: 0.3094020097158276, Train MAPE: 44.99502874154311, Test MAPE: 45.18065564082219\n",
      "Epoch 102, Loss: 0.30750726720031635, Train MAPE: 43.77431879226978, Test MAPE: 43.962236741872935\n",
      "Epoch 103, Loss: 0.30564840062019916, Train MAPE: 43.77214734921088, Test MAPE: 43.97406162115244\n",
      "Epoch 104, Loss: 0.30383735955692825, Train MAPE: 43.642173884465144, Test MAPE: 43.85074431346013\n",
      "Epoch 105, Loss: 0.3019892645426668, Train MAPE: 43.97721010721647, Test MAPE: 44.206769503079926\n",
      "Epoch 106, Loss: 0.30078235290443095, Train MAPE: 43.55451390743256, Test MAPE: 43.79222786243145\n",
      "Epoch 107, Loss: 0.29852613373659553, Train MAPE: 43.1655529994231, Test MAPE: 43.41252191983737\n",
      "Epoch 108, Loss: 0.29676534505202795, Train MAPE: 43.1384359066303, Test MAPE: 43.388435202378496\n",
      "Epoch 109, Loss: 0.2950556920805516, Train MAPE: 43.520786041479845, Test MAPE: 43.79341242129986\n",
      "Epoch 110, Loss: 0.29341744157270744, Train MAPE: 43.15676679611206, Test MAPE: 43.43472872513991\n",
      "Epoch 111, Loss: 0.29174560158012003, Train MAPE: 42.973519178537224, Test MAPE: 43.26878410486074\n",
      "Epoch 112, Loss: 0.2900661738195385, Train MAPE: 42.759570327171915, Test MAPE: 43.061227461007924\n",
      "Epoch 113, Loss: 0.2884750448883726, Train MAPE: 42.88523861261515, Test MAPE: 43.19587940802941\n",
      "Epoch 114, Loss: 0.28688793846835886, Train MAPE: 42.426248334921326, Test MAPE: 42.74882688522339\n",
      "Epoch 115, Loss: 0.285388236765105, Train MAPE: 42.47635240004613, Test MAPE: 42.79765854615432\n",
      "Epoch 116, Loss: 0.283839720999822, Train MAPE: 42.45486367665804, Test MAPE: 42.800076961517334\n",
      "Epoch 117, Loss: 0.282364203970736, Train MAPE: 42.21507725899036, Test MAPE: 42.56969949282133\n",
      "Epoch 118, Loss: 0.28083496515352563, Train MAPE: 42.35402896220867, Test MAPE: 42.72224052135761\n",
      "Epoch 119, Loss: 0.2793071973818139, Train MAPE: 41.897631969818704, Test MAPE: 42.26587897080641\n",
      "Epoch 120, Loss: 0.27790220368725177, Train MAPE: 41.952638756311856, Test MAPE: 42.338630889012265\n",
      "Epoch 121, Loss: 0.27648585439802936, Train MAPE: 41.56928121676812, Test MAPE: 41.96444489405705\n",
      "Epoch 122, Loss: 0.27499095619871067, Train MAPE: 41.92719901525057, Test MAPE: 42.33454084396362\n",
      "Epoch 123, Loss: 0.273707270475391, Train MAPE: 41.24024431155278, Test MAPE: 41.65366928394024\n",
      "Epoch 124, Loss: 0.27234549305688305, Train MAPE: 41.207186341285706, Test MAPE: 41.63135420725896\n",
      "Epoch 125, Loss: 0.27090229929711385, Train MAPE: 41.016272128545324, Test MAPE: 41.449544018965504\n",
      "Epoch 126, Loss: 0.2698591007707784, Train MAPE: 41.097572024968954, Test MAPE: 41.542212339547966\n",
      "Epoch 127, Loss: 0.26821308029958835, Train MAPE: 41.35956313793476, Test MAPE: 41.82136314098651\n",
      "Epoch 128, Loss: 0.2668927207338409, Train MAPE: 41.12743587493897, Test MAPE: 41.596945784642145\n",
      "Epoch 129, Loss: 0.26564164332544, Train MAPE: 40.82527359265548, Test MAPE: 41.30058135986328\n",
      "Epoch 130, Loss: 0.2644469538166259, Train MAPE: 40.33847483671629, Test MAPE: 40.8223719890301\n",
      "Epoch 131, Loss: 0.26308522776413995, Train MAPE: 40.43367005311526, Test MAPE: 40.93135166168213\n",
      "Epoch 132, Loss: 0.26180495016921596, Train MAPE: 40.924046256909, Test MAPE: 41.42603551424467\n",
      "Epoch 133, Loss: 0.2605135925149975, Train MAPE: 39.683816880446216, Test MAPE: 40.19475650053758\n",
      "Epoch 134, Loss: 0.2593650173789893, Train MAPE: 40.11206663571871, Test MAPE: 40.639273298703706\n",
      "Epoch 135, Loss: 0.2581611859325606, Train MAPE: 40.194973170757294, Test MAPE: 40.7315853705773\n",
      "Epoch 136, Loss: 0.25696265284115305, Train MAPE: 39.92981417362507, Test MAPE: 40.47819626881526\n",
      "Epoch 137, Loss: 0.25578866268579775, Train MAPE: 39.56899605530959, Test MAPE: 40.12763381371131\n",
      "Epoch 138, Loss: 0.2546058271277266, Train MAPE: 39.72359794103182, Test MAPE: 40.28883843055138\n",
      "Epoch 139, Loss: 0.25343774810361747, Train MAPE: 39.3559290427428, Test MAPE: 39.93083026592548\n",
      "Epoch 140, Loss: 0.25234168854971917, Train MAPE: 39.61023002037635, Test MAPE: 40.19563610370343\n",
      "Epoch 141, Loss: 0.2512220936827362, Train MAPE: 39.348720860481265, Test MAPE: 39.942643055549034\n",
      "Epoch 142, Loss: 0.25008803859281425, Train MAPE: 39.64781303222363, Test MAPE: 40.24908491281363\n",
      "Epoch 143, Loss: 0.24903655765005028, Train MAPE: 39.328304562201865, Test MAPE: 39.94126734366784\n",
      "Epoch 144, Loss: 0.24791804337874054, Train MAPE: 39.71332744818467, Test MAPE: 40.33214205961961\n",
      "Epoch 145, Loss: 0.24689647818628985, Train MAPE: 39.1580349445343, Test MAPE: 39.786418283902684\n",
      "Epoch 146, Loss: 0.2457929371139751, Train MAPE: 38.81479089810298, Test MAPE: 39.45051294473501\n",
      "Epoch 147, Loss: 0.244786106956263, Train MAPE: 38.8705194904254, Test MAPE: 39.513222217559814\n",
      "Epoch 148, Loss: 0.2438092809313765, Train MAPE: 39.02053330678206, Test MAPE: 39.67346476041354\n",
      "Epoch 149, Loss: 0.24280211307609884, Train MAPE: 38.82883046223567, Test MAPE: 39.49009715593778\n",
      "Epoch 150, Loss: 0.24175721215657317, Train MAPE: 39.08587817962353, Test MAPE: 39.750492858886716\n",
      "Epoch 151, Loss: 0.24080839820492725, Train MAPE: 39.06169825242116, Test MAPE: 39.73139888323271\n",
      "Epoch 152, Loss: 0.2398559355105345, Train MAPE: 38.297833347320555, Test MAPE: 38.97827928249652\n",
      "Epoch 153, Loss: 0.23883871043172594, Train MAPE: 38.248182744246265, Test MAPE: 38.93268006398127\n",
      "Epoch 154, Loss: 0.23790093492716552, Train MAPE: 38.25944081819974, Test MAPE: 38.95667434839102\n",
      "Epoch 155, Loss: 0.2370483287001172, Train MAPE: 38.11571302597339, Test MAPE: 38.81968997808603\n",
      "Epoch 156, Loss: 0.2360002046630073, Train MAPE: 37.765190115341774, Test MAPE: 38.47772649618295\n",
      "Epoch 157, Loss: 0.23508636177015993, Train MAPE: 38.414255239413336, Test MAPE: 39.12413630118737\n",
      "Epoch 158, Loss: 0.23422706769062923, Train MAPE: 38.16379930697955, Test MAPE: 38.881440947606016\n",
      "Epoch 159, Loss: 0.2332515728391277, Train MAPE: 37.98817743888268, Test MAPE: 38.71573729148278\n",
      "Epoch 160, Loss: 0.2323646920088392, Train MAPE: 37.70566953695737, Test MAPE: 38.441431485689606\n",
      "Epoch 161, Loss: 0.2314890086507568, Train MAPE: 37.87387233330653, Test MAPE: 38.611166029710034\n",
      "Epoch 162, Loss: 0.2306469681672752, Train MAPE: 37.7621458200308, Test MAPE: 38.50832669184758\n",
      "Epoch 163, Loss: 0.22974103025805492, Train MAPE: 37.44062754649382, Test MAPE: 38.1919386276832\n",
      "Epoch 164, Loss: 0.2289349277754529, Train MAPE: 37.42571415350987, Test MAPE: 38.18222985634437\n",
      "Epoch 165, Loss: 0.22807747402156775, Train MAPE: 37.953767699461714, Test MAPE: 38.71209673514733\n",
      "Epoch 166, Loss: 0.22717658028436394, Train MAPE: 36.998034579937276, Test MAPE: 37.77462681256808\n",
      "Epoch 167, Loss: 0.2263801784433711, Train MAPE: 37.42805950825031, Test MAPE: 38.20510752017682\n",
      "Epoch 168, Loss: 0.22557455510474167, Train MAPE: 37.12227172209666, Test MAPE: 37.90381562159612\n",
      "Epoch 169, Loss: 0.22475573641534607, Train MAPE: 37.06851796461986, Test MAPE: 37.85922490633451\n",
      "Epoch 170, Loss: 0.22396759369338934, Train MAPE: 37.05659694671631, Test MAPE: 37.851706827603856\n",
      "Epoch 171, Loss: 0.22316628870410987, Train MAPE: 36.8495673252986, Test MAPE: 37.65061962421124\n",
      "Epoch 172, Loss: 0.22237607542998516, Train MAPE: 37.0231231551904, Test MAPE: 37.82842455643874\n",
      "Epoch 173, Loss: 0.22155457198834763, Train MAPE: 36.883014700962946, Test MAPE: 37.6973978629479\n",
      "Epoch 174, Loss: 0.22081389842698207, Train MAPE: 36.46699874401092, Test MAPE: 37.28616994711069\n",
      "Epoch 175, Loss: 0.22004504236392677, Train MAPE: 36.47376810954167, Test MAPE: 37.302054772010216\n",
      "Epoch 176, Loss: 0.21925012671675245, Train MAPE: 37.22068620553384, Test MAPE: 38.04879878117488\n",
      "Epoch 177, Loss: 0.21856453539087223, Train MAPE: 36.3879132610101, Test MAPE: 37.22741173230685\n",
      "Epoch 178, Loss: 0.21784937935523116, Train MAPE: 36.643759308411525, Test MAPE: 37.4818670126108\n",
      "Epoch 179, Loss: 0.2170575024989935, Train MAPE: 36.44126867881188, Test MAPE: 37.28671756891104\n",
      "Epoch 180, Loss: 0.2163537829559153, Train MAPE: 36.59267873947437, Test MAPE: 37.44249872060922\n",
      "Epoch 181, Loss: 0.2156420906038525, Train MAPE: 36.271059520428, Test MAPE: 37.13398027420044\n",
      "Epoch 182, Loss: 0.2149015384994877, Train MAPE: 36.3327550814702, Test MAPE: 37.19600788263174\n",
      "Epoch 183, Loss: 0.2142069487223545, Train MAPE: 36.40901959125812, Test MAPE: 37.27712601148165\n",
      "Epoch 184, Loss: 0.21381152230673112, Train MAPE: 35.99803131543673, Test MAPE: 36.87240692285391\n",
      "Epoch 185, Loss: 0.21282942241344313, Train MAPE: 35.980747876717494, Test MAPE: 36.857419043320874\n",
      "Epoch 186, Loss: 0.21214386540321775, Train MAPE: 36.207456725377305, Test MAPE: 37.089575774853046\n",
      "Epoch 187, Loss: 0.21143242535539544, Train MAPE: 35.89235960887029, Test MAPE: 36.781331619849574\n",
      "Epoch 188, Loss: 0.21078504026652528, Train MAPE: 35.79262805076746, Test MAPE: 36.684997279827414\n",
      "Epoch 189, Loss: 0.21006866548067102, Train MAPE: 36.09508971434373, Test MAPE: 36.994579373873194\n",
      "Epoch 190, Loss: 0.20946509355965715, Train MAPE: 35.76545997124452, Test MAPE: 36.669051808577315\n",
      "Epoch 191, Loss: 0.20878569988104012, Train MAPE: 35.57644825256788, Test MAPE: 36.48324357546293\n",
      "Epoch 192, Loss: 0.20814633447175415, Train MAPE: 35.64254751572242, Test MAPE: 36.553654531332164\n",
      "Epoch 193, Loss: 0.20761077330591013, Train MAPE: 35.56465712235524, Test MAPE: 36.47395433279184\n",
      "Epoch 194, Loss: 0.20684921581727955, Train MAPE: 35.26525107530447, Test MAPE: 36.18329432560847\n",
      "Epoch 195, Loss: 0.2062167109635014, Train MAPE: 35.089746561417215, Test MAPE: 36.00898124254667\n",
      "Epoch 196, Loss: 0.20562899195445844, Train MAPE: 35.267609606339384, Test MAPE: 36.20037579903236\n",
      "Epoch 197, Loss: 0.2049786118253206, Train MAPE: 35.33124655485153, Test MAPE: 36.26499953636756\n",
      "Epoch 198, Loss: 0.2043108287983789, Train MAPE: 35.21634271144867, Test MAPE: 36.15225330499502\n",
      "Epoch 199, Loss: 0.2037321176075448, Train MAPE: 35.015760914179, Test MAPE: 35.9576141504141\n",
      "Epoch 200, Loss: 0.20313970802996595, Train MAPE: 35.114971306690805, Test MAPE: 36.06121888527503\n",
      "Epoch 201, Loss: 0.2028119047673849, Train MAPE: 35.16116104125977, Test MAPE: 36.11090245613685\n",
      "Epoch 202, Loss: 0.2019047220165913, Train MAPE: 35.061891012925365, Test MAPE: 36.01560202378493\n",
      "Epoch 203, Loss: 0.20131338709750426, Train MAPE: 34.786463192793036, Test MAPE: 35.74743870955247\n",
      "Epoch 204, Loss: 0.20077212321428725, Train MAPE: 34.652055129638086, Test MAPE: 35.61353602042565\n",
      "Epoch 205, Loss: 0.2001623394469229, Train MAPE: 35.05120250628545, Test MAPE: 36.02137538469755\n",
      "Epoch 206, Loss: 0.1995710119855805, Train MAPE: 34.56661333120786, Test MAPE: 35.538871823824365\n",
      "Epoch 207, Loss: 0.19899788541862598, Train MAPE: 34.78583986575787, Test MAPE: 35.75602903366089\n",
      "Epoch 208, Loss: 0.19844512667936776, Train MAPE: 34.71322670991604, Test MAPE: 35.68965472441453\n",
      "Epoch 209, Loss: 0.19782780687539622, Train MAPE: 34.943375035432666, Test MAPE: 35.924434441786545\n",
      "Epoch 210, Loss: 0.1973264558527332, Train MAPE: 34.686478888988496, Test MAPE: 35.67597128061148\n",
      "Epoch 211, Loss: 0.19675038946577564, Train MAPE: 34.34088346316264, Test MAPE: 35.33212880354661\n",
      "Epoch 212, Loss: 0.19621460037305952, Train MAPE: 34.63420178431731, Test MAPE: 35.63149498425997\n",
      "Epoch 213, Loss: 0.19566532625601843, Train MAPE: 34.895670452484715, Test MAPE: 35.89957871070275\n",
      "Epoch 214, Loss: 0.1951479852038364, Train MAPE: 34.31820041858233, Test MAPE: 35.32060006215022\n",
      "Epoch 215, Loss: 0.1946189213096379, Train MAPE: 34.258295680009404, Test MAPE: 35.26267717068012\n",
      "Epoch 216, Loss: 0.19409042965047635, Train MAPE: 34.56604754741375, Test MAPE: 35.57859350351187\n",
      "Epoch 217, Loss: 0.1935898177540646, Train MAPE: 34.1674782046905, Test MAPE: 35.179108120844916\n",
      "Epoch 218, Loss: 0.19311585823575464, Train MAPE: 34.32376915216446, Test MAPE: 35.33836196752695\n",
      "Epoch 219, Loss: 0.1925834476016462, Train MAPE: 33.89720089527277, Test MAPE: 34.91265139212975\n",
      "Epoch 220, Loss: 0.192114496564206, Train MAPE: 34.08592086938711, Test MAPE: 35.10766955155593\n",
      "Epoch 221, Loss: 0.19154587481529095, Train MAPE: 33.978306588759786, Test MAPE: 35.00629309874314\n",
      "Epoch 222, Loss: 0.19105207055556372, Train MAPE: 34.09832447125361, Test MAPE: 35.1248132118812\n",
      "Epoch 223, Loss: 0.19056414525526075, Train MAPE: 33.972269350748796, Test MAPE: 35.00283310229962\n",
      "Epoch 224, Loss: 0.18999626841037892, Train MAPE: 33.761202285839964, Test MAPE: 34.793257368527925\n",
      "Epoch 225, Loss: 0.18955961850543435, Train MAPE: 34.247503489714404, Test MAPE: 35.29319391984206\n",
      "Epoch 226, Loss: 0.18913842019481727, Train MAPE: 33.84768214592567, Test MAPE: 34.89165694163396\n",
      "Epoch 227, Loss: 0.18870003890926734, Train MAPE: 33.88937490353217, Test MAPE: 34.93793729635385\n",
      "Epoch 228, Loss: 0.18815560987434135, Train MAPE: 33.57051344376344, Test MAPE: 34.61415572533241\n",
      "Epoch 229, Loss: 0.1876415435439692, Train MAPE: 33.864178913373216, Test MAPE: 34.91935118161715\n",
      "Epoch 230, Loss: 0.18715122521031075, Train MAPE: 33.5619880758799, Test MAPE: 34.6167186663701\n",
      "Epoch 231, Loss: 0.18671361540014353, Train MAPE: 34.036582436011386, Test MAPE: 35.102599613483136\n",
      "Epoch 232, Loss: 0.18631810903047713, Train MAPE: 33.515834953234744, Test MAPE: 34.57619077975934\n",
      "Epoch 233, Loss: 0.18583998332301585, Train MAPE: 33.37884280223113, Test MAPE: 34.43906842011672\n",
      "Epoch 234, Loss: 0.18535868948540435, Train MAPE: 33.56850011165326, Test MAPE: 34.63734653179462\n",
      "Epoch 235, Loss: 0.1849024268332869, Train MAPE: 33.56854908191241, Test MAPE: 34.63874230018029\n",
      "Epoch 236, Loss: 0.1844587749783666, Train MAPE: 33.332015150326946, Test MAPE: 34.40026305272029\n",
      "Epoch 237, Loss: 0.1840387245389418, Train MAPE: 33.537770059475534, Test MAPE: 34.61522330504197\n",
      "Epoch 238, Loss: 0.18362741943520422, Train MAPE: 33.2349416751128, Test MAPE: 34.31092999531673\n",
      "Epoch 239, Loss: 0.18319458276964723, Train MAPE: 33.45745939566539, Test MAPE: 34.54154146634615\n",
      "Epoch 240, Loss: 0.18270280744808798, Train MAPE: 33.121786363308246, Test MAPE: 34.20122821514423\n",
      "Epoch 241, Loss: 0.1822622197536895, Train MAPE: 32.919825956454645, Test MAPE: 34.003352898817795\n",
      "Epoch 242, Loss: 0.18185068521505365, Train MAPE: 33.194302830329306, Test MAPE: 34.28766265282264\n",
      "Epoch 243, Loss: 0.18146992426795455, Train MAPE: 33.11562359333038, Test MAPE: 34.20898444102361\n",
      "Epoch 244, Loss: 0.18102258214225564, Train MAPE: 33.21075244408387, Test MAPE: 34.30910290938157\n",
      "Epoch 245, Loss: 0.18058634457500794, Train MAPE: 33.07020952976667, Test MAPE: 34.1711886552664\n",
      "Epoch 246, Loss: 0.18016004211340958, Train MAPE: 32.86520819664001, Test MAPE: 33.96441894677969\n",
      "Epoch 247, Loss: 0.17974619538379977, Train MAPE: 32.621167215017174, Test MAPE: 33.72005409094004\n",
      "Epoch 248, Loss: 0.17942204661309147, Train MAPE: 32.71223903802725, Test MAPE: 33.81303102053129\n",
      "Epoch 249, Loss: 0.17892989549750032, Train MAPE: 32.669229258023776, Test MAPE: 33.770865755814775\n",
      "Epoch 250, Loss: 0.17850869515767465, Train MAPE: 33.143409625383526, Test MAPE: 34.25753493675819\n",
      "Epoch 251, Loss: 0.17813670923575187, Train MAPE: 33.10157189094103, Test MAPE: 34.22532234191895\n",
      "Epoch 252, Loss: 0.17778234243966065, Train MAPE: 33.15403281175173, Test MAPE: 34.28452350909893\n",
      "Epoch 253, Loss: 0.1773723214255789, Train MAPE: 32.692541085756744, Test MAPE: 33.81229973572951\n",
      "Epoch 254, Loss: 0.1769492172743552, Train MAPE: 32.88534409999848, Test MAPE: 34.016281788165756\n",
      "Epoch 255, Loss: 0.17659355633032436, Train MAPE: 32.69698723462912, Test MAPE: 33.82444859284621\n",
      "Epoch 256, Loss: 0.17615748130035802, Train MAPE: 32.9282202738982, Test MAPE: 34.06829580160288\n",
      "Epoch 257, Loss: 0.17580910870948663, Train MAPE: 32.604060160196745, Test MAPE: 33.734427129305324\n",
      "Epoch 258, Loss: 0.17539983989599234, Train MAPE: 32.84522777520693, Test MAPE: 33.98802234209501\n",
      "Epoch 259, Loss: 0.17503817323774387, Train MAPE: 32.685204990093524, Test MAPE: 33.8271782875061\n",
      "Epoch 260, Loss: 0.17467104809609457, Train MAPE: 32.255131633465105, Test MAPE: 33.387374070974495\n",
      "Epoch 261, Loss: 0.1742613248097209, Train MAPE: 32.70948764819365, Test MAPE: 33.85215362402109\n",
      "Epoch 262, Loss: 0.17391163305952573, Train MAPE: 32.247347202667825, Test MAPE: 33.387639141082765\n",
      "Epoch 263, Loss: 0.17352400993247732, Train MAPE: 32.478497664745035, Test MAPE: 33.63016030971821\n",
      "Epoch 264, Loss: 0.17316096388340857, Train MAPE: 32.15737855251019, Test MAPE: 33.30293436784011\n",
      "Epoch 265, Loss: 0.1727868526493414, Train MAPE: 32.32694132236334, Test MAPE: 33.47786187392015\n",
      "Epoch 266, Loss: 0.17248365846820748, Train MAPE: 32.22808284025926, Test MAPE: 33.38217325944167\n",
      "Epoch 267, Loss: 0.17208959059073375, Train MAPE: 32.39079484939575, Test MAPE: 33.54735838816716\n",
      "Epoch 268, Loss: 0.17172963216256062, Train MAPE: 32.471555460416354, Test MAPE: 33.638756421896126\n",
      "Epoch 269, Loss: 0.1713661364440878, Train MAPE: 32.120543834796315, Test MAPE: 33.283584051865795\n",
      "Epoch 270, Loss: 0.17101768131475323, Train MAPE: 32.15715203835414, Test MAPE: 33.32488331427941\n",
      "Epoch 271, Loss: 0.17069592867046596, Train MAPE: 32.23111430314871, Test MAPE: 33.40017915872427\n",
      "Epoch 272, Loss: 0.17034690590360416, Train MAPE: 32.06904236628459, Test MAPE: 33.2401726942796\n",
      "Epoch 273, Loss: 0.16996320472098886, Train MAPE: 32.059052545290726, Test MAPE: 33.225704647944525\n",
      "Epoch 274, Loss: 0.16962324529528044, Train MAPE: 32.1538064956665, Test MAPE: 33.333415251511795\n",
      "Epoch 275, Loss: 0.16923427823035475, Train MAPE: 31.580434253582588, Test MAPE: 32.749270446483905\n",
      "Epoch 276, Loss: 0.1689575667385585, Train MAPE: 31.689311809723193, Test MAPE: 32.866604386843164\n",
      "Epoch 277, Loss: 0.16862567275977478, Train MAPE: 31.996632155088278, Test MAPE: 33.18439534994272\n",
      "Epoch 278, Loss: 0.16826366763024664, Train MAPE: 31.992404232575343, Test MAPE: 33.18153332196749\n",
      "Epoch 279, Loss: 0.16793693993956996, Train MAPE: 31.593606450007513, Test MAPE: 32.780604934692384\n",
      "Epoch 280, Loss: 0.1676225116619697, Train MAPE: 31.852730313631206, Test MAPE: 33.04637002211351\n",
      "Epoch 281, Loss: 0.16727617125456723, Train MAPE: 31.658661120671493, Test MAPE: 32.85410981545081\n",
      "Epoch 282, Loss: 0.16695686262459136, Train MAPE: 31.85678070141719, Test MAPE: 33.05906677246094\n",
      "Epoch 283, Loss: 0.166613209206396, Train MAPE: 31.599553559376645, Test MAPE: 32.797967213850754\n",
      "Epoch 284, Loss: 0.16631867472046558, Train MAPE: 31.614942943132842, Test MAPE: 32.81468023153452\n",
      "Epoch 285, Loss: 0.1659869295401642, Train MAPE: 31.487879958519567, Test MAPE: 32.689381526066704\n",
      "Epoch 286, Loss: 0.16567637459195864, Train MAPE: 31.540816787573007, Test MAPE: 32.744821783212515\n",
      "Epoch 287, Loss: 0.16530876202293887, Train MAPE: 31.57754806005038, Test MAPE: 32.790102848639854\n",
      "Epoch 288, Loss: 0.16506366096257877, Train MAPE: 31.410158521395463, Test MAPE: 32.621421403151295\n",
      "Epoch 289, Loss: 0.16472955748367196, Train MAPE: 31.443857364471143, Test MAPE: 32.65262996233427\n",
      "Epoch 290, Loss: 0.164408066123724, Train MAPE: 31.457855601494128, Test MAPE: 32.67876416719877\n",
      "Epoch 291, Loss: 0.16405883450419284, Train MAPE: 31.21933382657858, Test MAPE: 32.43276654023391\n",
      "Epoch 292, Loss: 0.16374586752902429, Train MAPE: 31.41821860716893, Test MAPE: 32.6440155176016\n",
      "Epoch 293, Loss: 0.1634450508012938, Train MAPE: 31.616068795094122, Test MAPE: 32.850239937122055\n",
      "Epoch 294, Loss: 0.1631755472196696, Train MAPE: 31.245440906744737, Test MAPE: 32.47487590496357\n",
      "Epoch 295, Loss: 0.1628450402004931, Train MAPE: 31.36568448818647, Test MAPE: 32.59962946084829\n",
      "Epoch 296, Loss: 0.1625582624029798, Train MAPE: 31.334463794414813, Test MAPE: 32.569658162043645\n",
      "Epoch 297, Loss: 0.1622545207850635, Train MAPE: 31.186845197127415, Test MAPE: 32.424954194288986\n",
      "Epoch 298, Loss: 0.16195556685829965, Train MAPE: 31.301009735694297, Test MAPE: 32.54212830616878\n",
      "Epoch 299, Loss: 0.16164717404530027, Train MAPE: 31.20540320598162, Test MAPE: 32.44924484399649\n",
      "Epoch 300, Loss: 0.1613573079642195, Train MAPE: 31.044724667072295, Test MAPE: 32.28995579206026\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>test_mape</td><td>â–ˆâ–‡â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>time_test</td><td>â–‚â–†â–ƒâ–„â–‚â–‚â–ƒâ–ƒâ–„â–ƒâ–â–‚â–‚â–‚â–‚â–â–‚â–â–ƒâ–…â–‚â–„â–‚â–‚â–…â–‚â–ƒâ–‚â–â–â–‚â–â–â–â–‚â–‚â–â–â–ˆâ–‚</td></tr><tr><td>time_train</td><td>â–ƒâ–â–„â–ƒâ–…â–ƒâ–‚â–ƒâ–…â–…â–„â–„â–‚â–‡â–…â–‡â–ƒâ–â–ƒâ–„â–…â–†â–„â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–â–†â–ƒâ–â–‚â–ƒâ–ˆâ–‚â–‚</td></tr><tr><td>train_loss</td><td>â–ˆâ–‡â–†â–†â–†â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train_mape</td><td>â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>0.31469</td></tr><tr><td>test_mape</td><td>32.28996</td></tr><tr><td>time_test</td><td>1.6412</td></tr><tr><td>time_train</td><td>2.69727</td></tr><tr><td>train_loss</td><td>0.16136</td></tr><tr><td>train_mape</td><td>31.04472</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">MLP-bs16-lr1e-05</strong> at: <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/zlpn57jh' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/zlpn57jh</a><br/> View project at: <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20241218_093356-zlpn57jh\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src import torchModelRun\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "inputs_nums = len(df.columns) - 1\n",
    "\n",
    "class FullyConnectedModel(nn.Module):\n",
    "    import torch.nn.functional as F\n",
    "    def __init__(self):\n",
    "        super(FullyConnectedModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs_nums, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 32)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "models = torchModelRun.run(FullyConnectedModel, df, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-18T08:56:17.359328600Z",
     "start_time": "2024-12-18T08:33:49.732393500Z"
    }
   },
   "id": "1ec75e098d8fb34e",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "largest_num = 5e+7\n",
    "transformer = models[0]['transform']\n",
    "model = models[0]['model']\n",
    "model.eval()\n",
    "model = model.to('cpu')\n",
    "\n",
    "df_exclude = df.drop(columns=['price_cleaned'])\n",
    "df_exclude_np = df_exclude.values\n",
    "input = transformer(torch.tensor(df_exclude_np, dtype=torch.float32))\n",
    "output = model(input)\n",
    "#output_cliped = torch.clamp(output, 6.0, 18.0)\n",
    "output = torch.exp(output)\n",
    "\n",
    "plt.scatter(df['price_cleaned'], output.detach().numpy(), s=10, alpha=1)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.ylim([0, largest_num])\n",
    "plt.xlim([0, largest_num])\n",
    "plt.plot([0, largest_num], [0, largest_num], color='red')\n",
    "\n",
    "# Set the axes to display whole numbers\n",
    "plt.gca().xaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# rotate the x-axis labels\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-18T08:56:42.828261300Z",
     "start_time": "2024-12-18T08:56:42.171336300Z"
    }
   },
   "id": "85a4ca400ab6d17e",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullyConnectedModel(\n",
      "  (fc1): Linear(in_features=77, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FullyConnectedModel' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(models[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Model Summary with counts of parameters\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mmodels\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msummary\u001B[49m())\n",
      "File \u001B[1;32mC:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1929\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[0;32m   1930\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[1;32m-> 1931\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[0;32m   1932\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1933\u001B[0m )\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'FullyConnectedModel' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "print(models[0]['model'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T12:53:11.597652300Z",
     "start_time": "2024-11-14T12:53:11.562113300Z"
    }
   },
   "id": "9913115d1f91b6d7",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min outputs: 44824.0781, max outputs: 48190656.0000\n",
      "ss_res: 2525576731257844269056.0000, ss_tot: 61300058171113472.0000\n",
      "R^2-Wert: -41199.2344\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "trues = torch.tensor(df['price_cleaned'].values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "print(f'min outputs: {torch.min(output).item():.4f}, max outputs: {torch.max(output).item():.4f}')\n",
    "# Berechnung des R^2-Werts\n",
    "ss_res = torch.sum((trues - output) ** 2)\n",
    "ss_tot = torch.sum((trues - torch.mean(trues)) ** 2)\n",
    "\n",
    "print(f\"ss_res: {ss_res.item():.4f}, ss_tot: {ss_tot.item():.4f}\")\n",
    "\n",
    "r2_score = 1 - (ss_res / ss_tot)  # R^2-Formel\n",
    "print(f\"R^2-Wert: {r2_score.item():.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T12:49:18.800782400Z",
     "start_time": "2024-11-14T12:49:17.423119800Z"
    }
   },
   "id": "bf02cfe8b9f454a9",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 366, max value: 65659968\n"
     ]
    },
    {
     "data": {
      "text/plain": "     Floor  detail_responsive#surface_usable  Floor_space_merged  \\\n366    0.0                             800.0                 0.0   \n\n     gde_foreigners_percentage  gde_population  gde_social_help_quota  \\\n366                  28.144885          2043.0               0.689995   \n\n     gde_tax  price_cleaned  Space extracted  Plot_area_unified  ...  \\\n366     5.67       795000.0            200.0              980.0  ...   \n\n     type_unified_penthouse  type_unified_rustico  \\\n366                     0.0                   0.0   \n\n     type_unified_secondary-suite  type_unified_semi-detached-house  \\\n366                           0.0                               0.0   \n\n     type_unified_single-room  type_unified_stepped-apartment  \\\n366                       0.0                             0.0   \n\n     type_unified_stepped-house  type_unified_studio  \\\n366                         0.0                  0.0   \n\n     type_unified_terrace-house  type_unified_villa  \n366                         0.0                 0.0  \n\n[1 rows x 38 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Floor</th>\n      <th>detail_responsive#surface_usable</th>\n      <th>Floor_space_merged</th>\n      <th>gde_foreigners_percentage</th>\n      <th>gde_population</th>\n      <th>gde_social_help_quota</th>\n      <th>gde_tax</th>\n      <th>price_cleaned</th>\n      <th>Space extracted</th>\n      <th>Plot_area_unified</th>\n      <th>...</th>\n      <th>type_unified_penthouse</th>\n      <th>type_unified_rustico</th>\n      <th>type_unified_secondary-suite</th>\n      <th>type_unified_semi-detached-house</th>\n      <th>type_unified_single-room</th>\n      <th>type_unified_stepped-apartment</th>\n      <th>type_unified_stepped-house</th>\n      <th>type_unified_studio</th>\n      <th>type_unified_terrace-house</th>\n      <th>type_unified_villa</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>366</th>\n      <td>0.0</td>\n      <td>800.0</td>\n      <td>0.0</td>\n      <td>28.144885</td>\n      <td>2043.0</td>\n      <td>0.689995</td>\n      <td>5.67</td>\n      <td>795000.0</td>\n      <td>200.0</td>\n      <td>980.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows Ã— 38 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get index of max prediction to get original data\n",
    "index = torch.argmax(output)\n",
    "\n",
    "print(f'index: {index}, max value: {output[index].item():.0f}')\n",
    "df.iloc[[int(index)]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T16:04:58.404884100Z",
     "start_time": "2024-11-13T16:04:58.375984200Z"
    }
   },
   "id": "2775014563c65865",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 0.00000000e+00 2.42000000e+02 9.25566343e+00\n",
      " 1.54500000e+03 2.23425863e+00 5.89000000e+00 1.56000000e+02\n",
      " 2.22000000e+02 5.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 8.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      " 0.00000000e+00]\n",
      "output: tensor([[13.5669]], grad_fn=<AddmmBackward0>)\n",
      "mean: tensor([2.1500e+00, 8.8294e-01, 2.4629e+01, 2.4678e+01, 1.3220e+04, 2.7662e+00,\n",
      "        6.2422e+00, 1.5121e+02, 4.6831e+02, 4.8767e+00, 2.1362e+02, 6.3205e+02,\n",
      "        1.0192e+01, 4.8822e-02, 1.1532e-01, 4.1612e-01, 3.1292e-02, 5.7949e-04,\n",
      "        2.8974e-04, 3.1485e-02, 2.4184e-01, 1.9316e-04, 3.2500e-02, 7.2919e-03,\n",
      "        4.8629e-01, 1.1590e-03, 1.8833e-03, 2.9361e-02, 3.0906e-03, 4.8291e-05,\n",
      "        3.8005e-02, 9.6581e-05, 9.8030e-03, 2.7526e-03, 4.3944e-03, 2.3228e-02,\n",
      "        5.4423e-02]), std: tensor([3.7403e+01, 1.4964e+01, 9.1374e+01, 1.0910e+01, 2.9369e+04, 2.2509e+00,\n",
      "        2.0956e+00, 1.3060e+02, 3.9246e+03, 1.9806e+00, 6.2011e+02, 9.2582e+02,\n",
      "        7.1058e+00, 2.1550e-01, 3.1941e-01, 4.9293e-01, 1.7411e-01, 2.4066e-02,\n",
      "        1.7020e-02, 1.7463e-01, 4.2821e-01, 1.3897e-02, 1.7733e-01, 8.5083e-02,\n",
      "        4.9982e-01, 3.4025e-02, 4.3358e-02, 1.6882e-01, 5.5508e-02, 6.9491e-03,\n",
      "        1.9121e-01, 9.8273e-03, 9.8526e-02, 5.2394e-02, 6.6146e-02, 1.5063e-01,\n",
      "        2.2686e-01])\n",
      "output exp: 779902\n"
     ]
    }
   ],
   "source": [
    "index_nr = 1\n",
    "df_exclude = df.drop(columns=['price_cleaned'])\n",
    "datapoint = df_exclude.iloc[index_nr].values\n",
    "print(datapoint)\n",
    "datapoint = torch.tensor(datapoint, dtype=torch.float32).unsqueeze(0)\n",
    "transformer = models[0]['transform']\n",
    "\n",
    "datapoint = transformer(datapoint)\n",
    "\n",
    "\n",
    "model = models[0]['model']\n",
    "model.eval()\n",
    "model = model.to('cpu')\n",
    "output = model(datapoint)\n",
    "print(f'output: {output}')\n",
    "transformer = models[0]['transform']\n",
    "print(f'mean: {transformer.mean}, std: {transformer.std}')\n",
    "#output = transformer.inverse(output)\n",
    "#print(f'output inverst: {output}')\n",
    "output = torch.exp(output)\n",
    "print(f'output exp: {output[0][0]:.0f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T16:04:59.333085200Z",
     "start_time": "2024-11-13T16:04:59.319061Z"
    }
   },
   "id": "93d832963b9862d6",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "np.float64(1420000.0)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['price_cleaned'].iloc[index_nr]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T16:05:00.309177500Z",
     "start_time": "2024-11-13T16:05:00.300843300Z"
    }
   },
   "id": "b1d875d245b45d93",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      Floor  detail_responsive#surface_usable  Floor_space_merged  \\\n0  4.000000                        121.247562          136.313355   \n1  2.861332                        224.500270          242.000000   \n2  2.000000                        129.453708          120.182857   \n3 -0.126916                        232.231722          257.000000   \n4  0.000000                        190.906461          177.781407   \n\n   gde_foreigners_percentage  gde_population  gde_social_help_quota  gde_tax  \\\n0                   9.255663          1545.0               2.234259     5.89   \n1                   9.255663          1545.0               2.234259     5.89   \n2                  21.358623         21036.0               3.549010     6.05   \n3                   9.255663          1545.0               2.234259     5.89   \n4                  15.901990          6081.0               1.708126     6.30   \n\n   price_cleaned  Space extracted  Plot_area_unified  ...  \\\n0      1150000.0            100.0        1340.042762  ...   \n1      1420000.0            156.0         222.000000  ...   \n2       720000.0             93.0         462.657966  ...   \n3      1430000.0            154.0         370.000000  ...   \n4       995000.0            142.0          32.776514  ...   \n\n   type_unified_penthouse  type_unified_rustico  type_unified_secondary-suite  \\\n0                     1.0                   0.0                           0.0   \n1                     0.0                   0.0                           0.0   \n2                     1.0                   0.0                           0.0   \n3                     0.0                   0.0                           0.0   \n4                     0.0                   0.0                           0.0   \n\n   type_unified_semi-detached-house  type_unified_single-room  \\\n0                               0.0                       0.0   \n1                               0.0                       0.0   \n2                               0.0                       0.0   \n3                               0.0                       0.0   \n4                               0.0                       0.0   \n\n   type_unified_stepped-apartment  type_unified_stepped-house  \\\n0                             0.0                         0.0   \n1                             0.0                         0.0   \n2                             0.0                         0.0   \n3                             0.0                         0.0   \n4                             0.0                         0.0   \n\n   type_unified_studio  type_unified_terrace-house  type_unified_villa  \n0                  0.0                         0.0                 0.0  \n1                  0.0                         1.0                 0.0  \n2                  0.0                         0.0                 0.0  \n3                  0.0                         0.0                 0.0  \n4                  0.0                         0.0                 0.0  \n\n[5 rows x 36 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Floor</th>\n      <th>detail_responsive#surface_usable</th>\n      <th>Floor_space_merged</th>\n      <th>gde_foreigners_percentage</th>\n      <th>gde_population</th>\n      <th>gde_social_help_quota</th>\n      <th>gde_tax</th>\n      <th>price_cleaned</th>\n      <th>Space extracted</th>\n      <th>Plot_area_unified</th>\n      <th>...</th>\n      <th>type_unified_penthouse</th>\n      <th>type_unified_rustico</th>\n      <th>type_unified_secondary-suite</th>\n      <th>type_unified_semi-detached-house</th>\n      <th>type_unified_single-room</th>\n      <th>type_unified_stepped-apartment</th>\n      <th>type_unified_stepped-house</th>\n      <th>type_unified_studio</th>\n      <th>type_unified_terrace-house</th>\n      <th>type_unified_villa</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.000000</td>\n      <td>121.247562</td>\n      <td>136.313355</td>\n      <td>9.255663</td>\n      <td>1545.0</td>\n      <td>2.234259</td>\n      <td>5.89</td>\n      <td>1150000.0</td>\n      <td>100.0</td>\n      <td>1340.042762</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.861332</td>\n      <td>224.500270</td>\n      <td>242.000000</td>\n      <td>9.255663</td>\n      <td>1545.0</td>\n      <td>2.234259</td>\n      <td>5.89</td>\n      <td>1420000.0</td>\n      <td>156.0</td>\n      <td>222.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.000000</td>\n      <td>129.453708</td>\n      <td>120.182857</td>\n      <td>21.358623</td>\n      <td>21036.0</td>\n      <td>3.549010</td>\n      <td>6.05</td>\n      <td>720000.0</td>\n      <td>93.0</td>\n      <td>462.657966</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.126916</td>\n      <td>232.231722</td>\n      <td>257.000000</td>\n      <td>9.255663</td>\n      <td>1545.0</td>\n      <td>2.234259</td>\n      <td>5.89</td>\n      <td>1430000.0</td>\n      <td>154.0</td>\n      <td>370.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>190.906461</td>\n      <td>177.781407</td>\n      <td>15.901990</td>\n      <td>6081.0</td>\n      <td>1.708126</td>\n      <td>6.30</td>\n      <td>995000.0</td>\n      <td>142.0</td>\n      <td>32.776514</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 36 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T14:56:58.844430600Z",
     "start_time": "2024-11-13T14:56:58.787871500Z"
    }
   },
   "id": "c185cc492808f7ca",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_loader, val_loader, test_loader \u001B[38;5;241m=\u001B[39m torchModelRun\u001B[38;5;241m.\u001B[39mgetDataLoaders(df, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprice_cleaned\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      3\u001B[0m counter \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = torchModelRun.getDataLoaders(df, 'price_cleaned', 1)\n",
    "\n",
    "counter = 5\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    counter -= 1\n",
    "    if counter == 0:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T14:56:59.246497500Z",
     "start_time": "2024-11-13T14:56:58.808427700Z"
    }
   },
   "id": "e7803da5caab2ecd",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.std().values.reshape(1, -1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-13T14:56:59.238985Z"
    }
   },
   "id": "c0eae5eb7afd070d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-13T14:56:59.240984400Z"
    }
   },
   "id": "a77dd87943f775e7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input = 1150000.0\n",
    "log = np.log(input+1)\n",
    "min = 0.6931471824645996\n",
    "max = 17.70733070373535\n",
    "output = (log - min) / (max - min)\n",
    "\n",
    "print(f'output: {output}')\n",
    "\n",
    "output = output * (max - min) + min\n",
    "output = np.exp(output) - 1\n",
    "print(f'output: {output}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-13T14:56:59.242490100Z"
    }
   },
   "id": "170efc538d930cca",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-13T14:56:59.244495800Z"
    }
   },
   "id": "4e9378e20110a7da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embedding for Text with BERT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ecbd74d94f8dda9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertForRegression(nn.Module):\n",
    "    def __init__(self, pretrained_model_name):\n",
    "        super(BertForRegression, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output  # `[CLS]` token output\n",
    "        return self.regressor(cls_output)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe619c8cec2161b7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train BERT model\n",
    "\n",
    "model = BertForRegression('bert-base')\n",
    "\n",
    "epochs = 10\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train_loader, val_loader, test_loader = torchModelRun.getDataLoaders(df, 'price_cleaned', 1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        target = batch['target']\n",
    "        output = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4759b8f8dad1e59c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Kaggle Competition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94dde004e869b7a2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:43: DtypeWarning: Columns (3,4,5,6,11,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,45,46,47,49,50,107,110,114,115,116,119,120,121,124,125,126,128,131,132) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(filePath)\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:70: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Space extracted']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:75: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Plot_area_unified']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:81: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Availability']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "import os\n",
    "from src.dataPipeline import DataPipeline\n",
    "\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "dp = DataPipeline()\n",
    "df = dp.runPipeline(normalizeAndStandardize=False, imputer=imputer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T12:49:37.413279300Z",
     "start_time": "2024-12-13T12:48:51.229080600Z"
    }
   },
   "id": "cfa8f76149eb8d3a",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       Floor  detail_responsive#surface_usable  Floor_space_merged  \\\n0        4.0                             187.2               159.2   \n1        2.4                             191.8               242.0   \n2        2.0                             162.0               112.6   \n3        2.2                             191.8               257.0   \n4        0.0                             197.6               228.0   \n...      ...                               ...                 ...   \n20791    0.8                              67.8               100.8   \n20792    3.8                             207.2               249.2   \n20793    0.0                             233.2               318.0   \n20794    1.8                             167.8               149.8   \n20795    1.0                             361.2               122.6   \n\n       ForestDensityM  ForestDensityS  RiversAndLakesL  RiversAndLakesM  \\\n0            0.286451        0.090908         0.082170         0.001811   \n1            0.286451        0.090908         0.082170         0.001811   \n2            0.095877        0.001911         0.154274         0.188229   \n3            0.286451        0.090908         0.082170         0.001811   \n4            0.279276        0.145835         0.109586         0.141473   \n...               ...             ...              ...              ...   \n20791        0.385885        0.097278         0.000000         0.000000   \n20792        0.000000        0.000000         0.013997         0.000000   \n20793        0.000000        0.000000         0.000000         0.000000   \n20794        0.151363        0.038351         0.026686         0.021426   \n20795        0.153670        0.113892         0.055729         0.034357   \n\n       RiversAndLakesS  distanceToTrainStation  gde_area_forest_percentage  \\\n0             0.011871                3.038467                   51.449275   \n1             0.011871                3.038467                   51.449275   \n2             0.000000                0.909587                   32.197891   \n3             0.011871                3.038467                   51.449275   \n4             0.091805                1.460245                   49.705635   \n...                ...                     ...                         ...   \n20791         0.000000                1.375319                   29.028213   \n20792         0.000000                1.067011                   30.599295   \n20793         0.000000                0.230859                   30.599295   \n20794         0.000000                0.357339                   26.718547   \n20795         0.000000                0.530367                   45.357143   \n\n       ...  Swimming pool  View  Washing machine  Waste water connection  \\\n0      ...            0.0   0.0              0.0                     0.0   \n1      ...            0.0   0.0              0.0                     0.0   \n2      ...            0.0   0.0              0.0                     0.0   \n3      ...            0.0   0.0              0.0                     0.0   \n4      ...            0.0   0.0              0.0                     0.0   \n...    ...            ...   ...              ...                     ...   \n20791  ...            0.0   0.0              0.0                     0.0   \n20792  ...            0.0   1.0              1.0                     0.0   \n20793  ...            0.0   0.0              0.0                     0.0   \n20794  ...            0.0   1.0              0.0                     0.0   \n20795  ...            0.0   0.0              0.0                     0.0   \n\n       Water connection  Wheelchair access  With a summer house  covered  \\\n0                   0.0                0.0                  0.0      0.0   \n1                   0.0                0.0                  0.0      0.0   \n2                   0.0                0.0                  0.0      0.0   \n3                   0.0                0.0                  0.0      0.0   \n4                   0.0                0.0                  0.0      0.0   \n...                 ...                ...                  ...      ...   \n20791               0.0                0.0                  0.0      0.0   \n20792               0.0                1.0                  0.0      0.0   \n20793               0.0                0.0                  0.0      0.0   \n20794               0.0                1.0                  0.0      0.0   \n20795               0.0                0.0                  0.0      0.0   \n\n       Availability_Immediately  Availability_On request  \n0                           0.0                      1.0  \n1                           0.0                      1.0  \n2                           1.0                      0.0  \n3                           0.0                      1.0  \n4                           0.0                      1.0  \n...                         ...                      ...  \n20791                       0.0                      0.0  \n20792                       0.0                      0.0  \n20793                       0.0                      0.0  \n20794                       0.0                      0.0  \n20795                       0.0                      0.0  \n\n[20796 rows x 61 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Floor</th>\n      <th>detail_responsive#surface_usable</th>\n      <th>Floor_space_merged</th>\n      <th>ForestDensityM</th>\n      <th>ForestDensityS</th>\n      <th>RiversAndLakesL</th>\n      <th>RiversAndLakesM</th>\n      <th>RiversAndLakesS</th>\n      <th>distanceToTrainStation</th>\n      <th>gde_area_forest_percentage</th>\n      <th>...</th>\n      <th>Swimming pool</th>\n      <th>View</th>\n      <th>Washing machine</th>\n      <th>Waste water connection</th>\n      <th>Water connection</th>\n      <th>Wheelchair access</th>\n      <th>With a summer house</th>\n      <th>covered</th>\n      <th>Availability_Immediately</th>\n      <th>Availability_On request</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.0</td>\n      <td>187.2</td>\n      <td>159.2</td>\n      <td>0.286451</td>\n      <td>0.090908</td>\n      <td>0.082170</td>\n      <td>0.001811</td>\n      <td>0.011871</td>\n      <td>3.038467</td>\n      <td>51.449275</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.4</td>\n      <td>191.8</td>\n      <td>242.0</td>\n      <td>0.286451</td>\n      <td>0.090908</td>\n      <td>0.082170</td>\n      <td>0.001811</td>\n      <td>0.011871</td>\n      <td>3.038467</td>\n      <td>51.449275</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>162.0</td>\n      <td>112.6</td>\n      <td>0.095877</td>\n      <td>0.001911</td>\n      <td>0.154274</td>\n      <td>0.188229</td>\n      <td>0.000000</td>\n      <td>0.909587</td>\n      <td>32.197891</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.2</td>\n      <td>191.8</td>\n      <td>257.0</td>\n      <td>0.286451</td>\n      <td>0.090908</td>\n      <td>0.082170</td>\n      <td>0.001811</td>\n      <td>0.011871</td>\n      <td>3.038467</td>\n      <td>51.449275</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>197.6</td>\n      <td>228.0</td>\n      <td>0.279276</td>\n      <td>0.145835</td>\n      <td>0.109586</td>\n      <td>0.141473</td>\n      <td>0.091805</td>\n      <td>1.460245</td>\n      <td>49.705635</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20791</th>\n      <td>0.8</td>\n      <td>67.8</td>\n      <td>100.8</td>\n      <td>0.385885</td>\n      <td>0.097278</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.375319</td>\n      <td>29.028213</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20792</th>\n      <td>3.8</td>\n      <td>207.2</td>\n      <td>249.2</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.013997</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.067011</td>\n      <td>30.599295</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20793</th>\n      <td>0.0</td>\n      <td>233.2</td>\n      <td>318.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.230859</td>\n      <td>30.599295</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20794</th>\n      <td>1.8</td>\n      <td>167.8</td>\n      <td>149.8</td>\n      <td>0.151363</td>\n      <td>0.038351</td>\n      <td>0.026686</td>\n      <td>0.021426</td>\n      <td>0.000000</td>\n      <td>0.357339</td>\n      <td>26.718547</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20795</th>\n      <td>1.0</td>\n      <td>361.2</td>\n      <td>122.6</td>\n      <td>0.153670</td>\n      <td>0.113892</td>\n      <td>0.055729</td>\n      <td>0.034357</td>\n      <td>0.000000</td>\n      <td>0.530367</td>\n      <td>45.357143</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>20796 rows Ã— 61 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T12:49:37.451290500Z",
     "start_time": "2024-12-13T12:49:37.413279300Z"
    }
   },
   "id": "5485c9011d08fb0b",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input nums: 60\n"
     ]
    }
   ],
   "source": [
    "inputs_nums = len(df.columns) - 1\n",
    "print(f'input nums: {inputs_nums}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T12:49:37.453295400Z",
     "start_time": "2024-12-13T12:49:37.442583200Z"
    }
   },
   "id": "bf4020d443f327c4",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:zhmywanv) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">MLP-bs16-lr1e-05</strong> at: <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/zhmywanv' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/zhmywanv</a><br/> View project at: <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20241213_134939-zhmywanv\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:zhmywanv). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.18.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\wandb\\run-20241213_135117-owz0njn1</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/owz0njn1' target=\"_blank\">MLP-bs16-lr1e-05</a></strong> to <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/owz0njn1' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/owz0njn1</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 182.31597115443304, Train MAPE: 77839925.0576923, Test MAPE: 80949120.70769231\n",
      "Epoch 2, Loss: 166.76681319016677, Train MAPE: 34346591.37692308, Test MAPE: 35584276.384615384\n",
      "Epoch 3, Loss: 131.00556689775908, Train MAPE: 2311189.860576923, Test MAPE: 2355694.962980769\n",
      "Epoch 4, Loss: 43.89569594218181, Train MAPE: nan, Test MAPE: 4442.224892014724\n",
      "Epoch 5, Loss: 13.990102921999418, Train MAPE: nan, Test MAPE: 938.2629087594839\n",
      "Epoch 6, Loss: 7.851106465665194, Train MAPE: 603.8225309518667, Test MAPE: 594.1052240224985\n",
      "Epoch 7, Loss: 5.12880457049379, Train MAPE: 332.892286997575, Test MAPE: 328.2939253146832\n",
      "Epoch 8, Loss: 3.717655853296702, Train MAPE: 248.59473959482634, Test MAPE: 245.8888848231389\n",
      "Epoch 9, Loss: 2.8724275996478705, Train MAPE: 197.3469076670133, Test MAPE: 195.94455155592698\n",
      "Epoch 10, Loss: 2.3248596731573343, Train MAPE: 174.82429721538838, Test MAPE: 174.7405268742488\n",
      "Epoch 11, Loss: 1.9454785069880578, Train MAPE: 158.53121710190408, Test MAPE: 159.6049475156344\n",
      "Epoch 12, Loss: 1.6724828612632476, Train MAPE: 139.97240394078767, Test MAPE: 141.88293046217697\n",
      "Epoch 13, Loss: 1.4712275956829006, Train MAPE: 126.50854237263019, Test MAPE: 129.05807721064642\n",
      "Epoch 14, Loss: 1.320459328749432, Train MAPE: 121.7975908609537, Test MAPE: 125.032363818242\n",
      "Epoch 15, Loss: 1.205949128476473, Train MAPE: 112.93162812086253, Test MAPE: 116.5930681668795\n",
      "Epoch 16, Loss: 1.1158589485889443, Train MAPE: 105.30532316794762, Test MAPE: 109.25052324441764\n",
      "Epoch 17, Loss: 1.0434239947595276, Train MAPE: 100.76315776017996, Test MAPE: 104.94917978140025\n",
      "Epoch 18, Loss: 0.9843752332031727, Train MAPE: 99.35548201340896, Test MAPE: 103.91610130896935\n",
      "Epoch 19, Loss: 0.9348384560300753, Train MAPE: 93.88519255564763, Test MAPE: 98.50712834871732\n",
      "Epoch 20, Loss: 0.8927187437908007, Train MAPE: 91.90838169868175, Test MAPE: 96.74533924689659\n",
      "Epoch 21, Loss: 0.8561652502474877, Train MAPE: 89.09825936464163, Test MAPE: 94.00715041527381\n",
      "Epoch 22, Loss: 0.8239384811514845, Train MAPE: 86.06270523621485, Test MAPE: 90.98296875586877\n",
      "Epoch 23, Loss: 0.7956321190899381, Train MAPE: 87.45176809567671, Test MAPE: 92.66336182814378\n",
      "Epoch 24, Loss: 0.7704029593473444, Train MAPE: 83.96546484507047, Test MAPE: 89.13269016559308\n",
      "Epoch 25, Loss: 0.7474484893564994, Train MAPE: 81.89875668378977, Test MAPE: 87.06761126151451\n",
      "Epoch 26, Loss: 0.7263668173350967, Train MAPE: 81.35661180569575, Test MAPE: 86.62887048721313\n",
      "Epoch 27, Loss: 0.7068660847627772, Train MAPE: 79.21280787908114, Test MAPE: 84.44497394561768\n",
      "Epoch 28, Loss: 0.6888188309680957, Train MAPE: 76.91704259652357, Test MAPE: 82.06483409588154\n",
      "Epoch 29, Loss: 0.6720244277268648, Train MAPE: 76.44494000031398, Test MAPE: 81.64958017789401\n",
      "Epoch 30, Loss: 0.656445670256821, Train MAPE: 76.58954571027022, Test MAPE: 81.89265612088717\n",
      "Epoch 31, Loss: 0.6417989306438427, Train MAPE: 74.65796741338876, Test MAPE: 79.87114688433134\n",
      "Epoch 32, Loss: 0.6274255639300323, Train MAPE: 72.50915687450996, Test MAPE: 77.57988592294546\n",
      "Epoch 33, Loss: 0.6142908433595529, Train MAPE: 72.11962566375732, Test MAPE: 77.24725950681247\n",
      "Epoch 34, Loss: 0.6014776913520808, Train MAPE: 70.95021350200359, Test MAPE: 76.0288173528818\n",
      "Epoch 35, Loss: 0.5894069041053837, Train MAPE: 69.13851227210118, Test MAPE: 74.09246619297907\n",
      "Epoch 36, Loss: 0.5780040341393592, Train MAPE: 69.64650142009441, Test MAPE: 74.71239260160006\n",
      "Epoch 37, Loss: 0.56699664043979, Train MAPE: 68.31255650520325, Test MAPE: 73.28951545128456\n",
      "Epoch 38, Loss: 0.5563550065701398, Train MAPE: 68.3520539357112, Test MAPE: 73.37739193256084\n",
      "Epoch 39, Loss: 0.5463172111158761, Train MAPE: 66.376555620707, Test MAPE: 71.23469199400682\n",
      "Epoch 40, Loss: 0.5366808050813583, Train MAPE: 65.84810430636773, Test MAPE: 70.67677016625038\n",
      "Epoch 41, Loss: 0.5273267350947628, Train MAPE: 65.29286583203536, Test MAPE: 70.10641992275532\n",
      "Epoch 42, Loss: 0.5182828306363753, Train MAPE: 65.42663786961482, Test MAPE: 70.29233688207773\n",
      "Epoch 43, Loss: 0.5095465700428646, Train MAPE: 63.27105919764592, Test MAPE: 67.90478279407208\n",
      "Epoch 44, Loss: 0.5010366875535021, Train MAPE: 62.144594698685864, Test MAPE: 66.70055569135226\n",
      "Epoch 45, Loss: 0.4931614653637203, Train MAPE: 62.04641543351687, Test MAPE: 66.63341109202459\n",
      "Epoch 46, Loss: 0.4853009895278284, Train MAPE: 61.39431110528799, Test MAPE: 65.94278620206393\n",
      "Epoch 47, Loss: 0.47763370069890065, Train MAPE: 61.47390616306892, Test MAPE: 66.07459034552942\n",
      "Epoch 48, Loss: 0.47040268185620127, Train MAPE: 60.17562946172861, Test MAPE: 64.64855544016912\n",
      "Epoch 49, Loss: 0.46331219147317687, Train MAPE: 59.88325321857746, Test MAPE: 64.35332168432383\n",
      "Epoch 50, Loss: 0.45641716255161624, Train MAPE: 58.54104296610905, Test MAPE: 62.886298751831056\n",
      "Epoch 51, Loss: 0.44993710757161565, Train MAPE: 58.815171544368454, Test MAPE: 63.20371351975661\n",
      "Epoch 52, Loss: 0.4434375855570229, Train MAPE: 57.85900821502392, Test MAPE: 62.161624336242674\n",
      "Epoch 53, Loss: 0.437244844257545, Train MAPE: 57.57098857806279, Test MAPE: 61.87615214861356\n",
      "Epoch 54, Loss: 0.43111267897228783, Train MAPE: 56.71305555380308, Test MAPE: 60.93145200289213\n",
      "Epoch 55, Loss: 0.42534793183350794, Train MAPE: 56.59507263990549, Test MAPE: 60.825840832636906\n",
      "Epoch 56, Loss: 0.4195092759668254, Train MAPE: 56.1446192741394, Test MAPE: 60.364243522057166\n",
      "Epoch 57, Loss: 0.4139564994221123, Train MAPE: 55.04181642532349, Test MAPE: 59.151314331935005\n",
      "Epoch 58, Loss: 0.4084830636803347, Train MAPE: 55.463487670971794, Test MAPE: 59.65302043327918\n",
      "Epoch 59, Loss: 0.40320144350497195, Train MAPE: 54.80938673569606, Test MAPE: 58.93534937638503\n",
      "Epoch 60, Loss: 0.398007250820788, Train MAPE: 54.0405960724904, Test MAPE: 58.09615987630991\n",
      "Epoch 61, Loss: 0.3929976679241428, Train MAPE: 53.13479293493124, Test MAPE: 57.10416384476882\n",
      "Epoch 62, Loss: 0.38816887772140596, Train MAPE: 52.46791138648987, Test MAPE: 56.379203767042895\n",
      "Epoch 63, Loss: 0.38339253613152185, Train MAPE: 52.22409938848936, Test MAPE: 56.12568057867197\n",
      "Epoch 64, Loss: 0.37878420111508326, Train MAPE: 51.55488401376284, Test MAPE: 55.393701773423416\n",
      "Epoch 65, Loss: 0.3742905670633683, Train MAPE: 52.252796152921825, Test MAPE: 56.19824209946852\n",
      "Epoch 66, Loss: 0.3698488682078627, Train MAPE: 51.304939853228056, Test MAPE: 55.158678098825305\n",
      "Epoch 67, Loss: 0.36544856323072544, Train MAPE: 51.98005919273083, Test MAPE: 55.95377021202674\n",
      "Epoch 68, Loss: 0.36129969352354796, Train MAPE: 50.77416664086855, Test MAPE: 54.60658457829402\n",
      "Epoch 69, Loss: 0.3571959543471726, Train MAPE: 50.98925076081203, Test MAPE: 54.87679502780621\n",
      "Epoch 70, Loss: 0.3532397920659815, Train MAPE: 49.830407048188725, Test MAPE: 53.57760326678936\n",
      "Epoch 71, Loss: 0.34923338774018564, Train MAPE: 50.452764567962056, Test MAPE: 54.30214644211989\n",
      "Epoch 72, Loss: 0.34547591564747004, Train MAPE: 49.491551674329315, Test MAPE: 53.232656698960525\n",
      "Epoch 73, Loss: 0.3416993315045077, Train MAPE: 49.07762480882498, Test MAPE: 52.78662276634803\n",
      "Epoch 74, Loss: 0.3379794757240094, Train MAPE: 48.83665271722354, Test MAPE: 52.522584878481354\n",
      "Epoch 75, Loss: 0.33434810768812895, Train MAPE: 48.129607970897965, Test MAPE: 51.73443427452674\n",
      "Epoch 76, Loss: 0.33090117140673103, Train MAPE: 47.76352243240063, Test MAPE: 51.341766658196086\n",
      "Epoch 77, Loss: 0.32740409096631296, Train MAPE: 47.23444976439843, Test MAPE: 50.75578545056857\n",
      "Epoch 78, Loss: 0.3241001876644217, Train MAPE: 47.55765149960151, Test MAPE: 51.13810721177321\n",
      "Epoch 79, Loss: 0.32076648912177635, Train MAPE: 46.963746509185206, Test MAPE: 50.492724484663746\n",
      "Epoch 80, Loss: 0.3175058678580591, Train MAPE: 46.626065292725194, Test MAPE: 50.11371406408457\n",
      "Epoch 81, Loss: 0.31441579815716697, Train MAPE: 46.355240220289964, Test MAPE: 49.82908047896165\n",
      "Epoch 82, Loss: 0.3113642084913758, Train MAPE: 46.228685186459465, Test MAPE: 49.69987073311439\n",
      "Epoch 83, Loss: 0.30832151376928846, Train MAPE: 46.11301859525534, Test MAPE: 49.59131433046781\n",
      "Epoch 84, Loss: 0.30532552715784944, Train MAPE: 46.38351870866922, Test MAPE: 49.91146641511183\n",
      "Epoch 85, Loss: 0.3024847895336839, Train MAPE: 45.188211008218616, Test MAPE: 48.56453602864192\n",
      "Epoch 86, Loss: 0.29964811024972454, Train MAPE: 45.325228925851675, Test MAPE: 48.731951860281136\n",
      "Epoch 87, Loss: 0.2968936525894186, Train MAPE: 44.82831990352044, Test MAPE: 48.18089569531954\n",
      "Epoch 88, Loss: 0.2941189773058375, Train MAPE: 44.034484584514914, Test MAPE: 47.279623735868014\n",
      "Epoch 89, Loss: 0.2915389375474591, Train MAPE: 44.67622240690085, Test MAPE: 48.0314220061669\n",
      "Epoch 90, Loss: 0.2888930509511668, Train MAPE: 44.34233869772691, Test MAPE: 47.667918095221886\n",
      "Epoch 91, Loss: 0.28631481373539336, Train MAPE: 43.950921124678395, Test MAPE: 47.23231570904071\n",
      "Epoch 92, Loss: 0.2838935326426648, Train MAPE: 44.087246710520525, Test MAPE: 47.394130780146675\n",
      "Epoch 93, Loss: 0.28131105426627284, Train MAPE: 43.422650951605576, Test MAPE: 46.65371699699989\n",
      "Epoch 94, Loss: 0.2788933364029687, Train MAPE: 43.577137633470386, Test MAPE: 46.84614221866314\n",
      "Epoch 95, Loss: 0.27657581836415024, Train MAPE: 42.962161478629476, Test MAPE: 46.14416751127977\n",
      "Epoch 96, Loss: 0.27425388688794694, Train MAPE: 42.774451162264896, Test MAPE: 45.948169840299165\n",
      "Epoch 97, Loss: 0.27196319854030243, Train MAPE: 42.60068291058907, Test MAPE: 45.75486484674307\n",
      "Epoch 98, Loss: 0.26968247160816994, Train MAPE: 42.583147584475, Test MAPE: 45.747391062516435\n",
      "Epoch 99, Loss: 0.26747707832031525, Train MAPE: 41.99831218536084, Test MAPE: 45.09056674517118\n",
      "Epoch 100, Loss: 0.2653524878076636, Train MAPE: 41.718861889839175, Test MAPE: 44.78048422886775\n",
      "Epoch 101, Loss: 0.26350359705348425, Train MAPE: 41.746021517423486, Test MAPE: 44.82426245029156\n",
      "Epoch 102, Loss: 0.261082213318262, Train MAPE: 41.64695737545307, Test MAPE: 44.72411983196552\n",
      "Epoch 103, Loss: 0.25920473204639094, Train MAPE: 41.313199809881354, Test MAPE: 44.35740635945247\n",
      "Epoch 104, Loss: 0.2571402705632723, Train MAPE: 41.413936224350564, Test MAPE: 44.4849192399245\n",
      "Epoch 105, Loss: 0.25517835166806785, Train MAPE: 41.37505206878369, Test MAPE: 44.447984570723314\n",
      "Epoch 106, Loss: 0.2533261012644149, Train MAPE: 41.18157786406004, Test MAPE: 44.24508815178504\n",
      "Epoch 107, Loss: 0.2513644931622996, Train MAPE: 40.860141003131865, Test MAPE: 43.891902248676004\n",
      "Epoch 108, Loss: 0.24956106807176884, Train MAPE: 40.5156092441999, Test MAPE: 43.50457402742826\n",
      "Epoch 109, Loss: 0.24769594387796062, Train MAPE: 40.49755347875448, Test MAPE: 43.50407204261193\n",
      "Epoch 110, Loss: 0.24589048496877344, Train MAPE: 40.18814096084008, Test MAPE: 43.15136036506066\n",
      "Epoch 111, Loss: 0.24410791244859306, Train MAPE: 40.40644134924962, Test MAPE: 43.431483400785005\n",
      "Epoch 112, Loss: 0.2423814240269936, Train MAPE: 39.618858283299666, Test MAPE: 42.523249816894534\n",
      "Epoch 113, Loss: 0.24069325737296962, Train MAPE: 39.57783313531142, Test MAPE: 42.4914433992826\n",
      "Epoch 114, Loss: 0.23899269320715505, Train MAPE: 39.4578176681812, Test MAPE: 42.35968236189622\n",
      "Epoch 115, Loss: 0.23732548843925963, Train MAPE: 39.642306339740756, Test MAPE: 42.60381482931284\n",
      "Epoch 116, Loss: 0.23574065074252967, Train MAPE: 39.431102538108824, Test MAPE: 42.36800768191998\n",
      "Epoch 117, Loss: 0.23409165222770892, Train MAPE: 39.40745896467796, Test MAPE: 42.36497979530921\n",
      "Epoch 118, Loss: 0.23252398756762538, Train MAPE: 38.598959067234624, Test MAPE: 41.425498030735895\n",
      "Epoch 119, Loss: 0.23098675509771474, Train MAPE: 38.22817644155943, Test MAPE: 41.006312839801495\n",
      "Epoch 120, Loss: 0.2295676289197917, Train MAPE: 38.74843562749716, Test MAPE: 41.63535489302415\n",
      "Epoch 121, Loss: 0.22809368705090421, Train MAPE: 38.26763058075538, Test MAPE: 41.09108409881592\n",
      "Epoch 122, Loss: 0.22648431939216188, Train MAPE: 38.417552951666025, Test MAPE: 41.28461638964139\n",
      "Epoch 123, Loss: 0.22511468508925575, Train MAPE: 38.122918976270235, Test MAPE: 40.95755593226506\n",
      "Epoch 124, Loss: 0.2236385754488695, Train MAPE: 38.05811602610808, Test MAPE: 40.90132208604079\n",
      "Epoch 125, Loss: 0.22235553334108912, Train MAPE: 37.81522646042017, Test MAPE: 40.61985932863676\n",
      "Epoch 126, Loss: 0.22089263235505383, Train MAPE: 37.72800556512979, Test MAPE: 40.54147556011493\n",
      "Epoch 127, Loss: 0.2195383371749463, Train MAPE: 37.33721580505371, Test MAPE: 40.10050904934223\n",
      "Epoch 128, Loss: 0.21824170072348073, Train MAPE: 37.67990281948676, Test MAPE: 40.50517612603994\n",
      "Epoch 129, Loss: 0.21690283387971038, Train MAPE: 37.14478166928658, Test MAPE: 39.89949467732356\n",
      "Epoch 130, Loss: 0.21558844942886096, Train MAPE: 37.21232260649021, Test MAPE: 39.99142348949726\n",
      "Epoch 131, Loss: 0.21428700170766277, Train MAPE: 36.84492599597344, Test MAPE: 39.568438185178316\n",
      "Epoch 132, Loss: 0.21311523124145773, Train MAPE: 36.91379571327796, Test MAPE: 39.67421122330886\n",
      "Epoch 133, Loss: 0.21183042369353083, Train MAPE: 36.55766724164669, Test MAPE: 39.255608778733475\n",
      "Epoch 134, Loss: 0.21062514224591164, Train MAPE: 36.67684401182028, Test MAPE: 39.4173739873446\n",
      "Epoch 135, Loss: 0.20946598552310697, Train MAPE: 36.677635818261365, Test MAPE: 39.43193273544311\n",
      "Epoch 136, Loss: 0.20828120678734893, Train MAPE: 36.594793361883895, Test MAPE: 39.35644576732929\n",
      "Epoch 137, Loss: 0.20709828870872465, Train MAPE: 35.957659358244676, Test MAPE: 38.61738434571486\n",
      "Epoch 138, Loss: 0.20598496200803382, Train MAPE: 36.41246992074526, Test MAPE: 39.1587722998399\n",
      "Epoch 139, Loss: 0.20486215479098835, Train MAPE: 36.039292570260855, Test MAPE: 38.73045536921575\n",
      "Epoch 140, Loss: 0.2037098681160177, Train MAPE: 36.13127465798305, Test MAPE: 38.850470506227936\n",
      "Epoch 141, Loss: 0.20261900067114486, Train MAPE: 35.85534356190608, Test MAPE: 38.53475024149968\n",
      "Epoch 142, Loss: 0.201648336631031, Train MAPE: 35.97314304205088, Test MAPE: 38.683822954618016\n",
      "Epoch 143, Loss: 0.20043315823333194, Train MAPE: 35.61567176855527, Test MAPE: 38.27373604407677\n",
      "Epoch 144, Loss: 0.19939334026466196, Train MAPE: 35.80413321348337, Test MAPE: 38.50517283953153\n",
      "Epoch 145, Loss: 0.1983721661488884, Train MAPE: 35.510807597637175, Test MAPE: 38.17968630423913\n",
      "Epoch 146, Loss: 0.19733534408016848, Train MAPE: 35.22078328866225, Test MAPE: 37.83580937018761\n",
      "Epoch 147, Loss: 0.19637254419951486, Train MAPE: 35.16281257042518, Test MAPE: 37.78594631781945\n",
      "Epoch 148, Loss: 0.19531577891813448, Train MAPE: 35.35244446901174, Test MAPE: 38.01936889061561\n",
      "Epoch 149, Loss: 0.19434056157747714, Train MAPE: 34.84110900071951, Test MAPE: 37.426539098299465\n",
      "Epoch 150, Loss: 0.1933945087572703, Train MAPE: 34.929937792741335, Test MAPE: 37.54176056935237\n",
      "Epoch 151, Loss: 0.19245635023197302, Train MAPE: 35.08307938025548, Test MAPE: 37.73496919778677\n",
      "Epoch 152, Loss: 0.19148774030260168, Train MAPE: 34.692190344517044, Test MAPE: 37.280748220590446\n",
      "Epoch 153, Loss: 0.1905917516300598, Train MAPE: 34.78175928317584, Test MAPE: 37.39457292556763\n",
      "Epoch 154, Loss: 0.1896590854256199, Train MAPE: 34.24828851039593, Test MAPE: 36.77785436923687\n",
      "Epoch 155, Loss: 0.1888385384522665, Train MAPE: 34.48607538755123, Test MAPE: 37.065654747302716\n",
      "Epoch 156, Loss: 0.18783763971967765, Train MAPE: 34.66447199032857, Test MAPE: 37.289360508551965\n",
      "Epoch 157, Loss: 0.18701082374721478, Train MAPE: 34.04331903549341, Test MAPE: 36.560622706780066\n",
      "Epoch 158, Loss: 0.18614827082850613, Train MAPE: 34.325357951567725, Test MAPE: 36.903667882772595\n",
      "Epoch 159, Loss: 0.1852599416895268, Train MAPE: 34.009936859974495, Test MAPE: 36.53602688862727\n",
      "Epoch 160, Loss: 0.184443368200356, Train MAPE: 33.824401254837326, Test MAPE: 36.32905569076538\n",
      "Epoch 161, Loss: 0.1835877309553325, Train MAPE: 33.71926188835731, Test MAPE: 36.21468029022217\n",
      "Epoch 162, Loss: 0.18275928043115597, Train MAPE: 34.30825515068494, Test MAPE: 36.91675261717576\n",
      "Epoch 163, Loss: 0.18199491836130618, Train MAPE: 33.66956712007523, Test MAPE: 36.17738097264216\n",
      "Epoch 164, Loss: 0.18116191028832243, Train MAPE: 33.59217655200224, Test MAPE: 36.092993171398454\n",
      "Epoch 165, Loss: 0.18040318257497767, Train MAPE: 33.753462921656094, Test MAPE: 36.28928506557758\n",
      "Epoch 166, Loss: 0.17961059411307082, Train MAPE: 33.56317555170793, Test MAPE: 36.075743726583624\n",
      "Epoch 167, Loss: 0.1788638808298856, Train MAPE: 33.64694970937876, Test MAPE: 36.179476466545694\n",
      "Epoch 168, Loss: 0.1780754482678066, Train MAPE: 33.21291084289551, Test MAPE: 35.676552024254434\n",
      "Epoch 169, Loss: 0.17728804716697105, Train MAPE: 33.46817235488158, Test MAPE: 35.98185682296753\n",
      "Epoch 170, Loss: 0.17655332634368767, Train MAPE: 33.139184211767635, Test MAPE: 35.59824480643639\n",
      "Epoch 171, Loss: 0.17589760843592767, Train MAPE: 33.246776097554424, Test MAPE: 35.73686094284058\n",
      "Epoch 172, Loss: 0.17511728485114872, Train MAPE: 33.08925155217831, Test MAPE: 35.55782266029945\n",
      "Epoch 173, Loss: 0.17436459981478178, Train MAPE: 32.96971590427252, Test MAPE: 35.42645974526039\n",
      "Epoch 174, Loss: 0.17366381089489621, Train MAPE: 32.877137459241425, Test MAPE: 35.32198300728431\n",
      "Epoch 175, Loss: 0.17299501874412482, Train MAPE: 32.795683917632466, Test MAPE: 35.23481530409593\n",
      "Epoch 176, Loss: 0.17231662678890503, Train MAPE: 32.826101660728455, Test MAPE: 35.2754547559298\n",
      "Epoch 177, Loss: 0.17161483355224705, Train MAPE: 32.54018410077462, Test MAPE: 34.944344117091255\n",
      "Epoch 178, Loss: 0.17093091542975833, Train MAPE: 32.722857938363, Test MAPE: 35.167205909582286\n",
      "Epoch 179, Loss: 0.17024305559551486, Train MAPE: 32.66298520840132, Test MAPE: 35.10878190627465\n",
      "Epoch 180, Loss: 0.16958961978268164, Train MAPE: 32.69672329425812, Test MAPE: 35.15589764301593\n",
      "Epoch 181, Loss: 0.16898603176053326, Train MAPE: 32.586962566925926, Test MAPE: 35.02954378861647\n",
      "Epoch 182, Loss: 0.1682800437323749, Train MAPE: 32.51420082220665, Test MAPE: 34.94867329964271\n",
      "Epoch 183, Loss: 0.16763423287596266, Train MAPE: 32.65929482441682, Test MAPE: 35.12300090422997\n",
      "Epoch 184, Loss: 0.16704849028386748, Train MAPE: 32.32833969501348, Test MAPE: 34.750784276081966\n",
      "Epoch 185, Loss: 0.16640286159773285, Train MAPE: 31.94002063366083, Test MAPE: 34.29905141683725\n",
      "Epoch 186, Loss: 0.1658613066941213, Train MAPE: 32.102140487157385, Test MAPE: 34.49632160113408\n",
      "Epoch 187, Loss: 0.16520619458841304, Train MAPE: 32.1373026233453, Test MAPE: 34.53871140113244\n",
      "Epoch 188, Loss: 0.16457425293046982, Train MAPE: 32.020655070818385, Test MAPE: 34.41901362859286\n",
      "Epoch 189, Loss: 0.16400960305920587, Train MAPE: 31.72262077514942, Test MAPE: 34.07101945510277\n",
      "Epoch 190, Loss: 0.16337384286497791, Train MAPE: 32.06069846886855, Test MAPE: 34.47537423647367\n",
      "Epoch 191, Loss: 0.16279079805558117, Train MAPE: 32.043275912908406, Test MAPE: 34.46110779322111\n",
      "Epoch 192, Loss: 0.16222719194618268, Train MAPE: 31.635462916814365, Test MAPE: 33.98781343606802\n",
      "Epoch 193, Loss: 0.16169094703375148, Train MAPE: 31.451890540122985, Test MAPE: 33.774382396844715\n",
      "Epoch 194, Loss: 0.16113443510033765, Train MAPE: 31.395774869735423, Test MAPE: 33.71233322436993\n",
      "Epoch 195, Loss: 0.16054348569864837, Train MAPE: 31.3190850010285, Test MAPE: 33.638273301491374\n",
      "Epoch 196, Loss: 0.15999278387842844, Train MAPE: 31.434451255431544, Test MAPE: 33.77298460373512\n",
      "Epoch 197, Loss: 0.1594317652320919, Train MAPE: 31.166843787523415, Test MAPE: 33.46616192230812\n",
      "Epoch 198, Loss: 0.1588884672526127, Train MAPE: 31.01215298634309, Test MAPE: 33.29012045860291\n",
      "Epoch 199, Loss: 0.15836094501786507, Train MAPE: 31.367339307528276, Test MAPE: 33.716537006084735\n",
      "Epoch 200, Loss: 0.15785142860303705, Train MAPE: 31.23437780692027, Test MAPE: 33.56824000798739\n",
      "Epoch 201, Loss: 0.157347629000791, Train MAPE: 31.03291359406251, Test MAPE: 33.33889709986173\n",
      "Epoch 202, Loss: 0.1567641749798965, Train MAPE: 30.74405770760316, Test MAPE: 32.99971470832825\n",
      "Epoch 203, Loss: 0.1564193497161166, Train MAPE: 31.206660002928515, Test MAPE: 33.54898610115051\n",
      "Epoch 204, Loss: 0.15575788764175602, Train MAPE: 31.23077775790141, Test MAPE: 33.58439082732568\n",
      "Epoch 205, Loss: 0.15528437856119126, Train MAPE: 30.88693530192742, Test MAPE: 33.18915727688716\n",
      "Epoch 206, Loss: 0.15478025303007317, Train MAPE: 30.872925966519574, Test MAPE: 33.18019375434289\n",
      "Epoch 207, Loss: 0.15429635496469787, Train MAPE: 30.82565707060007, Test MAPE: 33.134807583001944\n",
      "Epoch 208, Loss: 0.15382925285957755, Train MAPE: 30.707459071049325, Test MAPE: 32.99950056809645\n",
      "Epoch 209, Loss: 0.15330826458162986, Train MAPE: 30.962972402572632, Test MAPE: 33.30373275463398\n",
      "Epoch 210, Loss: 0.15282398922583804, Train MAPE: 30.579358515372643, Test MAPE: 32.8546486047598\n",
      "Epoch 211, Loss: 0.15235532763987206, Train MAPE: 30.41808384198409, Test MAPE: 32.670232505064746\n",
      "Epoch 212, Loss: 0.1518769815725346, Train MAPE: 30.419454671786383, Test MAPE: 32.68042268019456\n",
      "Epoch 213, Loss: 0.15143230084079104, Train MAPE: 30.530023190608393, Test MAPE: 32.815523628088144\n",
      "Epoch 214, Loss: 0.1509767778020782, Train MAPE: 30.41931392779717, Test MAPE: 32.693386558385995\n",
      "Epoch 215, Loss: 0.15053113881283656, Train MAPE: 30.4501500413968, Test MAPE: 32.73264506413386\n",
      "Epoch 216, Loss: 0.15006699615802901, Train MAPE: 30.234767686403714, Test MAPE: 32.487076950073245\n",
      "Epoch 217, Loss: 0.14960775379664623, Train MAPE: 30.315353751182556, Test MAPE: 32.583087044495805\n",
      "Epoch 218, Loss: 0.14918581631500274, Train MAPE: 30.354662811756135, Test MAPE: 32.63352471498342\n",
      "Epoch 219, Loss: 0.1487527443490063, Train MAPE: 30.29723358979592, Test MAPE: 32.573920521369345\n",
      "Epoch 220, Loss: 0.14827546247173673, Train MAPE: 29.931960756962116, Test MAPE: 32.15196721370403\n",
      "Epoch 221, Loss: 0.147867147753445, Train MAPE: 29.920867347717284, Test MAPE: 32.1443412230565\n",
      "Epoch 222, Loss: 0.1474596686123942, Train MAPE: 30.096084923927602, Test MAPE: 32.35051621657151\n",
      "Epoch 223, Loss: 0.14702299998786586, Train MAPE: 30.045500586583064, Test MAPE: 32.29739089012146\n",
      "Epoch 224, Loss: 0.14664869901115218, Train MAPE: 29.8345870751601, Test MAPE: 32.05760806890635\n",
      "Epoch 225, Loss: 0.14616498950725565, Train MAPE: 29.979222811185398, Test MAPE: 32.23581762313843\n",
      "Epoch 226, Loss: 0.1457889607301555, Train MAPE: 29.875737080207237, Test MAPE: 32.116109756323006\n",
      "Epoch 227, Loss: 0.14536250115264782, Train MAPE: 29.98067642725431, Test MAPE: 32.248465211574846\n",
      "Epoch 228, Loss: 0.14495052290507235, Train MAPE: 29.649150770444137, Test MAPE: 31.862242045769325\n",
      "Epoch 229, Loss: 0.14459737956237334, Train MAPE: 29.753648746930637, Test MAPE: 31.989645451765796\n",
      "Epoch 230, Loss: 0.14417399330637776, Train MAPE: 29.74555030419276, Test MAPE: 31.986851827914897\n",
      "Epoch 231, Loss: 0.14379147381808322, Train MAPE: 29.693387092076815, Test MAPE: 31.92738585472107\n",
      "Epoch 232, Loss: 0.14342660267765706, Train MAPE: 29.702066098726714, Test MAPE: 31.94358623944796\n",
      "Epoch 233, Loss: 0.14298751955636993, Train MAPE: 29.29726244486295, Test MAPE: 31.475268066846407\n",
      "Epoch 234, Loss: 0.14263849822768512, Train MAPE: 29.386470869871285, Test MAPE: 31.577470761079056\n",
      "Epoch 235, Loss: 0.14225807604379953, Train MAPE: 29.421863986895634, Test MAPE: 31.632838792067307\n",
      "Epoch 236, Loss: 0.14189484351529524, Train MAPE: 29.32486110650576, Test MAPE: 31.52113396204435\n",
      "Epoch 237, Loss: 0.14151513174606056, Train MAPE: 29.460039755931266, Test MAPE: 31.6839649273799\n",
      "Epoch 238, Loss: 0.14113112416369125, Train MAPE: 29.444245006487918, Test MAPE: 31.67036271095276\n",
      "Epoch 239, Loss: 0.14083357971663085, Train MAPE: 29.120690221052904, Test MAPE: 31.296543102998\n",
      "Epoch 240, Loss: 0.14038580380953275, Train MAPE: 29.1050356223033, Test MAPE: 31.28580299157363\n",
      "Epoch 241, Loss: 0.14004727524991792, Train MAPE: 29.22137901232793, Test MAPE: 31.43025823373061\n",
      "Epoch 242, Loss: 0.1396815173059272, Train MAPE: 29.28540366612948, Test MAPE: 31.505204607890203\n",
      "Epoch 243, Loss: 0.13935769613509855, Train MAPE: 29.073636753742512, Test MAPE: 31.25924837405865\n",
      "Epoch 244, Loss: 0.1389820909772355, Train MAPE: 28.966788401053503, Test MAPE: 31.13941070116483\n",
      "Epoch 245, Loss: 0.13865405823319005, Train MAPE: 28.924813395280104, Test MAPE: 31.094023774220393\n",
      "Epoch 246, Loss: 0.13832485248657087, Train MAPE: 29.1174457852657, Test MAPE: 31.32782168021569\n",
      "Epoch 247, Loss: 0.13799952301017654, Train MAPE: 28.94328301503108, Test MAPE: 31.128472551932703\n",
      "Epoch 248, Loss: 0.13761848295286586, Train MAPE: 28.96300556201201, Test MAPE: 31.157009429198045\n",
      "Epoch 249, Loss: 0.13729613811816446, Train MAPE: 28.900245022773742, Test MAPE: 31.08831244615408\n",
      "Epoch 250, Loss: 0.13695976740071694, Train MAPE: 28.934979217786054, Test MAPE: 31.13803551747249\n",
      "Epoch 251, Loss: 0.1366041310597211, Train MAPE: 28.650586450099944, Test MAPE: 30.801920223236085\n",
      "Epoch 252, Loss: 0.13628691967863302, Train MAPE: 28.848051330676444, Test MAPE: 31.040083224956806\n",
      "Epoch 253, Loss: 0.13599497985381345, Train MAPE: 28.723720892576072, Test MAPE: 30.899284659899198\n",
      "Epoch 254, Loss: 0.13564830539174952, Train MAPE: 28.859618800420026, Test MAPE: 31.06569202863253\n",
      "Epoch 255, Loss: 0.13534686574891497, Train MAPE: 28.598662988956157, Test MAPE: 30.761237419568577\n",
      "Epoch 256, Loss: 0.13499475273017128, Train MAPE: 28.81663506948031, Test MAPE: 31.02680583366981\n",
      "Epoch 257, Loss: 0.13472833235103351, Train MAPE: 28.568491950401892, Test MAPE: 30.735972430155826\n",
      "Epoch 258, Loss: 0.13441808437212155, Train MAPE: 28.27491705784431, Test MAPE: 30.39052811035743\n",
      "Epoch 259, Loss: 0.1341746880690782, Train MAPE: 28.354123154970317, Test MAPE: 30.4928259739509\n",
      "Epoch 260, Loss: 0.13377306416738205, Train MAPE: 28.366846159788278, Test MAPE: 30.512057671180138\n",
      "Epoch 261, Loss: 0.13348039890138003, Train MAPE: 28.45823279619217, Test MAPE: 30.62684842256399\n",
      "Epoch 262, Loss: 0.13313488455560918, Train MAPE: 28.55714450432704, Test MAPE: 30.747220039367676\n",
      "Epoch 263, Loss: 0.13288258889403481, Train MAPE: 28.417582440376282, Test MAPE: 30.586431411596443\n",
      "Epoch 264, Loss: 0.1325747594380608, Train MAPE: 28.20795300923861, Test MAPE: 30.342109735195454\n",
      "Epoch 265, Loss: 0.1323055939605603, Train MAPE: 28.27920112518164, Test MAPE: 30.434695265843317\n",
      "Epoch 266, Loss: 0.13199510041337748, Train MAPE: 28.22533786205145, Test MAPE: 30.374105922992413\n",
      "Epoch 267, Loss: 0.13170071069616823, Train MAPE: 28.159932640882637, Test MAPE: 30.30073078595675\n",
      "Epoch 268, Loss: 0.13142254705039355, Train MAPE: 28.084915228990408, Test MAPE: 30.218012277896587\n",
      "Epoch 269, Loss: 0.13114338539218387, Train MAPE: 28.144292424275324, Test MAPE: 30.29478988647461\n",
      "Epoch 270, Loss: 0.13083779832813888, Train MAPE: 28.126040561382588, Test MAPE: 30.279648901866032\n",
      "Epoch 271, Loss: 0.13056660344322712, Train MAPE: 28.237284939105695, Test MAPE: 30.418416481751663\n",
      "Epoch 272, Loss: 0.13028753033767526, Train MAPE: 28.069024157524108, Test MAPE: 30.218716687422532\n",
      "Epoch 273, Loss: 0.13001116356955697, Train MAPE: 27.962695491313934, Test MAPE: 30.095895279370822\n",
      "Epoch 274, Loss: 0.12974223433993756, Train MAPE: 27.84072683774508, Test MAPE: 29.95650416154128\n",
      "Epoch 275, Loss: 0.12946850807011986, Train MAPE: 27.82155213906215, Test MAPE: 29.93851529634916\n",
      "Epoch 276, Loss: 0.12918586499559193, Train MAPE: 28.139311327384068, Test MAPE: 30.32679807956402\n",
      "Epoch 277, Loss: 0.12893671162713033, Train MAPE: 27.805799903319432, Test MAPE: 29.932124658731315\n",
      "Epoch 278, Loss: 0.1286697183055087, Train MAPE: 27.87787462381216, Test MAPE: 30.023466414671677\n",
      "Epoch 279, Loss: 0.12839297400966573, Train MAPE: 27.707283495939695, Test MAPE: 29.82321084829477\n",
      "Epoch 280, Loss: 0.12812971661332995, Train MAPE: 27.756004489385166, Test MAPE: 29.89045140193059\n",
      "Epoch 281, Loss: 0.12789692777352266, Train MAPE: 27.79691862143003, Test MAPE: 29.940908648417548\n",
      "Epoch 282, Loss: 0.12765391337183807, Train MAPE: 27.752515571850996, Test MAPE: 29.89570776499235\n",
      "Epoch 283, Loss: 0.12734165376123902, Train MAPE: 27.41630937044437, Test MAPE: 29.4934601526994\n",
      "Epoch 284, Loss: 0.12711279422510416, Train MAPE: 27.627658680769112, Test MAPE: 29.752992116487942\n",
      "Epoch 285, Loss: 0.1268543365304, Train MAPE: 27.677914470892688, Test MAPE: 29.822472854760978\n",
      "Epoch 286, Loss: 0.12656642731565695, Train MAPE: 27.34938646554947, Test MAPE: 29.427190494537353\n",
      "Epoch 287, Loss: 0.12638040970957193, Train MAPE: 27.711280725552484, Test MAPE: 29.87273341325613\n",
      "Epoch 288, Loss: 0.12610119524626778, Train MAPE: 27.761703813993012, Test MAPE: 29.941417477681085\n",
      "Epoch 289, Loss: 0.12585988407238172, Train MAPE: 27.491110577033115, Test MAPE: 29.615020865660448\n",
      "Epoch 290, Loss: 0.12558566044586209, Train MAPE: 27.374671190518598, Test MAPE: 29.47855909054096\n",
      "Epoch 291, Loss: 0.12536857023906822, Train MAPE: 27.43717552423477, Test MAPE: 29.56118571941669\n",
      "Epoch 292, Loss: 0.12511090457600613, Train MAPE: 27.50838287793673, Test MAPE: 29.656044259438147\n",
      "Epoch 293, Loss: 0.12488572764246222, Train MAPE: 27.243205867363855, Test MAPE: 29.344278071476865\n",
      "Epoch 294, Loss: 0.1246578054377236, Train MAPE: 27.340654004537143, Test MAPE: 29.463254946928757\n",
      "Epoch 295, Loss: 0.12440898971667944, Train MAPE: 27.545685075796566, Test MAPE: 29.71839451789856\n",
      "Epoch 296, Loss: 0.12417749397528287, Train MAPE: 27.3340161892084, Test MAPE: 29.461270200289214\n",
      "Epoch 297, Loss: 0.12400141990062996, Train MAPE: 27.296143802312706, Test MAPE: 29.418336079670834\n",
      "Epoch 298, Loss: 0.1237432504287706, Train MAPE: 27.115786190216358, Test MAPE: 29.207331749109123\n",
      "Epoch 299, Loss: 0.12344147907211804, Train MAPE: 26.95913803577423, Test MAPE: 29.022316730939426\n",
      "Epoch 300, Loss: 0.12327351687440219, Train MAPE: 27.206000306973092, Test MAPE: 29.328046758358294\n",
      "Epoch 301, Loss: 0.12303724981522045, Train MAPE: 27.15143781900406, Test MAPE: 29.266391526735745\n",
      "Epoch 302, Loss: 0.12278055873198004, Train MAPE: 27.250908182217525, Test MAPE: 29.390352792006272\n",
      "Epoch 303, Loss: 0.12262379804518647, Train MAPE: 27.060475066991952, Test MAPE: 29.165151614409226\n",
      "Epoch 304, Loss: 0.12237075511187029, Train MAPE: 27.133438608279594, Test MAPE: 29.262655272850623\n",
      "Epoch 305, Loss: 0.12215321637833347, Train MAPE: 27.111837009283214, Test MAPE: 29.237986003435577\n",
      "Epoch 306, Loss: 0.12192760406850049, Train MAPE: 26.936133488324973, Test MAPE: 29.025399409807644\n",
      "Epoch 307, Loss: 0.12169241934405783, Train MAPE: 26.865598468597117, Test MAPE: 28.94898436986483\n",
      "Epoch 308, Loss: 0.12150047014180858, Train MAPE: 26.90966723882235, Test MAPE: 29.00900421509376\n",
      "Epoch 309, Loss: 0.12126255918365832, Train MAPE: 27.020606432511258, Test MAPE: 29.146357587667612\n",
      "Epoch 310, Loss: 0.12106795612400255, Train MAPE: 26.815370847628667, Test MAPE: 28.90025973686805\n",
      "Epoch 311, Loss: 0.12086791809504994, Train MAPE: 26.790825450420378, Test MAPE: 28.877478236418504\n",
      "Epoch 312, Loss: 0.12060683033823107, Train MAPE: 27.14096939105254, Test MAPE: 29.303651413550742\n",
      "Epoch 313, Loss: 0.12044655465735839, Train MAPE: 26.779647680429314, Test MAPE: 28.87592264322134\n",
      "Epoch 314, Loss: 0.12024831042195169, Train MAPE: 26.731991498286906, Test MAPE: 28.822623722369855\n",
      "Epoch 315, Loss: 0.12002628840183696, Train MAPE: 26.748117859546955, Test MAPE: 28.845816373825073\n",
      "Epoch 316, Loss: 0.11982267438792266, Train MAPE: 26.707741034030914, Test MAPE: 28.79951953154344\n",
      "Epoch 317, Loss: 0.11959398789689518, Train MAPE: 26.817638786939476, Test MAPE: 28.93738631468553\n",
      "Epoch 318, Loss: 0.11944353056874556, Train MAPE: 26.660255226722132, Test MAPE: 28.75024958023658\n",
      "Epoch 319, Loss: 0.11921025430425429, Train MAPE: 26.76562452499683, Test MAPE: 28.88344202041626\n",
      "Epoch 320, Loss: 0.11901136764253561, Train MAPE: 26.755437883963953, Test MAPE: 28.87583426328806\n",
      "Epoch 321, Loss: 0.11882736110486664, Train MAPE: 26.591245864904845, Test MAPE: 28.67799017612751\n",
      "Epoch 322, Loss: 0.11862640515722048, Train MAPE: 26.649495712610392, Test MAPE: 28.749812801067645\n",
      "Epoch 323, Loss: 0.11847088075636958, Train MAPE: 26.436819892663223, Test MAPE: 28.499953743127676\n",
      "Epoch 324, Loss: 0.11822039201461639, Train MAPE: 26.665722327965955, Test MAPE: 28.784738991810727\n",
      "Epoch 325, Loss: 0.11805063229806434, Train MAPE: 26.47617264344142, Test MAPE: 28.560153348629292\n",
      "Epoch 326, Loss: 0.11784260058059143, Train MAPE: 26.320180201530455, Test MAPE: 28.36772626363314\n",
      "Epoch 327, Loss: 0.11763691814759603, Train MAPE: 26.52271134211467, Test MAPE: 28.622424844595102\n",
      "Epoch 328, Loss: 0.11746709213162271, Train MAPE: 26.416971588134764, Test MAPE: 28.498830641233003\n",
      "Epoch 329, Loss: 0.1172589358730385, Train MAPE: 26.70326037131823, Test MAPE: 28.852406787872315\n",
      "Epoch 330, Loss: 0.11708949214348999, Train MAPE: 26.37203482939647, Test MAPE: 28.447736663084765\n",
      "Epoch 331, Loss: 0.1169274730554137, Train MAPE: 26.334987813692827, Test MAPE: 28.411123587534977\n",
      "Epoch 332, Loss: 0.11673162851769191, Train MAPE: 26.632526926810925, Test MAPE: 28.77674802633432\n",
      "Epoch 333, Loss: 0.11654089144431054, Train MAPE: 26.316283412163074, Test MAPE: 28.39475200726436\n",
      "Epoch 334, Loss: 0.11642742046298316, Train MAPE: 26.290712207097272, Test MAPE: 28.37269973388085\n",
      "Epoch 335, Loss: 0.11617088415839065, Train MAPE: 26.375962725969462, Test MAPE: 28.479261134221005\n",
      "Epoch 336, Loss: 0.11599566723721531, Train MAPE: 26.315774138157185, Test MAPE: 28.395347338456375\n",
      "Epoch 337, Loss: 0.11578831604252068, Train MAPE: 26.203456410994896, Test MAPE: 28.27583353336041\n",
      "Epoch 338, Loss: 0.11560726958828477, Train MAPE: 26.331041508454543, Test MAPE: 28.439069729584915\n",
      "Epoch 339, Loss: 0.11544413268745232, Train MAPE: 26.327240616541644, Test MAPE: 28.43817002223088\n",
      "Epoch 340, Loss: 0.11527286159017912, Train MAPE: 26.18185816544753, Test MAPE: 28.263641592172476\n",
      "Epoch 341, Loss: 0.11508025962931033, Train MAPE: 25.84341522822013, Test MAPE: 27.846590739030106\n",
      "Epoch 342, Loss: 0.11490977847268088, Train MAPE: 26.288587809526003, Test MAPE: 28.39686618951651\n",
      "Epoch 343, Loss: 0.11474120614942736, Train MAPE: 26.096914705863366, Test MAPE: 28.172672612850484\n",
      "Epoch 344, Loss: 0.11458768570312083, Train MAPE: 26.227868622999924, Test MAPE: 28.333515482682447\n",
      "Epoch 345, Loss: 0.11442047082102642, Train MAPE: 26.12496862594898, Test MAPE: 28.212912819935724\n",
      "Epoch 346, Loss: 0.11424996137010077, Train MAPE: 26.16391187264369, Test MAPE: 28.269982286599966\n",
      "Epoch 347, Loss: 0.11411780823344508, Train MAPE: 26.021894587003267, Test MAPE: 28.097332928730893\n",
      "Epoch 348, Loss: 0.11389769644727214, Train MAPE: 26.07750798830619, Test MAPE: 28.166571477743297\n",
      "Epoch 349, Loss: 0.11374133712827013, Train MAPE: 25.98678290752264, Test MAPE: 28.066171275652373\n",
      "Epoch 350, Loss: 0.11357474352340573, Train MAPE: 25.952524307140937, Test MAPE: 28.02304694102361\n",
      "Epoch 351, Loss: 0.11338502210744011, Train MAPE: 25.68614097558535, Test MAPE: 27.694183217562163\n",
      "Epoch 352, Loss: 0.1132700085801144, Train MAPE: 25.948281255135168, Test MAPE: 28.031478940523588\n",
      "Epoch 353, Loss: 0.11307766009659435, Train MAPE: 25.887925008627086, Test MAPE: 27.960941688831035\n",
      "Epoch 354, Loss: 0.11292280812676136, Train MAPE: 25.92833211513666, Test MAPE: 28.015652601535503\n",
      "Epoch 355, Loss: 0.11275265862484678, Train MAPE: 25.776159965991972, Test MAPE: 27.828886600641106\n",
      "Epoch 356, Loss: 0.11259581733566637, Train MAPE: 25.910028166954334, Test MAPE: 28.001464913441584\n",
      "Epoch 357, Loss: 0.11243640308698209, Train MAPE: 25.72233920464149, Test MAPE: 27.77524607731746\n",
      "Epoch 358, Loss: 0.11227551359110154, Train MAPE: 25.859185857039233, Test MAPE: 27.947343873977662\n",
      "Epoch 359, Loss: 0.1121068702580837, Train MAPE: 25.68137791248468, Test MAPE: 27.728625378241905\n",
      "Epoch 360, Loss: 0.11195963498097486, Train MAPE: 25.69278760598256, Test MAPE: 27.750205560830924\n",
      "Epoch 361, Loss: 0.1118000341561408, Train MAPE: 25.886005202623515, Test MAPE: 27.991984253663283\n",
      "Epoch 362, Loss: 0.11164774488872634, Train MAPE: 25.725762723042415, Test MAPE: 27.798920774459837\n",
      "Epoch 363, Loss: 0.11148921469799601, Train MAPE: 25.799172422519096, Test MAPE: 27.89532288771409\n",
      "Epoch 364, Loss: 0.11136379959240843, Train MAPE: 25.69081214482968, Test MAPE: 27.76751079559326\n",
      "Epoch 365, Loss: 0.11117112251858298, Train MAPE: 25.56097148014949, Test MAPE: 27.605823762600238\n",
      "Epoch 366, Loss: 0.11104286824030658, Train MAPE: 25.603874158859252, Test MAPE: 27.666151211811947\n",
      "Epoch 367, Loss: 0.11087446002683674, Train MAPE: 25.467348438959856, Test MAPE: 27.504033767260037\n",
      "Epoch 368, Loss: 0.11073784242527417, Train MAPE: 25.547526990450347, Test MAPE: 27.609340139535757\n",
      "Epoch 369, Loss: 0.11058111327180925, Train MAPE: 25.665228659373064, Test MAPE: 27.75638093948364\n",
      "Epoch 370, Loss: 0.11042834551503453, Train MAPE: 25.545035209105563, Test MAPE: 27.610814872154823\n",
      "Epoch 371, Loss: 0.11028350041462825, Train MAPE: 25.46690678504797, Test MAPE: 27.51716251373291\n",
      "Epoch 372, Loss: 0.11015181194346112, Train MAPE: 25.518487912874956, Test MAPE: 27.588546829957227\n",
      "Epoch 373, Loss: 0.10997773837298155, Train MAPE: 25.62423897248048, Test MAPE: 27.721584265048687\n",
      "Epoch 374, Loss: 0.10986128978681965, Train MAPE: 25.594743933127475, Test MAPE: 27.689057386838474\n",
      "Epoch 375, Loss: 0.10970751876190592, Train MAPE: 25.36400141440905, Test MAPE: 27.404486304063063\n",
      "Epoch 376, Loss: 0.10954395959225412, Train MAPE: 25.559286023103272, Test MAPE: 27.650294043467596\n",
      "Epoch 377, Loss: 0.10938755541036908, Train MAPE: 25.47321986601903, Test MAPE: 27.550181143100446\n",
      "Epoch 378, Loss: 0.10926034589692091, Train MAPE: 25.4224433357899, Test MAPE: 27.489217314353358\n",
      "Epoch 379, Loss: 0.109110084340836, Train MAPE: 25.502913707953233, Test MAPE: 27.591692627393282\n",
      "Epoch 380, Loss: 0.10900971537921578, Train MAPE: 25.363228106498717, Test MAPE: 27.42231188920828\n",
      "Epoch 381, Loss: 0.10886475583717514, Train MAPE: 25.445987902237817, Test MAPE: 27.529822595302875\n",
      "Epoch 382, Loss: 0.10869571091607214, Train MAPE: 25.355085205114804, Test MAPE: 27.420891978190497\n",
      "Epoch 383, Loss: 0.10859059774173567, Train MAPE: 25.423926797279943, Test MAPE: 27.5068270609929\n",
      "Epoch 384, Loss: 0.10845128405445184, Train MAPE: 25.304052367577185, Test MAPE: 27.363420801896314\n",
      "Epoch 385, Loss: 0.10829026512622547, Train MAPE: 25.158608243098627, Test MAPE: 27.19107001744784\n",
      "Epoch 386, Loss: 0.1081683092025252, Train MAPE: 25.414637126372412, Test MAPE: 27.51178076817439\n",
      "Epoch 387, Loss: 0.10801435880219708, Train MAPE: 25.383222046265235, Test MAPE: 27.474480291513295\n",
      "Epoch 388, Loss: 0.10788703705184162, Train MAPE: 25.344579747089973, Test MAPE: 27.43660474190345\n",
      "Epoch 389, Loss: 0.10773700560944585, Train MAPE: 25.188879423875076, Test MAPE: 27.239984681056097\n",
      "Epoch 390, Loss: 0.10757391811527599, Train MAPE: 25.485054998214427, Test MAPE: 27.611171117195717\n",
      "Epoch 391, Loss: 0.10749535205093427, Train MAPE: 25.334776789408465, Test MAPE: 27.433542992518497\n",
      "Epoch 392, Loss: 0.10732242521077681, Train MAPE: 25.065639456418843, Test MAPE: 27.10467430628263\n",
      "Epoch 393, Loss: 0.1072525400435552, Train MAPE: 25.18919697174659, Test MAPE: 27.25712029383733\n",
      "Epoch 394, Loss: 0.10708083277747321, Train MAPE: 25.121164465867555, Test MAPE: 27.181128821006187\n",
      "Epoch 395, Loss: 0.10696332497128214, Train MAPE: 25.203133299717535, Test MAPE: 27.28503843821012\n",
      "Epoch 396, Loss: 0.10681828260779953, Train MAPE: 25.119056201898136, Test MAPE: 27.18799773362967\n",
      "Epoch 397, Loss: 0.10669464562446453, Train MAPE: 25.19660804638496, Test MAPE: 27.28701662650475\n",
      "Epoch 398, Loss: 0.10656862742124269, Train MAPE: 25.20368488568526, Test MAPE: 27.30028850848858\n",
      "Epoch 399, Loss: 0.10643148394480634, Train MAPE: 24.87807311920019, Test MAPE: 26.90173416871291\n",
      "Epoch 400, Loss: 0.10633974938402668, Train MAPE: 25.0744257743542, Test MAPE: 27.145029794252835\n",
      "Epoch 401, Loss: 0.10619460261211945, Train MAPE: 25.01614919075599, Test MAPE: 27.075587305655848\n",
      "Epoch 402, Loss: 0.10606075717100444, Train MAPE: 24.97965995531816, Test MAPE: 27.036993547586295\n",
      "Epoch 403, Loss: 0.10595882659903369, Train MAPE: 24.951786847297964, Test MAPE: 27.00869824702923\n",
      "Epoch 404, Loss: 0.10581415679771453, Train MAPE: 25.011836288525508, Test MAPE: 27.08656788972708\n",
      "Epoch 405, Loss: 0.10568702555297373, Train MAPE: 25.072093549141517, Test MAPE: 27.165653096712553\n",
      "Epoch 406, Loss: 0.10556752283233577, Train MAPE: 24.97136373428198, Test MAPE: 27.043264040580162\n",
      "Epoch 407, Loss: 0.1054472156155568, Train MAPE: 24.75339254966149, Test MAPE: 26.775453164027287\n",
      "Epoch 408, Loss: 0.10531222369486036, Train MAPE: 25.079903304576874, Test MAPE: 27.18566930110638\n",
      "Epoch 409, Loss: 0.10519555925319975, Train MAPE: 24.82343643261836, Test MAPE: 26.866292572021486\n",
      "Epoch 410, Loss: 0.10511780652181747, Train MAPE: 24.88768940797219, Test MAPE: 26.954300561318032\n",
      "Epoch 411, Loss: 0.10495326023799582, Train MAPE: 24.951822265294883, Test MAPE: 27.036772196109478\n",
      "Epoch 412, Loss: 0.10485016290958112, Train MAPE: 24.85143486903264, Test MAPE: 26.917491428668683\n",
      "Epoch 413, Loss: 0.10472999870096548, Train MAPE: 24.967308641397036, Test MAPE: 27.06482732112591\n",
      "Epoch 414, Loss: 0.10460768239930845, Train MAPE: 24.977285386965825, Test MAPE: 27.08169463964609\n",
      "Epoch 415, Loss: 0.10446128886407957, Train MAPE: 24.600952161275423, Test MAPE: 26.61657857161302\n",
      "Epoch 416, Loss: 0.10437262234492944, Train MAPE: 24.867047617068657, Test MAPE: 26.946481880774865\n",
      "Epoch 417, Loss: 0.10424670295951029, Train MAPE: 24.640191155213575, Test MAPE: 26.671500598467315\n",
      "Epoch 418, Loss: 0.10413915771596993, Train MAPE: 24.77654839203908, Test MAPE: 26.84119480940012\n",
      "Epoch 419, Loss: 0.10402644952544227, Train MAPE: 24.78649200109335, Test MAPE: 26.861452473126924\n",
      "Epoch 420, Loss: 0.1039041304262355, Train MAPE: 24.6948553947302, Test MAPE: 26.75040973516611\n",
      "Epoch 421, Loss: 0.10379880677657918, Train MAPE: 24.62992330606167, Test MAPE: 26.674951182878935\n",
      "Epoch 422, Loss: 0.10366428981785877, Train MAPE: 24.686400692279523, Test MAPE: 26.746558816616353\n",
      "Epoch 423, Loss: 0.1035888358216303, Train MAPE: 24.693134646232313, Test MAPE: 26.75321010076083\n",
      "Epoch 424, Loss: 0.10344916012030668, Train MAPE: 24.67968022089738, Test MAPE: 26.74534451044523\n",
      "Epoch 425, Loss: 0.10333666275076281, Train MAPE: 24.69047472843757, Test MAPE: 26.764914233867938\n",
      "Epoch 426, Loss: 0.10319853134524937, Train MAPE: 24.81640991797814, Test MAPE: 26.9202440738678\n",
      "Epoch 427, Loss: 0.10310590207182731, Train MAPE: 24.753527871462015, Test MAPE: 26.84386765406682\n",
      "Epoch 428, Loss: 0.10299430945529961, Train MAPE: 24.632379657488602, Test MAPE: 26.704691377052892\n",
      "Epoch 429, Loss: 0.10288749563758476, Train MAPE: 24.51841006095593, Test MAPE: 26.563434355075543\n",
      "Epoch 430, Loss: 0.10276642664192388, Train MAPE: 24.651306975804843, Test MAPE: 26.73275152719938\n",
      "Epoch 431, Loss: 0.10264481384790718, Train MAPE: 24.39519216464116, Test MAPE: 26.417939332815315\n",
      "Epoch 432, Loss: 0.10258142670903068, Train MAPE: 24.536094748973845, Test MAPE: 26.59590654373169\n",
      "Epoch 433, Loss: 0.10244197128877904, Train MAPE: 24.511385420652537, Test MAPE: 26.57035421224741\n",
      "Epoch 434, Loss: 0.10232737246506776, Train MAPE: 24.55288914350363, Test MAPE: 26.620427736869225\n",
      "Epoch 435, Loss: 0.10222565396509778, Train MAPE: 24.45163552944477, Test MAPE: 26.50254912009606\n",
      "Epoch 436, Loss: 0.10210200746794446, Train MAPE: 24.532487492377943, Test MAPE: 26.603445566617527\n",
      "Epoch 437, Loss: 0.10201824359380855, Train MAPE: 24.49316436785918, Test MAPE: 26.56195577107943\n",
      "Epoch 438, Loss: 0.1019086378447425, Train MAPE: 24.48542909530493, Test MAPE: 26.55606663777278\n",
      "Epoch 439, Loss: 0.10178341480294385, Train MAPE: 24.45249463686576, Test MAPE: 26.52120821292584\n",
      "Epoch 440, Loss: 0.10170582072952619, Train MAPE: 24.349645404632273, Test MAPE: 26.39498880092914\n",
      "Epoch 441, Loss: 0.10157362393533381, Train MAPE: 24.416259843569534, Test MAPE: 26.481836773799017\n",
      "Epoch 442, Loss: 0.10151232936276267, Train MAPE: 24.313579695041362, Test MAPE: 26.357916120382455\n",
      "Epoch 443, Loss: 0.10145835803929143, Train MAPE: 24.350626171552218, Test MAPE: 26.405655413407544\n",
      "Epoch 444, Loss: 0.10125727666219553, Train MAPE: 24.23701847058076, Test MAPE: 26.265725487929124\n",
      "Epoch 445, Loss: 0.10116100540527931, Train MAPE: 24.403810950425957, Test MAPE: 26.474981608757606\n",
      "Epoch 446, Loss: 0.10105676275427239, Train MAPE: 24.27361344099045, Test MAPE: 26.323366491611186\n",
      "Epoch 447, Loss: 0.10093759804104384, Train MAPE: 24.511808563195743, Test MAPE: 26.616125205846934\n",
      "Epoch 448, Loss: 0.10085710061964794, Train MAPE: 24.217287876972787, Test MAPE: 26.25860057610732\n",
      "Epoch 449, Loss: 0.10076592733832793, Train MAPE: 24.37442493897218, Test MAPE: 26.454593401688797\n",
      "Epoch 450, Loss: 0.10064452479844196, Train MAPE: 24.263516760789432, Test MAPE: 26.320090535970834\n",
      "Epoch 451, Loss: 0.10056203784815107, Train MAPE: 24.25123379322199, Test MAPE: 26.31260771017808\n",
      "Epoch 452, Loss: 0.10044596251727153, Train MAPE: 24.170300760635964, Test MAPE: 26.212572310521054\n",
      "Epoch 453, Loss: 0.10035465351807384, Train MAPE: 24.210772708287607, Test MAPE: 26.26743099506085\n",
      "Epoch 454, Loss: 0.10028688095223445, Train MAPE: 24.142101455651797, Test MAPE: 26.188494953742396\n",
      "Epoch 455, Loss: 0.10015331767499447, Train MAPE: 24.15920244730436, Test MAPE: 26.21135670221769\n",
      "Epoch 456, Loss: 0.10004539431180232, Train MAPE: 24.363251319298378, Test MAPE: 26.463086880170383\n",
      "Epoch 457, Loss: 0.09995029232858751, Train MAPE: 24.19170237963016, Test MAPE: 26.26083566225492\n",
      "Epoch 458, Loss: 0.09983879911910312, Train MAPE: 24.170073813658494, Test MAPE: 26.23252112682049\n",
      "Epoch 459, Loss: 0.09973806997832771, Train MAPE: 24.163125057403857, Test MAPE: 26.230705525324893\n",
      "Epoch 460, Loss: 0.09967230495889313, Train MAPE: 24.077178829449874, Test MAPE: 26.12809176811805\n",
      "Epoch 461, Loss: 0.09953620532670847, Train MAPE: 23.912381704954, Test MAPE: 25.925220903983483\n",
      "Epoch 462, Loss: 0.09943303908042322, Train MAPE: 24.229906410437366, Test MAPE: 26.322647281793447\n",
      "Epoch 463, Loss: 0.0993732761654358, Train MAPE: 24.24375054561175, Test MAPE: 26.340807863382192\n",
      "Epoch 464, Loss: 0.09927998525090516, Train MAPE: 24.14718869649447, Test MAPE: 26.22972997518686\n",
      "Epoch 465, Loss: 0.09915891553346927, Train MAPE: 24.039893359404346, Test MAPE: 26.098407499606793\n",
      "Epoch 466, Loss: 0.09907225851381485, Train MAPE: 24.205700960526098, Test MAPE: 26.30724835395813\n",
      "Epoch 467, Loss: 0.09897653149584165, Train MAPE: 24.03899453695004, Test MAPE: 26.107085550748383\n",
      "Epoch 468, Loss: 0.09887855181363053, Train MAPE: 24.12828400501838, Test MAPE: 26.217990732192995\n",
      "Epoch 469, Loss: 0.09876464564854709, Train MAPE: 24.221214801531573, Test MAPE: 26.33341604746305\n",
      "Epoch 470, Loss: 0.09869303476298227, Train MAPE: 24.118183018611028, Test MAPE: 26.205512321912327\n",
      "Epoch 471, Loss: 0.09860070244803165, Train MAPE: 24.020880888975583, Test MAPE: 26.092788909031796\n",
      "Epoch 472, Loss: 0.09850900588163103, Train MAPE: 24.019528132218582, Test MAPE: 26.09654154043931\n",
      "Epoch 473, Loss: 0.0984019932605756, Train MAPE: 23.937292591425088, Test MAPE: 25.996906518936157\n",
      "Epoch 474, Loss: 0.09830939208623021, Train MAPE: 23.900601443877587, Test MAPE: 25.958548241395217\n",
      "Epoch 475, Loss: 0.09823268584889144, Train MAPE: 24.044524523845087, Test MAPE: 26.137280119382417\n",
      "Epoch 476, Loss: 0.09814217947423458, Train MAPE: 24.006114203196304, Test MAPE: 26.096872813885028\n",
      "Epoch 477, Loss: 0.09804521573958203, Train MAPE: 23.88179787489084, Test MAPE: 25.944560450773974\n",
      "Epoch 478, Loss: 0.09794127791224477, Train MAPE: 24.01246315057461, Test MAPE: 26.108337765473586\n",
      "Epoch 479, Loss: 0.09786774605297699, Train MAPE: 24.012371457540073, Test MAPE: 26.107636668131903\n",
      "Epoch 480, Loss: 0.09776241013135474, Train MAPE: 24.06113672714967, Test MAPE: 26.169177473508395\n",
      "Epoch 481, Loss: 0.0976670825191272, Train MAPE: 24.013297000298135, Test MAPE: 26.11187199812669\n",
      "Epoch 482, Loss: 0.0975887599436996, Train MAPE: 23.809744997207936, Test MAPE: 25.873818496557384\n",
      "Epoch 483, Loss: 0.0975090336591865, Train MAPE: 23.883471626501816, Test MAPE: 25.963229637879593\n",
      "Epoch 484, Loss: 0.09741943691176577, Train MAPE: 23.86616687132762, Test MAPE: 25.947358975043663\n",
      "Epoch 485, Loss: 0.09734801554765839, Train MAPE: 23.811033061834483, Test MAPE: 25.883265271553626\n",
      "Epoch 486, Loss: 0.09723261881333131, Train MAPE: 23.82968349456787, Test MAPE: 25.905556777807384\n",
      "Epoch 487, Loss: 0.09715082970662758, Train MAPE: 23.917501852604058, Test MAPE: 26.017500649965726\n",
      "Epoch 488, Loss: 0.0970661629904778, Train MAPE: 23.77671343913445, Test MAPE: 25.849180940481332\n",
      "Epoch 489, Loss: 0.09699597136260799, Train MAPE: 23.813287572677318, Test MAPE: 25.896593933839064\n",
      "Epoch 490, Loss: 0.09688985001188345, Train MAPE: 23.971983375916114, Test MAPE: 26.091369889332697\n",
      "Epoch 491, Loss: 0.09683256111143587, Train MAPE: 23.738729464090788, Test MAPE: 25.8092901963454\n",
      "Epoch 492, Loss: 0.0967093715390477, Train MAPE: 23.850318882098566, Test MAPE: 25.950448575386634\n",
      "Epoch 493, Loss: 0.09662012307044979, Train MAPE: 23.59895118016463, Test MAPE: 25.645752855447622\n",
      "Epoch 494, Loss: 0.09655203179348833, Train MAPE: 23.717361748218536, Test MAPE: 25.79062417470492\n",
      "Epoch 495, Loss: 0.096464252169244, Train MAPE: 23.73431953925353, Test MAPE: 25.820073953041664\n",
      "Epoch 496, Loss: 0.09636387181504127, Train MAPE: 23.685176154283376, Test MAPE: 25.759553039990937\n",
      "Epoch 497, Loss: 0.09629756858070883, Train MAPE: 23.743194086735066, Test MAPE: 25.833507816608137\n",
      "Epoch 498, Loss: 0.09619195715058595, Train MAPE: 23.67717929849258, Test MAPE: 25.757256350150474\n",
      "Epoch 499, Loss: 0.0961261301749171, Train MAPE: 23.664652965619013, Test MAPE: 25.74386073259207\n",
      "Epoch 500, Loss: 0.09602371727008946, Train MAPE: 23.94239858022103, Test MAPE: 26.085807961684008\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>test_mape</td><td>â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>time_test</td><td>â–„â–„â–„â–ƒâ–‚â–„â–ƒâ–„â–…â–‚â–‡â–‡â–…â–ƒâ–‚â–‚â–ƒâ–‚â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–â–‚â–‚â–ˆâ–ƒ</td></tr><tr><td>time_train</td><td>â–„â–â–‚â–ƒâ–„â–ƒâ–‚â–â–‚â–‚â–‚â–„â–„â–„â–ƒâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–„â–ˆâ–„â–‚</td></tr><tr><td>train_loss</td><td>â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train_mape</td><td>â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>0.12026</td></tr><tr><td>test_mape</td><td>26.08581</td></tr><tr><td>time_test</td><td>1.38516</td></tr><tr><td>time_train</td><td>2.55458</td></tr><tr><td>train_loss</td><td>0.09602</td></tr><tr><td>train_mape</td><td>23.9424</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">MLP-bs16-lr1e-05</strong> at: <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/owz0njn1' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/owz0njn1</a><br/> View project at: <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20241213_135117-owz0njn1\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src import torchModelRun\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "inputs_nums = len(df.columns) - 1\n",
    "\n",
    "class FullyConnectedModel(nn.Module):\n",
    "    import torch.nn.functional as F\n",
    "    def __init__(self):\n",
    "        super(FullyConnectedModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs_nums, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 32)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "models = torchModelRun.run(FullyConnectedModel, df, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T13:28:29.479665800Z",
     "start_time": "2024-12-13T12:51:17.390896Z"
    }
   },
   "id": "eeeae7de1d6f31eb",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:43: DtypeWarning: Columns (3,4,5,6,11,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,45,46,48,49,104,111,112,115,116,117,120,121,122,124,127,128,130,131) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(filePath)\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:70: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Space extracted']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:75: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Plot_area_unified']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:81: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ]].bfill(axis=1)['Availability']\n",
      "C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\src\\dataPipeline.py:136: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[mask, 'plz_parsed'] = df.loc[mask, 'address'].apply(extractPlz)\n"
     ]
    }
   ],
   "source": [
    "df_test_kaggle = dp.runPipeline(filePath='C:/Users/denis/Downloads/test_data-Kaggle-v0.11.csv/test_data-Kaggle-v0.11.csv', normalizeAndStandardize=False, imputer=imputer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T13:29:59.530604200Z",
     "start_time": "2024-12-13T13:28:52.813667500Z"
    }
   },
   "id": "17437bfaa3b365a3",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       Floor  detail_responsive#surface_usable  Floor_space_merged  \\\n0        2.4                             152.6               193.8   \n1        2.4                             152.6               193.8   \n2        0.4                             121.6               198.6   \n3        2.2                             152.6               140.0   \n4        0.6                             113.0               242.0   \n...      ...                               ...                 ...   \n24551    2.0                             202.4                64.2   \n24552    3.0                             202.4                64.2   \n24553    2.4                             162.0                64.2   \n24554    2.0                             202.4                64.2   \n24555    2.4                             184.2                64.2   \n\n       ForestDensityM  ForestDensityS  RiversAndLakesL  RiversAndLakesM  \\\n0            0.100030        0.063548         0.020765         0.034714   \n1            0.170434        0.083253         0.017035         0.033235   \n2            0.357984        0.125505         0.127272         0.067030   \n3            0.076610        0.000000         0.024733         0.030412   \n4            0.286451        0.090908         0.082170         0.001811   \n...               ...             ...              ...              ...   \n24551        0.018827        0.000000         0.000000         0.000000   \n24552        0.003706        0.011718         0.000000         0.000000   \n24553        0.003706        0.011718         0.000000         0.000000   \n24554        0.003706        0.011718         0.000000         0.000000   \n24555        0.003706        0.011718         0.000000         0.000000   \n\n       RiversAndLakesS  distanceToTrainStation  gde_area_forest_percentage  \\\n0             0.051031                0.487037                   44.392083   \n1             0.053474                0.691089                   44.392083   \n2             0.000000                2.689617                   51.449275   \n3             0.000000                0.447804                   44.392083   \n4             0.011871                3.038467                   51.449275   \n...                ...                     ...                         ...   \n24551         0.000000                9.851342                   27.252973   \n24552         0.000000                9.753366                   27.252973   \n24553         0.000000                9.753366                   27.252973   \n24554         0.000000                9.753366                   27.252973   \n24555         0.000000                9.753366                   27.252973   \n\n       ...  Swimming pool  View  Washing machine  Waste water connection  \\\n0      ...            0.0   0.0              0.0                     0.0   \n1      ...            0.0   0.0              0.0                     0.0   \n2      ...            0.0   0.0              0.0                     0.0   \n3      ...            0.0   0.0              0.0                     0.0   \n4      ...            0.0   0.0              0.0                     0.0   \n...    ...            ...   ...              ...                     ...   \n24551  ...            0.0   0.0              0.0                     0.0   \n24552  ...            0.0   0.0              0.0                     0.0   \n24553  ...            0.0   1.0              0.0                     0.0   \n24554  ...            0.0   0.0              0.0                     0.0   \n24555  ...            0.0   0.0              0.0                     0.0   \n\n       Water connection  Wheelchair access  With a summer house  covered  \\\n0                   0.0                0.0                  0.0      0.0   \n1                   0.0                0.0                  0.0      0.0   \n2                   0.0                0.0                  0.0      0.0   \n3                   0.0                0.0                  0.0      0.0   \n4                   0.0                0.0                  0.0      0.0   \n...                 ...                ...                  ...      ...   \n24551               0.0                0.0                  0.0      0.0   \n24552               0.0                0.0                  0.0      0.0   \n24553               0.0                0.0                  0.0      0.0   \n24554               0.0                0.0                  0.0      0.0   \n24555               0.0                0.0                  0.0      0.0   \n\n       Availability_Immediately  Availability_On request  \n0                           0.0                      1.0  \n1                           0.0                      1.0  \n2                           0.0                      1.0  \n3                           1.0                      0.0  \n4                           0.0                      1.0  \n...                         ...                      ...  \n24551                       0.0                      0.0  \n24552                       0.0                      0.0  \n24553                       0.0                      0.0  \n24554                       0.0                      0.0  \n24555                       0.0                      0.0  \n\n[24556 rows x 60 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Floor</th>\n      <th>detail_responsive#surface_usable</th>\n      <th>Floor_space_merged</th>\n      <th>ForestDensityM</th>\n      <th>ForestDensityS</th>\n      <th>RiversAndLakesL</th>\n      <th>RiversAndLakesM</th>\n      <th>RiversAndLakesS</th>\n      <th>distanceToTrainStation</th>\n      <th>gde_area_forest_percentage</th>\n      <th>...</th>\n      <th>Swimming pool</th>\n      <th>View</th>\n      <th>Washing machine</th>\n      <th>Waste water connection</th>\n      <th>Water connection</th>\n      <th>Wheelchair access</th>\n      <th>With a summer house</th>\n      <th>covered</th>\n      <th>Availability_Immediately</th>\n      <th>Availability_On request</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.4</td>\n      <td>152.6</td>\n      <td>193.8</td>\n      <td>0.100030</td>\n      <td>0.063548</td>\n      <td>0.020765</td>\n      <td>0.034714</td>\n      <td>0.051031</td>\n      <td>0.487037</td>\n      <td>44.392083</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.4</td>\n      <td>152.6</td>\n      <td>193.8</td>\n      <td>0.170434</td>\n      <td>0.083253</td>\n      <td>0.017035</td>\n      <td>0.033235</td>\n      <td>0.053474</td>\n      <td>0.691089</td>\n      <td>44.392083</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.4</td>\n      <td>121.6</td>\n      <td>198.6</td>\n      <td>0.357984</td>\n      <td>0.125505</td>\n      <td>0.127272</td>\n      <td>0.067030</td>\n      <td>0.000000</td>\n      <td>2.689617</td>\n      <td>51.449275</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.2</td>\n      <td>152.6</td>\n      <td>140.0</td>\n      <td>0.076610</td>\n      <td>0.000000</td>\n      <td>0.024733</td>\n      <td>0.030412</td>\n      <td>0.000000</td>\n      <td>0.447804</td>\n      <td>44.392083</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.6</td>\n      <td>113.0</td>\n      <td>242.0</td>\n      <td>0.286451</td>\n      <td>0.090908</td>\n      <td>0.082170</td>\n      <td>0.001811</td>\n      <td>0.011871</td>\n      <td>3.038467</td>\n      <td>51.449275</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24551</th>\n      <td>2.0</td>\n      <td>202.4</td>\n      <td>64.2</td>\n      <td>0.018827</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>9.851342</td>\n      <td>27.252973</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>24552</th>\n      <td>3.0</td>\n      <td>202.4</td>\n      <td>64.2</td>\n      <td>0.003706</td>\n      <td>0.011718</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>9.753366</td>\n      <td>27.252973</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>24553</th>\n      <td>2.4</td>\n      <td>162.0</td>\n      <td>64.2</td>\n      <td>0.003706</td>\n      <td>0.011718</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>9.753366</td>\n      <td>27.252973</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>24554</th>\n      <td>2.0</td>\n      <td>202.4</td>\n      <td>64.2</td>\n      <td>0.003706</td>\n      <td>0.011718</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>9.753366</td>\n      <td>27.252973</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>24555</th>\n      <td>2.4</td>\n      <td>184.2</td>\n      <td>64.2</td>\n      <td>0.003706</td>\n      <td>0.011718</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>9.753366</td>\n      <td>27.252973</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>24556 rows Ã— 60 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_kaggle"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T13:30:22.745495900Z",
     "start_time": "2024-12-13T13:30:22.717005Z"
    }
   },
   "id": "dd4a1b65a56568a",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = models[0]['model']\n",
    "model.eval()\n",
    "test_data = models[0]['transform'](torch.tensor(df_test_kaggle.values, dtype=torch.float32))\n",
    "output = model(test_data)\n",
    "\n",
    "#clamp output\n",
    "output = torch.clamp(output, 6.0, 18.0)\n",
    "output = torch.exp(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T13:30:26.660446600Z",
     "start_time": "2024-12-13T13:30:26.606828200Z"
    }
   },
   "id": "1cf2444367ef83c9",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(123131.2812, grad_fn=<MinBackward1>),\n tensor(65659968., grad_fn=<MaxBackward1>))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.min(), output.max()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T13:30:27.870487300Z",
     "start_time": "2024-12-13T13:30:27.848578100Z"
    }
   },
   "id": "d7f5eb79864d96ba",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "plt.hist(output.detach().numpy(), bins=100)\n",
    "plt.ylim([0, 100])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T13:30:32.875634500Z",
     "start_time": "2024-12-13T13:30:32.699315Z"
    }
   },
   "id": "f01a16b99505884a",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denis\\AppData\\Local\\Temp\\ipykernel_30592\\4266205768.py:3: DtypeWarning: Columns (3,4,5,6,11,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,45,46,48,49,104,111,112,115,116,117,120,121,122,124,127,128,130,131) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test_kaggle_unclean = pd.read_csv('C:/Users/denis/Downloads/test_data-Kaggle-v0.11.csv/test_data-Kaggle-v0.11.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": "       Unnamed: 0.1  Unnamed: 0 Municipality Living space Plot area  \\\n0                 0           0         Suhr       220 mÂ²    733 mÂ²   \n1                 1           1         Suhr       230 mÂ²    702 mÂ²   \n2                 2           2   Biberstein       131 mÂ²       NaN   \n3                 3           3         Suhr       140 mÂ²    206 mÂ²   \n4                 4           4   Biberstein       156 mÂ²    222 mÂ²   \n...             ...         ...          ...          ...       ...   \n24551         24561       24561     Wildhaus          NaN       NaN   \n24552         24562       24562     Wildhaus          NaN       NaN   \n24553         24563       24563     Wildhaus          NaN       NaN   \n24554         24564       24564     Wildhaus          NaN       NaN   \n24555         24565       24565     Wildhaus          NaN       NaN   \n\n      Floor space Availability                              location  \\\n0             NaN   On request            Galeggenweg 95034 Suhr, AG   \n1             NaN   On request     Hofstattmattenweg 195034 Suhr, AG   \n2             NaN   On request        Gheldweg 105023 Biberstein, AG   \n3          140 mÂ²  Immediately     Obere Dorfstrasse 275034 Suhr, AG   \n4          242 mÂ²   On request  Buhldenstrasse 8d5023 Biberstein, AG   \n...           ...          ...                                   ...   \n24551         NaN          NaN                  Hof 2, 9658 Wildhaus   \n24552         NaN          NaN                         9658 Wildhaus   \n24553         NaN          NaN                         9658 Wildhaus   \n24554         NaN          NaN                         9658 Wildhaus   \n24555         NaN          NaN    SteinrÃ¼tistrasse 28, 9658 Wildhaus   \n\n                                             description  \\\n0      6.5 rooms, 220 mÂ²Â«Landhausvilla mit einzigarti...   \n1      7.5 rooms, 230 mÂ²Â«GrosszÃ¼giges Anwesen mit tra...   \n2      4.5 rooms, 131 mÂ²Â«Terrassenhaus mit traumhafte...   \n3      6.5 rooms, 140 mÂ²Â«Mittelhaus in Suhr an attrak...   \n4      4.5 rooms, 156 mÂ²Â«Bezugsbereit - ruhige Lage u...   \n...                                                  ...   \n24551  1.5 Zimmerwohnung an zentraler Lage mit schÃ¶ne...   \n24552  OBJEKT IST BEREITS RESERVIERT!!Sie suchen eine...   \n24553  Mitten im Ober - Toggenburg ist ein 4 Zimmer C...   \n24554  Zu verkaufen ist hier in Wildhaus eine schÃ¶n u...   \n24555  1-Zi-Whg. (EG) / 4-Zi-Whg. (OG) / 2 1/2-Zi-Whg...   \n\n                                    detailed_description  ... Floor space:  \\\n0      DescriptionVilla lÃ¤dt zum TrÃ¤umen ein  Wir ver...  ...          NaN   \n1      DescriptionLicht, Glas und Holz - ein freisteh...  ...          NaN   \n2      DescriptionLage  Das 4.5-Zimmer-Terrassenhaus ...  ...          NaN   \n3      DescriptionReiheneinfamilienhaus (Mittelhaus) ...  ...          NaN   \n4      DescriptionStilvolle Liegenschaft an ruhiger L...  ...          NaN   \n...                                                  ...  ...          ...   \n24551  Description\\n\"SchÃ¶ne Kleinwohnung mit CheminÃ©e...  ...          NaN   \n24552  Description\\n\"BEREITS RESERVIERT!\"\\nOBJEKT IST...  ...          NaN   \n24553  Description\\n\"Chalet mit grossem GrundstÃ¼ck\"\\n...  ...          NaN   \n24554  Description\\n\"5Â½ Zimmer Ferienwohnung mit Gara...  ...          NaN   \n24555  Description\\n\"Haus mit 2 Einliegerwohnungen in...  ...          NaN   \n\n      Number of floors:  Volume:     plz Number of toilets: Gross yield:  \\\n0                   NaN      NaN  5034.0                NaN          NaN   \n1                   NaN      NaN  5034.0                NaN          NaN   \n2                   NaN      NaN  5023.0                NaN          NaN   \n3                   NaN      NaN  5034.0                NaN          NaN   \n4                   NaN      NaN  5023.0                NaN          NaN   \n...                 ...      ...     ...                ...          ...   \n24551               NaN      NaN  9658.0                NaN          NaN   \n24552               6.0      NaN  9658.0                NaN          NaN   \n24553               1.0      NaN  9658.0                NaN          NaN   \n24554               4.0      NaN  9658.0                NaN          NaN   \n24555               NaN  1463 m3  9658.0                NaN          NaN   \n\n      Minimum floor space: space_cleaned         Type: Hall height:  \n0                      NaN         220.0           NaN          NaN  \n1                      NaN         230.0           NaN          NaN  \n2                      NaN         131.0           NaN          NaN  \n3                      NaN         140.0           NaN          NaN  \n4                      NaN         156.0           NaN          NaN  \n...                    ...           ...           ...          ...  \n24551                  NaN          36.0     Apartment          NaN  \n24552                  NaN           NaN     Apartment          NaN  \n24553                  NaN         800.0  Single house          NaN  \n24554                  NaN           NaN     Apartment          NaN  \n24555                  NaN           NaN  Single house          NaN  \n\n[24556 rows x 132 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0.1</th>\n      <th>Unnamed: 0</th>\n      <th>Municipality</th>\n      <th>Living space</th>\n      <th>Plot area</th>\n      <th>Floor space</th>\n      <th>Availability</th>\n      <th>location</th>\n      <th>description</th>\n      <th>detailed_description</th>\n      <th>...</th>\n      <th>Floor space:</th>\n      <th>Number of floors:</th>\n      <th>Volume:</th>\n      <th>plz</th>\n      <th>Number of toilets:</th>\n      <th>Gross yield:</th>\n      <th>Minimum floor space:</th>\n      <th>space_cleaned</th>\n      <th>Type:</th>\n      <th>Hall height:</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>Suhr</td>\n      <td>220 mÂ²</td>\n      <td>733 mÂ²</td>\n      <td>NaN</td>\n      <td>On request</td>\n      <td>Galeggenweg 95034 Suhr, AG</td>\n      <td>6.5 rooms, 220 mÂ²Â«Landhausvilla mit einzigarti...</td>\n      <td>DescriptionVilla lÃ¤dt zum TrÃ¤umen ein  Wir ver...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5034.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>220.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Suhr</td>\n      <td>230 mÂ²</td>\n      <td>702 mÂ²</td>\n      <td>NaN</td>\n      <td>On request</td>\n      <td>Hofstattmattenweg 195034 Suhr, AG</td>\n      <td>7.5 rooms, 230 mÂ²Â«GrosszÃ¼giges Anwesen mit tra...</td>\n      <td>DescriptionLicht, Glas und Holz - ein freisteh...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5034.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>230.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>Biberstein</td>\n      <td>131 mÂ²</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>On request</td>\n      <td>Gheldweg 105023 Biberstein, AG</td>\n      <td>4.5 rooms, 131 mÂ²Â«Terrassenhaus mit traumhafte...</td>\n      <td>DescriptionLage  Das 4.5-Zimmer-Terrassenhaus ...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5023.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>131.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>Suhr</td>\n      <td>140 mÂ²</td>\n      <td>206 mÂ²</td>\n      <td>140 mÂ²</td>\n      <td>Immediately</td>\n      <td>Obere Dorfstrasse 275034 Suhr, AG</td>\n      <td>6.5 rooms, 140 mÂ²Â«Mittelhaus in Suhr an attrak...</td>\n      <td>DescriptionReiheneinfamilienhaus (Mittelhaus) ...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5034.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>140.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n      <td>Biberstein</td>\n      <td>156 mÂ²</td>\n      <td>222 mÂ²</td>\n      <td>242 mÂ²</td>\n      <td>On request</td>\n      <td>Buhldenstrasse 8d5023 Biberstein, AG</td>\n      <td>4.5 rooms, 156 mÂ²Â«Bezugsbereit - ruhige Lage u...</td>\n      <td>DescriptionStilvolle Liegenschaft an ruhiger L...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5023.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>156.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24551</th>\n      <td>24561</td>\n      <td>24561</td>\n      <td>Wildhaus</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Hof 2, 9658 Wildhaus</td>\n      <td>1.5 Zimmerwohnung an zentraler Lage mit schÃ¶ne...</td>\n      <td>Description\\n\"SchÃ¶ne Kleinwohnung mit CheminÃ©e...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9658.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>36.0</td>\n      <td>Apartment</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>24552</th>\n      <td>24562</td>\n      <td>24562</td>\n      <td>Wildhaus</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9658 Wildhaus</td>\n      <td>OBJEKT IST BEREITS RESERVIERT!!Sie suchen eine...</td>\n      <td>Description\\n\"BEREITS RESERVIERT!\"\\nOBJEKT IST...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>NaN</td>\n      <td>9658.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apartment</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>24553</th>\n      <td>24563</td>\n      <td>24563</td>\n      <td>Wildhaus</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9658 Wildhaus</td>\n      <td>Mitten im Ober - Toggenburg ist ein 4 Zimmer C...</td>\n      <td>Description\\n\"Chalet mit grossem GrundstÃ¼ck\"\\n...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>9658.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>800.0</td>\n      <td>Single house</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>24554</th>\n      <td>24564</td>\n      <td>24564</td>\n      <td>Wildhaus</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9658 Wildhaus</td>\n      <td>Zu verkaufen ist hier in Wildhaus eine schÃ¶n u...</td>\n      <td>Description\\n\"5Â½ Zimmer Ferienwohnung mit Gara...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>9658.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apartment</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>24555</th>\n      <td>24565</td>\n      <td>24565</td>\n      <td>Wildhaus</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>SteinrÃ¼tistrasse 28, 9658 Wildhaus</td>\n      <td>1-Zi-Whg. (EG) / 4-Zi-Whg. (OG) / 2 1/2-Zi-Whg...</td>\n      <td>Description\\n\"Haus mit 2 Einliegerwohnungen in...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1463 m3</td>\n      <td>9658.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Single house</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>24556 rows Ã— 132 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_test_kaggle_unclean = pd.read_csv('C:/Users/denis/Downloads/test_data-Kaggle-v0.11.csv/test_data-Kaggle-v0.11.csv')\n",
    "df_test_kaggle_unclean"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T13:31:33.984382800Z",
     "start_time": "2024-12-13T13:31:32.391240Z"
    }
   },
   "id": "a712a978c706c5f8",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_test_results = pd.DataFrame(columns=['Id','Expected'])\n",
    "df_test_results['Id'] = df_test_kaggle_unclean['Unnamed: 0']\n",
    "df_test_results['Expected'] = output.detach().numpy()\n",
    "df_test_results.to_csv('kaggle_results.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T13:32:01.820947300Z",
     "start_time": "2024-12-13T13:32:01.773977600Z"
    }
   },
   "id": "f8d5962e00221029",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# store model for later use\n",
    "torch.save(model, 'model.pt')\n",
    "\n",
    "#store transformer for later use\n",
    "torch.save(models[0]['transform'], 'transformer.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T13:30:49.078494500Z",
     "start_time": "2024-12-13T13:30:49.060260400Z"
    }
   },
   "id": "98ca9a507dfbab69",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a1177246b49aea34"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overfit Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3230a0b10c9edcfb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: denis-schatzmann. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888925108, max=1.0â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75378dbd89984d84bcb25901d89e2931"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.18.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\FHNW_Programmiersachen\\5_Sem\\immo_challenge\\wandb\\run-20241218_100149-b0sqyh0i</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/b0sqyh0i' target=\"_blank\">MLP-bs16-lr1e-05</a></strong> to <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/b0sqyh0i' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/b0sqyh0i</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 2, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 3, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 4, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 5, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 6, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 7, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 8, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 9, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 10, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 11, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 12, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 13, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 14, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 15, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 16, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 17, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 18, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 19, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 20, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 21, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 22, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 23, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 24, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 25, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 26, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 27, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 28, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 29, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 30, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 31, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 32, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 33, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 34, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 35, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 36, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 37, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 38, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 39, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 40, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 41, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 42, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 43, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 44, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 45, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 46, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 47, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 48, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 49, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 50, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 51, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 52, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 53, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 54, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 55, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 56, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 57, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 58, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 59, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 60, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 61, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 62, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 63, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 64, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 65, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 66, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 67, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 68, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 69, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 70, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 71, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 72, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 73, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 74, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 75, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 76, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 77, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 78, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 79, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 80, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 81, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 82, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 83, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 84, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 85, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 86, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 87, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 88, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 89, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 90, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 91, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 92, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 93, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 94, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 95, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 96, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 97, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 98, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 99, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 100, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 101, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 102, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 103, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 104, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 105, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 106, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 107, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 108, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 109, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 110, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 111, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 112, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 113, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 114, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 115, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 116, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 117, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 118, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 119, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 120, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 121, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 122, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 123, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 124, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 125, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 126, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 127, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 128, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 129, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 130, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 131, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 132, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 133, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 134, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 135, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 136, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 137, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 138, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 139, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 140, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 141, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 142, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 143, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 144, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 145, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 146, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 147, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 148, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 149, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 150, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 151, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 152, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 153, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 154, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 155, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 156, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 157, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 158, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 159, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 160, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 161, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 162, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 163, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 164, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 165, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 166, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 167, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 168, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 169, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 170, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 171, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 172, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 173, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 174, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 175, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 176, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 177, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 178, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 179, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 180, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 181, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 182, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 183, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 184, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 185, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 186, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 187, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 188, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 189, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 190, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 191, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 192, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 193, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 194, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 195, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 196, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 197, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 198, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 199, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 200, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 201, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 202, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 203, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 204, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 205, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 206, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 207, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 208, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 209, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 210, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 211, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 212, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 213, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 214, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 215, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 216, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 217, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 218, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 219, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 220, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 221, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 222, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 223, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 224, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 225, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 226, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 227, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 228, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 229, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 230, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 231, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 232, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 233, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 234, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 235, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 236, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 237, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 238, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 239, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 240, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 241, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 242, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 243, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 244, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 245, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 246, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 247, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 248, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 249, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 250, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 251, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 252, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 253, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 254, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 255, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 256, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 257, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 258, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 259, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 260, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 261, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 262, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 263, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 264, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 265, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 266, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 267, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 268, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 269, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 270, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 271, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 272, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 273, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 274, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 275, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 276, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 277, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 278, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 279, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 280, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 281, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 282, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 283, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 284, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 285, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 286, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 287, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 288, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 289, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 290, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 291, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 292, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 293, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 294, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 295, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 296, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 297, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 298, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 299, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Epoch 300, Loss: nan, Train MAPE: nan, Test MAPE: nan\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>time_test</td><td>â–ˆâ–‚â–‚â–‚â–‚â–‚â–ƒâ–„â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–â–„â–‚â–‚â–ƒâ–‚â–‚â–â–ƒâ–‚â–„â–â–â–ƒâ–‚â–‚â–â–‚â–‚â–‚</td></tr><tr><td>time_train</td><td>â–ƒâ–†â–†â–…â–ƒâ–ƒâ–…â–ƒâ–â–ƒâ–„â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–„â–…â–„â–„â–…â–„â–„â–â–…â–„â–†â–ƒâ–ˆâ–†â–ƒâ–ƒâ–„â–…â–ƒâ–ƒâ–…â–‚â–…</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>nan</td></tr><tr><td>test_mape</td><td>nan</td></tr><tr><td>time_test</td><td>0.02513</td></tr><tr><td>time_train</td><td>0.04938</td></tr><tr><td>train_loss</td><td>nan</td></tr><tr><td>train_mape</td><td>nan</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">MLP-bs16-lr1e-05</strong> at: <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/b0sqyh0i' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge/runs/b0sqyh0i</a><br/> View project at: <a href='https://wandb.ai/denis-schatzmann/Immo-Challenge' target=\"_blank\">https://wandb.ai/denis-schatzmann/Immo-Challenge</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20241218_100149-b0sqyh0i\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src import torchModelRun\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "inputs_nums = len(df.columns) - 1\n",
    "\n",
    "class FullyConnectedModel(nn.Module):\n",
    "    import torch.nn.functional as F\n",
    "    def __init__(self):\n",
    "        super(FullyConnectedModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs_nums, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 32)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "df = df.iloc[:128]\n",
    "models = torchModelRun.run(FullyConnectedModel, df, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-18T09:02:17.781440300Z",
     "start_time": "2024-12-18T09:01:42.046977600Z"
    }
   },
   "id": "19e8a06833cb520f",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6aebbec879e024a7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
